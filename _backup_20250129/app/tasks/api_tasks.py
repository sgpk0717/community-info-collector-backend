from celery import Task
from app.core.celery_app import celery_app
from app.services.call_queue_service import CallQueueService
from app.services.reddit_service import RedditService
from app.schemas.call_queue import CallQueueStatus
import asyncio
import logging
from typing import Dict, Any
import aiohttp
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

logger = logging.getLogger(__name__)

class APICallTask(Task):
    """API í˜¸ì¶œ íƒœìŠ¤í¬ ê¸°ë³¸ í´ë˜ìŠ¤"""
    autoretry_for = (aiohttp.ClientError, TimeoutError)
    retry_kwargs = {
        'max_retries': 3,
        'countdown': 60  # 60ì´ˆ í›„ ì¬ì‹œë„
    }

@celery_app.task(base=APICallTask, bind=True)
def make_api_call(self, queue_item_id: str) -> Dict[str, Any]:
    """ì‹¤ì œ API í˜¸ì¶œì„ ìˆ˜í–‰í•˜ëŠ” íƒœìŠ¤í¬"""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        result = loop.run_until_complete(_make_api_call_async(queue_item_id))
        return result
    finally:
        loop.close()

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=60),
    retry=retry_if_exception_type((aiohttp.ClientError, TimeoutError))
)
async def _make_api_call_async(queue_item_id: str) -> Dict[str, Any]:
    """ë¹„ë™ê¸° API í˜¸ì¶œ êµ¬í˜„"""
    queue_service = CallQueueService()
    reddit_service = RedditService()
    
    try:
        # í í•­ëª© ì¡°íšŒ
        result = queue_service.client.table('call_queue')\
            .select("*")\
            .eq('id', queue_item_id)\
            .single()\
            .execute()
        
        if not result.data:
            raise ValueError(f"Queue item not found: {queue_item_id}")
        
        queue_item = result.data
        
        # ìƒíƒœë¥¼ processingìœ¼ë¡œ ì—…ë°ì´íŠ¸
        await queue_service.update_status(queue_item_id, CallQueueStatus.PROCESSING)
        
        # Reddit API í˜¸ì¶œ
        logger.info(f"ğŸ”„ API í˜¸ì¶œ ì‹œì‘: {queue_item['source_url']}")
        
        # URLì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œí•˜ì—¬ Reddit ë°ì´í„° ìˆ˜ì§‘
        if 'reddit.com' in queue_item['source_url']:
            # ì„œë¸Œë ˆë”§ ì •ë³´ ì¶”ì¶œ
            url_parts = queue_item['source_url'].split('/')
            if 'r' in url_parts:
                subreddit_idx = url_parts.index('r')
                if subreddit_idx + 1 < len(url_parts):
                    subreddit = url_parts[subreddit_idx + 1]
                    
                    # API íŒŒë¼ë¯¸í„° ì ìš©
                    limit = queue_item['api_params'].get('limit', 25)
                    sort = queue_item['api_params'].get('sort', 'hot')
                    
                    # Reddit ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ë°ì´í„° ìˆ˜ì§‘
                    posts = await reddit_service.get_subreddit_posts(
                        subreddit=subreddit,
                        limit=limit,
                        sort=sort
                    )
                    
                    # ìˆ˜ì§‘ëœ ë°ì´í„° ì €ì¥
                    saved_count = 0
                    for post in posts:
                        # source_contents í…Œì´ë¸”ì— ì €ì¥
                        content_data = {
                            'source_id': post.get('id', ''),
                            'source_url': f"https://reddit.com{post.get('permalink', '')}",
                            'raw_text': f"{post.get('title', '')} {post.get('selftext', '')}",
                            'metadata': {
                                'author': post.get('author', ''),
                                'score': post.get('score', 0),
                                'num_comments': post.get('num_comments', 0),
                                'created_utc': post.get('created_utc', 0),
                                'subreddit': post.get('subreddit', ''),
                                'title': post.get('title', ''),
                                'queue_item_id': queue_item_id
                            }
                        }
                        
                        result = queue_service.client.table('source_contents').insert(content_data).execute()
                        if result.data:
                            saved_count += 1
                    
                    # ì„±ê³µ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                    await queue_service.update_status(queue_item_id, CallQueueStatus.COMPLETED)
                    
                    logger.info(f"âœ… API í˜¸ì¶œ ì™„ë£Œ: {saved_count}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘")
                    return {
                        'status': 'success',
                        'posts_collected': saved_count,
                        'queue_item_id': queue_item_id
                    }
        
        # Redditì´ ì•„ë‹Œ ê²½ìš° ì—ëŸ¬
        raise ValueError(f"Unsupported URL: {queue_item['source_url']}")
        
    except Exception as e:
        logger.error(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
        
        # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
        await queue_service.increment_retry_count(queue_item_id)
        
        # ì—ëŸ¬ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
        await queue_service.update_status(
            queue_item_id, 
            CallQueueStatus.ERROR,
            error=str(e)
        )
        
        # ì¬ì‹œë„ë¥¼ ìœ„í•´ ì˜ˆì™¸ ì¬ë°œìƒ
        raise