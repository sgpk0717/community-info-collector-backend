import logging
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer
from app.services.llm_service import LLMService
from app.core.dependencies import get_supabase_client
import json

logger = logging.getLogger(__name__)

class TopicModelingService:
    def __init__(self):
        logger.info("ğŸ§  TopicModelingService ì´ˆê¸°í™” ì‹œì‘")
        
        # í•œêµ­ì–´ ì§€ì› sentence transformer ëª¨ë¸ ì‚¬ìš©
        self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: paraphrase-multilingual-MiniLM-L12-v2")
        
        # BERTopic ì´ˆê¸°í™” (í•œêµ­ì–´ ë¶ˆìš©ì–´ ì œê±° ì—†ì´)
        self.topic_model = BERTopic(
            embedding_model=self.embedding_model,
            min_topic_size=3,  # ìµœì†Œ ì£¼ì œ í¬ê¸°
            nr_topics="auto",  # ìë™ìœ¼ë¡œ ì£¼ì œ ìˆ˜ ê²°ì •
            calculate_probabilities=True,
            verbose=True
        )
        logger.info("âœ… BERTopic ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        self.llm_service = LLMService()
        self.client = get_supabase_client()
    
    async def analyze_topics(self, session_id: str) -> List[Dict[str, Any]]:
        """ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì œë¥¼ ì¶”ì¶œí•˜ê³  ë¶„ì„"""
        logger.info(f"ğŸ“Š ì£¼ì œ ë¶„ì„ ì‹œì‘ - Session ID: {session_id}")
        
        try:
            # 1. ìˆ˜ì§‘ëœ ë°ì´í„° ì¡°íšŒ
            logger.info("ğŸ“¥ ìˆ˜ì§‘ëœ ë°ì´í„° ì¡°íšŒ ì¤‘...")
            result = self.client.table('source_contents')\
                .select("*")\
                .eq('metadata->>session_id', session_id)\
                .execute()
            
            if not result.data:
                logger.warning("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            documents = result.data
            logger.info(f"âœ… {len(documents)}ê°œì˜ ë¬¸ì„œ ì¡°íšŒ ì™„ë£Œ")
            
            # 2. í…ìŠ¤íŠ¸ ì¤€ë¹„
            texts = []
            doc_ids = []
            for doc in documents:
                text = doc['raw_text']
                if text and len(text.strip()) > 10:  # ìµœì†Œ ê¸¸ì´ í™•ì¸
                    texts.append(text)
                    doc_ids.append(doc['content_id'])
            
            logger.info(f"ğŸ“ ë¶„ì„ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸: {len(texts)}ê°œ")
            
            if len(texts) < 5:
                logger.warning("âš ï¸ ë¶„ì„í•˜ê¸°ì— í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ (ìµœì†Œ 5ê°œ í•„ìš”)")
                return await self._create_single_topic(documents)
            
            # 3. ì„ë² ë”© ìƒì„± (ì¬ì‚¬ìš©ì„ ìœ„í•´ ì €ì¥)
            logger.info("ğŸ”„ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings = self._create_embeddings(texts)
            
            # 4. BERTopicìœ¼ë¡œ ì£¼ì œ ëª¨ë¸ë§
            logger.info("ğŸ¯ BERTopic ì£¼ì œ ëª¨ë¸ë§ ì‹œì‘...")
            topics, probs = self.topic_model.fit_transform(texts, embeddings)
            
            # 5. ì£¼ì œ ì •ë³´ ì¶”ì¶œ
            topic_info = self.topic_model.get_topic_info()
            logger.info(f"âœ… {len(topic_info) - 1}ê°œì˜ ì£¼ì œ ë°œê²¬ (ì´ìƒì¹˜ ì œì™¸)")
            
            # ë¡œê·¸ë¡œ ì£¼ì œ ì •ë³´ ì¶œë ¥
            for idx, row in topic_info.iterrows():
                if row['Topic'] != -1:  # -1ì€ ì´ìƒì¹˜
                    logger.info(f"  ì£¼ì œ {row['Topic']}: {row['Count']}ê°œ ë¬¸ì„œ - {row['Name']}")
            
            # 6. ê° ì£¼ì œì— ëŒ€í•œ ìƒì„¸ ë¶„ì„
            topic_packages = []
            
            for topic_id in set(topics):
                if topic_id == -1:  # ì´ìƒì¹˜ ì œì™¸
                    continue
                
                # í•´ë‹¹ ì£¼ì œì˜ ë¬¸ì„œë“¤
                topic_docs = [texts[i] for i, t in enumerate(topics) if t == topic_id]
                topic_doc_ids = [doc_ids[i] for i, t in enumerate(topics) if t == topic_id]
                
                # ì£¼ì œì˜ í•µì‹¬ í‚¤ì›Œë“œ
                keywords = self.topic_model.get_topic(topic_id)
                keyword_list = [word for word, score in keywords[:10]]  # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
                
                logger.info(f"ğŸ·ï¸ ì£¼ì œ {topic_id} ë¶„ì„ ì¤‘... (ë¬¸ì„œ {len(topic_docs)}ê°œ)")
                logger.info(f"   í‚¤ì›Œë“œ: {', '.join(keyword_list[:5])}")
                
                # LLMìœ¼ë¡œ ì£¼ì œ ë ˆì´ë¸” ìƒì„±
                topic_label = await self._generate_topic_label(
                    topic_docs[:5],  # ëŒ€í‘œ ë¬¸ì„œ 5ê°œ
                    keyword_list
                )
                
                logger.info(f"   ìƒì„±ëœ ë ˆì´ë¸”: {topic_label}")
                
                # í•´ë‹¹ ì£¼ì œì˜ ë¬¸ì„œë“¤ì„ DBì— ì—…ë°ì´íŠ¸
                for doc_id in topic_doc_ids:
                    self.client.table('source_contents')\
                        .update({'topic_id': topic_id})\
                        .eq('content_id', doc_id)\
                        .execute()
                
                topic_package = {
                    'topic_id': topic_id,
                    'topic_label': topic_label,
                    'document_count': len(topic_docs),
                    'keywords': keyword_list,
                    'representative_docs': topic_docs[:3],  # ëŒ€í‘œ ë¬¸ì„œ 3ê°œ
                    'doc_ids': topic_doc_ids
                }
                
                topic_packages.append(topic_package)
            
            # ì£¼ì œë¥¼ ë¬¸ì„œ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            topic_packages.sort(key=lambda x: x['document_count'], reverse=True)
            
            logger.info(f"ğŸ‰ ì£¼ì œ ë¶„ì„ ì™„ë£Œ! ì´ {len(topic_packages)}ê°œ ì£¼ì œ")
            return topic_packages
            
        except Exception as e:
            logger.error(f"âŒ ì£¼ì œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë‹¨ì¼ ì£¼ì œë¡œ ì²˜ë¦¬
            return await self._create_single_topic(documents)
    
    def _create_embeddings(self, texts: List[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
        logger.info(f"ğŸ”¢ {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘...")
        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)
        logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: shape={embeddings.shape}")
        return embeddings
    
    async def _generate_topic_label(self, sample_docs: List[str], keywords: List[str]) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„± ë†’ì€ ì£¼ì œ ë ˆì´ë¸” ìƒì„±"""
        prompt = f"""ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œë“¤ê³¼ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•˜ì—¬ ì „ë¬¸ì ì¸ ì£¼ì œ ë ˆì´ë¸”ì„ ìƒì„±í•˜ëŠ” ë¦¬ì„œì¹˜ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ë¬¸ì„œ ìƒ˜í”Œ:
{chr(10).join(f'- {doc[:200]}...' for doc in sample_docs)}

í•µì‹¬ í‚¤ì›Œë“œ:
{', '.join(keywords)}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ì ì ˆí•œ í•œêµ­ì–´ ì£¼ì œ ë ˆì´ë¸”ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ë ˆì´ë¸”ì€ 10-20ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê³  ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤.
ì£¼ì œ ë ˆì´ë¸”ë§Œ ì‘ë‹µí•˜ì„¸ìš”."""
        
        try:
            response = await self.llm_service._call_openai(prompt, temperature=0.3)
            label = response.strip().strip('"').strip("'")
            return label
        except Exception as e:
            logger.error(f"LLM ë ˆì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # í´ë°±: í‚¤ì›Œë“œ ê¸°ë°˜ ë ˆì´ë¸”
            return f"{keywords[0]} ê´€ë ¨ ë…¼ì˜"
    
    async def _create_single_topic(self, documents: List[Dict]) -> List[Dict[str, Any]]:
        """ë¬¸ì„œê°€ ì ì„ ë•Œ ë‹¨ì¼ ì£¼ì œë¡œ ì²˜ë¦¬"""
        logger.info("ğŸ“¦ ë‹¨ì¼ ì£¼ì œë¡œ ì²˜ë¦¬")
        
        texts = [doc['raw_text'] for doc in documents if doc['raw_text']]
        doc_ids = [doc['content_id'] for doc in documents if doc['raw_text']]
        
        # ëª¨ë“  ë¬¸ì„œë¥¼ ì£¼ì œ 0ìœ¼ë¡œ í• ë‹¹
        for doc_id in doc_ids:
            self.client.table('source_contents')\
                .update({'topic_id': 0})\
                .eq('content_id', doc_id)\
                .execute()
        
        return [{
            'topic_id': 0,
            'topic_label': 'ì „ì²´ ë‚´ìš© ì¢…í•©',
            'document_count': len(texts),
            'keywords': ['ì¢…í•©', 'ì „ì²´'],
            'representative_docs': texts[:3],
            'doc_ids': doc_ids
        }]
    
    def save_topic_model(self, session_id: str):
        """í•™ìŠµëœ í† í”½ ëª¨ë¸ ì €ì¥ (ì„ íƒì‚¬í•­)"""
        try:
            model_path = f"models/topic_model_{session_id}"
            self.topic_model.save(model_path)
            logger.info(f"ğŸ’¾ í† í”½ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")