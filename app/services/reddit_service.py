import praw
from typing import List, Dict, Any, Optional
from app.core.dependencies import get_reddit_client
from app.core.exceptions import RedditAPIException
import logging
from datetime import datetime, timedelta
import asyncio
import re
from functools import partial
from concurrent.futures import ThreadPoolExecutor
from collections import deque
import time

logger = logging.getLogger(__name__)

# ë£¨ë¨¸ íƒì§€ìš© í‚¤ì›Œë“œ ì„¸íŠ¸
SPECULATIVE_WORDS = {
    'might', 'could', 'perhaps', 'likely', 'possibly', 'maybe', 'supposedly',
    'allegedly', 'rumor', 'rumour', 'unconfirmed', 'sources say', 'i heard',
    'word is', 'gossip', 'speculation', 'apparently', 'seems like',
    # í•œêµ­ì–´ ì¶”ì¸¡ í‘œí˜„
    'ì•„ë§ˆë„', 'ì•„ë§ˆ', '~ì¼ ìˆ˜ë„', '~ì¸ ê²ƒ ê°™', 'ì†Œë¬¸ì—', 'ì¹´ë”ë¼', 'ë“¤ì—ˆëŠ”ë°',
    '~ë¼ëŠ” ì–˜ê¸°', '~ê°™ë‹¤ëŠ”', 'ì¶”ì¸¡', 'ì˜ˆìƒ', '~ì¼ì§€ë„'
}

NEGATIVE_EMOTION_WORDS = {
    'disaster', 'crisis', 'collapse', 'crash', 'terrible', 'awful', 'horrible',
    'devastating', 'nightmare', 'panic', 'fear', 'doom', 'failed', 'failure',
    'warning', 'danger', 'risk', 'threat', 'concerned', 'worried',
    # í•œêµ­ì–´ ë¶€ì •ì  ê°ì • í‘œí˜„
    'ì¬ì•™', 'ìœ„ê¸°', 'ë¶•ê´´', 'ì¶”ë½', 'ë”ì°í•œ', 'ë¬´ì„œìš´', 'ê³µí¬', 'ì‹¤íŒ¨', 'ìœ„í—˜',
    'ê²½ê³ ', 'ê±±ì •', 'ìš°ë ¤', 'ë¬¸ì œ', 'ì‹¬ê°í•œ', 'ìµœì•…'
}

class RedditService:
    def __init__(self, thread_pool: Optional[ThreadPoolExecutor] = None):
        self.client = get_reddit_client()
        self.thread_pool = thread_pool
        
        # Rate Limit ê´€ë¦¬ë¥¼ ìœ„í•œ ì†ì„±
        self.request_timestamps = deque(maxlen=60)  # ìµœê·¼ 60ê°œ ìš”ì²­ ì‹œê°„ ì €ì¥
        self.rate_limit_lock = asyncio.Lock()  # ë™ì‹œì„± ì œì–´
    
    def _calculate_rumor_score_sync(self, submission) -> float:
        """ë£¨ë¨¸ ì ìˆ˜ ê³„ì‚° (0-10 ë²”ìœ„) - ë™ê¸° ë²„ì „"""
        score = 0.0
        score_breakdown = []
        
        # 1. ë…¼ë€ì„± ì§€í‘œ (upvote_ratio)
        if hasattr(submission, 'upvote_ratio') and submission.upvote_ratio < 0.7:
            controversy_score = (0.7 - submission.upvote_ratio) * 5  # ìµœëŒ€ 3.5ì 
            score += controversy_score
            score_breakdown.append(f"ë…¼ë€ì„±({submission.upvote_ratio:.2f}): +{controversy_score:.1f}")
        
        # 2. ì–¸ì–´í•™ì  ì‹ í˜¸ íƒì§€
        text = (submission.title + ' ' + (submission.selftext or '')).lower()
        
        # ì¶”ì¸¡ì„± ë‹¨ì–´ ê°œìˆ˜
        speculation_count = sum(1 for word in SPECULATIVE_WORDS if word in text)
        if speculation_count > 0:
            speculation_score = min(speculation_count * 1.5, 3.0)  # ìµœëŒ€ 3ì 
            score += speculation_score
            score_breakdown.append(f"ì¶”ì¸¡ì„±({speculation_count}ê°œ): +{speculation_score:.1f}")
        
        # ë¶€ì •ì  ê°ì • ë‹¨ì–´ ê°œìˆ˜
        negative_count = sum(1 for word in NEGATIVE_EMOTION_WORDS if word in text)
        if negative_count > 0:
            negative_score = min(negative_count * 1.0, 2.0)  # ìµœëŒ€ 2ì 
            score += negative_score
            score_breakdown.append(f"ë¶€ì •ê°ì •({negative_count}ê°œ): +{negative_score:.1f}")
        
        # 3. ê²Œì‹œë¬¼ íŠ¹ì„±
        if hasattr(submission, 'is_self') and submission.is_self:
            score += 0.5  # ìì²´ ê²Œì‹œë¬¼ (ë§í¬ê°€ ì•„ë‹Œ í…ìŠ¤íŠ¸)
            score_breakdown.append(f"ìì²´ê²Œì‹œë¬¼: +0.5")
        
        # 4. ìˆ˜ì§‘ ë²¡í„° ê°€ì¤‘ì¹˜
        if hasattr(submission, '_collection_vector'):
            if submission._collection_vector == 'underground':
                score += 1.0  # ë…¼ë€ì„± ë²¡í„°ì—ì„œ ìˆ˜ì§‘ëœ ê²½ìš°
                score_breakdown.append(f"ë…¼ë€ì„±ë²¡í„°: +1.0")
            elif submission._collection_vector == 'vanguard':
                score += 0.5  # ìµœì‹  ë²¡í„°ì—ì„œ ìˆ˜ì§‘ëœ ê²½ìš°
                score_breakdown.append(f"ìµœì‹ ë²¡í„°: +0.5")
        
        final_score = min(score, 10.0)  # ìµœëŒ€ 10ì ìœ¼ë¡œ ì œí•œ
        
        # ë£¨ë¨¸ ì ìˆ˜ ê³„ì‚° ë¡œê·¸ (ì ìˆ˜ê°€ 5 ì´ìƒì¼ ë•Œë§Œ)
        if final_score >= 5.0:
            logger.info(f"ğŸš¨ ë†’ì€ ë£¨ë¨¸ ì ìˆ˜ ê°ì§€ [{final_score:.1f}/10] - {submission.title[:50]}...")
            logger.info(f"   ì„¸ë¶€ì‚¬í•­: {', '.join(score_breakdown)}")
        
        return final_score
    
    def _extract_linguistic_flags_sync(self, text: str) -> List[str]:
        """ì–¸ì–´í•™ì  ì‹ í˜¸ í”Œë˜ê·¸ ì¶”ì¶œ - ë™ê¸° ë²„ì „"""
        flags = []
        text_lower = text.lower()
        
        # ì¶”ì¸¡ì„± ì–¸ì–´ íƒì§€
        speculation_words = [word for word in SPECULATIVE_WORDS if word in text_lower]
        if speculation_words:
            flags.append('speculation')
            logger.debug(f"ğŸ” ì¶”ì¸¡ì„± ì–¸ì–´ ê°ì§€: {speculation_words[:3]}...")
        
        # ë¶€ì •ì  ê°ì • íƒì§€
        negative_words = [word for word in NEGATIVE_EMOTION_WORDS if word in text_lower]
        if negative_words:
            flags.append('negative_emotion')
            logger.debug(f"ğŸ˜  ë¶€ì •ì  ê°ì • ê°ì§€: {negative_words[:3]}...")
        
        # ë¹„ê³µì‹ì„± íƒì§€ (ëŠë‚Œí‘œ, ëŒ€ë¬¸ì ê³¼ë‹¤ ì‚¬ìš©)
        exclamation_count = text.count('!')
        caps_words = re.findall(r'[A-Z]{3,}', text)
        if exclamation_count > 2 or len(caps_words) > 0:
            flags.append('informal')
            logger.debug(f"ğŸ“¢ ë¹„ê³µì‹ì„± ê°ì§€: ëŠë‚Œí‘œ {exclamation_count}ê°œ, ëŒ€ë¬¸ì {len(caps_words)}ê°œ")
        
        return flags
    
    async def _process_submission_batch(self, submissions: List[Any]) -> List[Dict[str, Any]]:
        """ê²Œì‹œë¬¼ ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
        if not self.thread_pool:
            # ìŠ¤ë ˆë“œí’€ì´ ì—†ìœ¼ë©´ ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬
            return [self._process_submission_sync(sub) for sub in submissions]
        
        # ìŠ¤ë ˆë“œí’€ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(self.thread_pool, self._process_submission_sync, sub)
            for sub in submissions
        ]
        return await asyncio.gather(*tasks)
    
    def _process_submission_sync(self, submission) -> Dict[str, Any]:
        """ë‹¨ì¼ ê²Œì‹œë¬¼ ì²˜ë¦¬ - ë™ê¸° ë²„ì „"""
        text_to_analyze = submission.title + ' ' + (submission.selftext or '')
        
        # CPU ì§‘ì•½ì  ì‘ì—…ë“¤
        rumor_score = self._calculate_rumor_score_sync(submission)
        linguistic_flags = self._extract_linguistic_flags_sync(text_to_analyze)
        
        return {
            'id': submission.id,
            'title': submission.title,
            'selftext': submission.selftext,
            'score': submission.score,
            'num_comments': submission.num_comments,
            'created_utc': submission.created_utc,
            'author': str(submission.author),
            'subreddit': str(submission.subreddit),
            'url': f"https://reddit.com{submission.permalink}",
            'upvote_ratio': getattr(submission, 'upvote_ratio', 0.5),
            'is_self': getattr(submission, 'is_self', True),
            'collection_vector': getattr(submission, '_collection_vector', 'zeitgeist'),
            'rumor_score': rumor_score,
            'linguistic_flags': linguistic_flags
        }
        
    async def search_posts(self, query: str, limit: int = 50, time_filter: str = 'all') -> List[Dict[str, Any]]:
        """Redditì—ì„œ í‚¤ì›Œë“œë¡œ ê²Œì‹œë¬¼ ê²€ìƒ‰ - ë‹¤ì¤‘ ë²¡í„° ìˆ˜ì§‘ ì „ëµ
        
        Args:
            query: ê²€ìƒ‰ í‚¤ì›Œë“œ
            limit: ìµœëŒ€ ê²Œì‹œë¬¼ ìˆ˜
            time_filter: ì‹œê°„ í•„í„° ('hour', 'day', 'week', 'month', 'year', 'all')
        """
        try:
            # Rate limit ì²´í¬
            await self._check_rate_limit()
            
            logger.info(f"ğŸ” Reddit ê²€ìƒ‰ ì‹œì‘: '{query}' (ìµœëŒ€ {limit}ê°œ ê²Œì‹œë¬¼, ê¸°ê°„: {time_filter})")
            
            # Reddit API í˜¸ì¶œì€ ìŠ¤ë ˆë“œí’€ì—ì„œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            
            def _search():
                # ë‹¤ì¤‘ ë²¡í„° ìˆ˜ì§‘ ì „ëµ (ì‹œê°„ í•„í„° ì ìš©)
                # ì‚¬ìš©ì ì§€ì • time_filterê°€ ìˆìœ¼ë©´ ëª¨ë“  ë²¡í„°ì— ì ìš©
                if time_filter != 'all':
                    vectors = [
                        {'name': 'zeitgeist', 'sort': 'hot', 'time_filter': time_filter, 'limit': limit//3},
                        {'name': 'underground', 'sort': 'controversial', 'time_filter': time_filter, 'limit': limit//3},
                        {'name': 'vanguard', 'sort': 'new', 'time_filter': time_filter, 'limit': limit//3}
                    ]
                else:
                    # ê¸°ë³¸ ì „ëµ
                    vectors = [
                        {'name': 'zeitgeist', 'sort': 'hot', 'time_filter': 'week', 'limit': limit//3},
                        {'name': 'underground', 'sort': 'controversial', 'time_filter': 'month', 'limit': limit//3},
                        {'name': 'vanguard', 'sort': 'new', 'time_filter': 'all', 'limit': limit//3}
                    ]
                
                logger.info(f"ğŸ“Š ë‹¤ì¤‘ ë²¡í„° ìˆ˜ì§‘ ì „ëµ ì‹œì‘ - ì´ {len(vectors)}ê°œ ë²¡í„°")
                all_submissions = []
                
                for vector in vectors:
                    try:
                        logger.info(f"ğŸ¯ ë²¡í„° '{vector['name']}' ê²€ìƒ‰ ì‹œì‘ - ì •ë ¬: {vector['sort']}, ê¸°ê°„: {vector['time_filter']}, ì œí•œ: {vector['limit']}")
                        
                        subreddit = self.client.subreddit('all')
                        
                        # ë²¡í„°ë³„ ê²€ìƒ‰ ìˆ˜í–‰
                        if vector['sort'] == 'hot':
                            search_results = subreddit.search(
                                query, 
                                limit=vector['limit'], 
                                sort='hot',
                                time_filter=vector['time_filter']
                            )
                        elif vector['sort'] == 'controversial':
                            search_results = subreddit.search(
                                query, 
                                limit=vector['limit'], 
                                sort='controversial',
                                time_filter=vector['time_filter']
                            )
                        else:  # new
                            search_results = subreddit.search(
                                query, 
                                limit=vector['limit'], 
                                sort='new',
                                time_filter=vector['time_filter']
                            )
                        
                        # ë²¡í„° ì •ë³´ë¥¼ ê° submissionì— ì¶”ê°€
                        vector_submissions = []
                        for submission in search_results:
                            submission._collection_vector = vector['name']
                            vector_submissions.append(submission)
                        
                        all_submissions.extend(vector_submissions)
                        logger.info(f"âœ… ë²¡í„° '{vector['name']}' ìˆ˜ì§‘ ì™„ë£Œ: {len(vector_submissions)}ê°œ ê²Œì‹œë¬¼")
                        
                    except Exception as e:
                        logger.error(f"âŒ ë²¡í„° '{vector['name']}' ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                        continue
                
                logger.info(f"ğŸ“ˆ ì „ì²´ ë²¡í„° ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_submissions)}ê°œ ê²Œì‹œë¬¼")
                return all_submissions
            
            # Reddit API ê²€ìƒ‰ì„ ìŠ¤ë ˆë“œí’€ì—ì„œ ì‹¤í–‰
            if self.thread_pool:
                all_submissions = await loop.run_in_executor(self.thread_pool, _search)
            else:
                all_submissions = await loop.run_in_executor(None, _search)
            
            # ê²Œì‹œë¬¼ ì²˜ë¦¬ë¥¼ ë³‘ë ¬ë¡œ ìˆ˜í–‰
            posts = await self._process_submission_batch(all_submissions)
            
            logger.info(f"âœ… Reddit ê²€ìƒ‰ ì™„ë£Œ - ì´ {len(posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘")
            return posts
            
        except Exception as e:
            logger.error(f"Reddit search error: {str(e)}")
            raise RedditAPIException(f"Failed to search Reddit: {str(e)}")
    
    async def get_subreddit_info(self, subreddit_name: str) -> Dict[str, Any]:
        """íŠ¹ì • subreddit ì •ë³´ ì¡°íšŒ"""
        try:
            loop = asyncio.get_event_loop()
            
            def _get_info():
                subreddit = self.client.subreddit(subreddit_name)
                return {
                    'name': subreddit.display_name,
                    'title': subreddit.title,
                    'subscribers': subreddit.subscribers,
                    'description': subreddit.public_description,
                    'created_utc': subreddit.created_utc,
                    'url': f"https://reddit.com/r/{subreddit_name}"
                }
            
            if self.thread_pool:
                return await loop.run_in_executor(self.thread_pool, _get_info)
            else:
                return await loop.run_in_executor(None, _get_info)
            
        except Exception as e:
            logger.error(f"Failed to get subreddit info: {str(e)}")
            raise RedditAPIException(f"Failed to get subreddit info: {str(e)}")
            
    def _calculate_weighted_scores(self, posts: List[Dict[str, Any]], query_words: List[str]) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ì–´ ì¼ì¹˜ë„ ë° ê¸°íƒ€ ìš”ì†Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì ìˆ˜ ê³„ì‚°"""
        for post in posts:
            relevance_score = 0
            
            # 1. ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¼ì¹˜ í™•ì¸ (ê°€ì¤‘ì¹˜ 2.0)
            title_lower = post['title'].lower()
            for word in query_words:
                if word.lower() in title_lower:
                    relevance_score += 2.0
            
            # 2. ë³¸ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¼ì¹˜ í™•ì¸ (ê°€ì¤‘ì¹˜ 1.0)
            if post.get('selftext'):
                selftext_lower = post['selftext'].lower()
                for word in query_words:
                    if word.lower() in selftext_lower:
                        relevance_score += 1.0
            
            # 3. Reddit ì ìˆ˜ ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ, ê°€ì¤‘ì¹˜ 0.5)
            reddit_score = post['score']
            normalized_reddit_score = min(reddit_score / 1000, 1.0) * 0.5
            relevance_score += normalized_reddit_score
            
            # 4. ëŒ“ê¸€ ìˆ˜ ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ, ê°€ì¤‘ì¹˜ 0.3) 
            comments = post['num_comments']
            normalized_comments = min(comments / 100, 1.0) * 0.3
            relevance_score += normalized_comments
            
            # 5. ìµœì‹ ì„± ê°€ì¤‘ì¹˜ (24ì‹œê°„ ì´ë‚´ë©´ ë³´ë„ˆìŠ¤)
            created_time = datetime.fromtimestamp(post['created_utc'])
            hours_old = (datetime.now() - created_time).total_seconds() / 3600
            if hours_old < 24:
                relevance_score += 0.5
            elif hours_old < 48:
                relevance_score += 0.3
            
            post['weighted_score'] = relevance_score
            
        return posts
    
    async def _check_rate_limit(self):
        """Rate limit í™•ì¸ ë° ëŒ€ê¸° (Reddit API: 60 requests/minute)"""
        async with self.rate_limit_lock:
            now = datetime.now()
            
            # 1ë¶„ ì´ë‚´ì˜ ìš”ì²­ë§Œ ìœ ì§€
            while self.request_timestamps and (now - self.request_timestamps[0]) > timedelta(minutes=1):
                self.request_timestamps.popleft()
            
            # 59ê°œ ì´ìƒì˜ ìš”ì²­ì´ ìˆìœ¼ë©´ ëŒ€ê¸°
            if len(self.request_timestamps) >= 59:
                # ê°€ì¥ ì˜¤ë˜ëœ ìš”ì²­ìœ¼ë¡œë¶€í„° 1ë¶„ í›„ê¹Œì§€ ëŒ€ê¸°
                oldest_request = self.request_timestamps[0]
                wait_time = (oldest_request + timedelta(minutes=1) - now).total_seconds()
                
                if wait_time > 0:
                    logger.info(f"â³ Reddit API Rate limit ë„ë‹¬. {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    await asyncio.sleep(wait_time)
            
            # í˜„ì¬ ìš”ì²­ ì‹œê°„ ê¸°ë¡
            self.request_timestamps.append(now)
    
    async def search_with_keywords(self, keywords: List[str], limit_per_keyword: int = 10) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ì—¬ ê²Œì‹œë¬¼ ìˆ˜ì§‘ (Rate Limit ì¤€ìˆ˜)"""
        all_posts = []
        total_keywords = len(keywords)
        
        logger.info(f"ğŸ” ë‹¤ì¤‘ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œì‘: {total_keywords}ê°œ í‚¤ì›Œë“œ")
        logger.info(f"   í‚¤ì›Œë“œ ëª©ë¡: {keywords[:5]}{'...' if len(keywords) > 5 else ''}")
        
        for i, keyword in enumerate(keywords):
            try:
                # Rate limit ì²´í¬
                await self._check_rate_limit()
                
                # ì§„í–‰ ìƒí™© ë¡œê·¸
                logger.info(f"ğŸ” [{i+1}/{total_keywords}] í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì¤‘...")
                
                # ì‹¤ì œ ê²€ìƒ‰ ìˆ˜í–‰ (ê° í‚¤ì›Œë“œë‹¹ ì œí•œëœ ìˆ˜ë§Œ ìˆ˜ì§‘)
                posts = await self.search_posts(keyword, limit=limit_per_keyword)
                all_posts.extend(posts)
                
                logger.info(f"âœ… í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì™„ë£Œ: {len(posts)}ê°œ ìˆ˜ì§‘")
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if (i + 1) % 5 == 0 or (i + 1) == total_keywords:
                    logger.info(f"ğŸ“Š ì§„í–‰ë¥ : {(i+1)/total_keywords*100:.1f}% ì™„ë£Œ ({i+1}/{total_keywords})")
                
            except Exception as e:
                logger.warning(f"âš ï¸ í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                
                # Rate limit ì—ëŸ¬ì¸ ê²½ìš° ì¶”ê°€ ëŒ€ê¸°
                if "rate limit" in str(e).lower():
                    logger.warning("ğŸš¨ Rate limit ì—ëŸ¬ ê°ì§€. 30ì´ˆ ì¶”ê°€ ëŒ€ê¸°...")
                    await asyncio.sleep(30)
                
                continue
        
        logger.info(f"âœ… ë‹¤ì¤‘ í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘")
        return all_posts