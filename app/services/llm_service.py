from typing import List, Dict, Any, Optional, Literal
from app.core.exceptions import OpenAIAPIException
from app.schemas.search import ReportLength
from app.services.llm_providers import BaseLLMProvider, OpenAIProvider, GeminiProvider
import logging
import json
import os
import asyncio

logger = logging.getLogger(__name__)

# Provider íƒ€ì… ì •ì˜
LLMProviderType = Literal["openai", "gemini"]


class LLMService:
    """ë‹¤ì¤‘ LLM Providerë¥¼ ì§€ì›í•˜ëŠ” í†µí•© LLM Service"""
    
    def __init__(self, provider_type: Optional[LLMProviderType] = None, api_semaphore: Optional[asyncio.Semaphore] = None):
        """
        LLMService ì´ˆê¸°í™”
        
        Args:
            provider_type: ì‚¬ìš©í•  LLM provider ("openai" ë˜ëŠ” "gemini")
                          Noneì¸ ê²½ìš° í™˜ê²½ë³€ìˆ˜ LLM_PROVIDERì—ì„œ ì½ìŒ (ê¸°ë³¸ê°’: "openai")
        """
        # Provider íƒ€ì… ê²°ì •
        if provider_type is None:
            provider_type = os.getenv('LLM_PROVIDER', 'openai').lower()
        
        # Provider ì´ˆê¸°í™”
        self.api_semaphore = api_semaphore
        self.provider = self._initialize_provider(provider_type)
        logger.info(f"LLMService ì´ˆê¸°í™” ì™„ë£Œ - Provider: {self.provider.provider_name}, Model: {self.provider.default_model}")
    
    def _initialize_provider(self, provider_type: str) -> BaseLLMProvider:
        """Provider íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ provider ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        if provider_type == "openai":
            return OpenAIProvider(api_semaphore=self.api_semaphore)
        elif provider_type == "gemini":
            return GeminiProvider()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” provider íƒ€ì…: {provider_type}")
    
    async def translate_to_english(self, query: str) -> str:
        """í•œê¸€ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"""
        logger.info(f"ğŸŒ ë²ˆì—­ ì‹œì‘: '{query}'")
        try:
            prompt = f"""Translate the following Korean search query to English for Reddit search.
            
            Rules:
            1. If it's already in English, return as is
            2. Translate company/brand names to their official English names
            3. Keep the search intent clear and specific
            4. Use common English terms that Reddit users would use
            5. Only return the translated text, nothing else
            
            Examples:
            - "êµ¬ê¸€ ì‹¤ì ë°œí‘œ ì˜ˆì¸¡" â†’ "Google earnings prediction"
            - "í…ŒìŠ¬ë¼ ììœ¨ì£¼í–‰ ê¸°ìˆ " â†’ "Tesla autonomous driving technology"
            - "ì‚¼ì„± ì‹ ì œí’ˆ ë£¨ë¨¸" â†’ "Samsung new product rumors"
            
            Keyword: {query}
            Translation:
            """
            
            response = await self.provider.generate(
                prompt=prompt,
                system_prompt="You are a professional translator.",
                temperature=0.3,
                max_tokens=100
            )
            
            translated = response.content.strip()
            logger.info(f"âœ… ë²ˆì—­ ì™„ë£Œ: '{query}' â†’ '{translated}'")
            return translated
            
        except Exception as e:
            logger.error(f"âŒ ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"   Provider: {self.provider.provider_name}")
            logger.error(f"   Model: {self.provider.default_model}")
            logger.error(f"   Query: '{query}'")
            import traceback
            logger.error(f"   Stack trace:\n{traceback.format_exc()}")
            return query  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    async def expand_keywords(self, query: str) -> List[str]:
        """ì£¼ì–´ì§„ í‚¤ì›Œë“œë¥¼ í™•ì¥í•˜ì—¬ ê´€ë ¨ ê²€ìƒ‰ì–´ ìƒì„± (ì˜ì–´)"""
        logger.info(f"ğŸ” í‚¤ì›Œë“œ í™•ì¥ ì‹œì‘: '{query}'")
        try:
            # ë¨¼ì € ì˜ì–´ë¡œ ë²ˆì—­
            english_query = await self.translate_to_english(query)
            logger.info(f"   ë²ˆì—­ëœ ì¿¼ë¦¬: '{english_query}'")
            
            prompt = f"""Extract ALL effective search keywords for Reddit about: "{english_query}"
            
            SEARCH STRATEGY RULES:
            1. Generate SHORT, HIGH-IMPACT keywords (1-3 words preferred)
            2. Include multiple variations:
               - Main topic alone (e.g., "Google")
               - Topic + action words (e.g., "Google earnings", "GOOGL forecast")
               - Stock symbols if applicable (e.g., "GOOGL", "GOOG")
               - Common abbreviations and full names
               - Singular AND plural forms
               - Present AND future tense variations
            3. Focus on HIGH-INTENT search patterns:
               - Questions: "how", "what", "when" + topic
               - Comparisons: "vs", "versus", "or"
               - Opinions: "best", "worst", "review"
               - Predictions: "forecast", "prediction", "outlook"
            4. Include Reddit-specific terms:
               - DD (Due Diligence)
               - YOLO, calls, puts (for stock-related)
               - ELI5 (Explain Like I'm 5)
            5. Extract 10-20 keywords to maximize coverage
            
            Examples:
            - For "Tesla earnings prediction": ["Tesla", "TSLA", "Tesla earnings", "TSLA earnings", "Tesla Q4", "Tesla forecast", "TSLA prediction", "Tesla revenue", "Tesla results", "Tesla call", "TSLA DD", "Tesla outlook", "when Tesla earnings", "TSLA vs", "Tesla profit"]
            - For "Apple AI": ["Apple", "AAPL", "Apple AI", "Apple artificial intelligence", "Apple ML", "Apple GPT", "Apple Siri", "AAPL AI", "Apple vs Google AI", "Apple AI news", "when Apple AI", "Apple AI chip"]
            
            Generate comprehensive keywords for: "{english_query}"
            Return as JSON array:
            """
            
            response = await self.provider.generate(
                prompt=prompt,
                system_prompt="You are a keyword expansion expert.",
                temperature=0.7,
                max_tokens=200
            )
            
            content = response.content
            logger.info(f"   LLM ì‘ë‹µ ìˆ˜ì‹  (ê¸¸ì´: {len(content)})")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # ì½”ë“œ ë¸”ë¡ ì œê±°
                if '```json' in content:
                    content = content.replace('```json', '').replace('```', '').strip()
                
                keywords = json.loads(content)
                if isinstance(keywords, list):
                    result = keywords[:5]  # ìµœëŒ€ 5ê°œ
                    logger.info(f"âœ… í‚¤ì›Œë“œ í™•ì¥ ì™„ë£Œ: {len(result)}ê°œ - {result}")
                    return result
            except json.JSONDecodeError:
                logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸: {content[:200]}...")
            
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ í‚¤ì›Œë“œë§Œ ë°˜í™˜
            logger.warning(f"âš ï¸ í‚¤ì›Œë“œ í™•ì¥ ì‹¤íŒ¨, ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
            return []
            
        except Exception as e:
            logger.error(f"âŒ í‚¤ì›Œë“œ í™•ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            logger.error(f"   Provider: {self.provider.provider_name}")
            logger.error(f"   Model: {self.provider.default_model}")
            import traceback
            logger.error(f"   Stack trace:\n{traceback.format_exc()}")
            return []  # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
    
    async def generate_report(self, posts: List[Dict[str, Any]], query: str, length: ReportLength) -> Dict[str, Any]:
        """ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        try:
            logger.info(f"ğŸ“ ë³´ê³ ì„œ ìƒì„± ì‹œì‘ - í‚¤ì›Œë“œ: '{query}', ê¸¸ì´: {length.value}, ê²Œì‹œë¬¼ ìˆ˜: {len(posts)}")
            
            # ê²Œì‹œë¬¼ ì •ë³´ í¬ë§·íŒ…
            posts_text = self._format_posts_for_prompt(posts[:30])  # ìµœëŒ€ 30ê°œ ê²Œì‹œë¬¼
            logger.info(f"ğŸ“„ ê²Œì‹œë¬¼ í¬ë§·íŒ… ì™„ë£Œ - {min(len(posts), 30)}ê°œ ê²Œì‹œë¬¼ ì‚¬ìš©")
            
            # ë³´ê³ ì„œ ê¸¸ì´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
            length_guide = {
                ReportLength.simple: "ê°„ë‹¨íˆ 3-5 ë¬¸ì¥ìœ¼ë¡œ",
                ReportLength.moderate: "ì ë‹¹íˆ ìƒì„¸í•˜ê²Œ 2-3 ë‹¨ë½ìœ¼ë¡œ", 
                ReportLength.detailed: "ë§¤ìš° ìƒì„¸í•˜ê²Œ ê° ì„¹ì…˜ë³„ë¡œ"
            }
            
            prompt = f"""You are a professional community analyst. The following are social media posts collected with the keyword '{query}'.

{posts_text}

Based on this English data, create a comprehensive analysis report in KOREAN following these guidelines:

Length: {length_guide[length]}

Required sections (write all section headers and content in Korean):

1. **í•µì‹¬ ìš”ì•½**: Summarize the key findings
2. **ì£¼ìš” í† í”½**: Categorize and explain main topics discussed
3. **ì»¤ë®¤ë‹ˆí‹° ë°˜ì‘**: Analyze positive/negative sentiment ratios with evidence
4. **ì¸ìƒì ì¸ ì˜ê²¬**: Highlight 2-3 most notable opinions or insights
5. **ì¢…í•© ë¶„ì„**: Overall community perspective and trends

**CRITICAL FOOTNOTE REQUIREMENTS:**
- When referencing specific posts or opinions, you MUST use the exact format [ref:POST_ID] where POST_ID is the Reddit post ID from the data
- Example: "ë§ì€ ì‚¬ìš©ìë“¤ì´ ë°°í„°ë¦¬ ë¬¸ì œë¥¼ ì§€ì í–ˆìŠµë‹ˆë‹¤ [ref:t3_abc123]. íŠ¹íˆ í•œ ì‚¬ìš©ìëŠ” ì„±ëŠ¥ì´ 50% ì €í•˜ë˜ì—ˆë‹¤ê³  ë³´ê³ í–ˆìŠµë‹ˆë‹¤ [ref:t3_def456]."
- Use [ref:POST_ID] markers for:
  - Direct quotes from posts
  - Specific statistics or claims
  - Notable opinions or insights
  - Any fact that comes from a specific post
- You can use multiple references in one sentence: [ref:id1][ref:id2]
- These markers will be converted to numbered footnotes later, so use them liberally

DO NOT create a References section - the system will handle that automatically.

Important: 
- The input data is in English, but write the ENTIRE report in Korean
- Use markdown format
- Maintain objective and balanced perspective
- Translate key terms appropriately into Korean
- MUST include [ref:POST_ID] markers when referencing specific posts
"""
            
            logger.info(f"ğŸ¤– {self.provider.provider_name} API í˜¸ì¶œ ì‹œì‘...")
            
            response = await self.provider.generate(
                prompt=prompt,
                system_prompt="You are a professional community analyst who creates insightful reports in Korean.",
                temperature=0.7,
                max_tokens=2000 if length == ReportLength.detailed else 1000
            )
            
            full_report = response.content
            logger.info(f"âœ… {self.provider.provider_name} API ì‘ë‹µ ìˆ˜ì‹  - ë³´ê³ ì„œ ê¸¸ì´: {len(full_report)} ë¬¸ì")
            
            # ê°ì£¼ ë§¤í•‘ ì¶”ì¶œ (ë³€í™˜ ì „)
            footnote_mapping = self._extract_footnote_mapping(full_report, posts)
            
            # [ref:POST_ID]ë¥¼ ë²ˆí˜¸ë¡œ ë³€í™˜
            logger.info("ğŸ”„ ê°ì£¼ ë³€í™˜ ì‹œì‘...")
            processed_report = self._convert_refs_to_footnotes(full_report, footnote_mapping)
            logger.info(f"âœ… ê°ì£¼ ë³€í™˜ ì™„ë£Œ - {len(footnote_mapping)}ê°œ ê°ì£¼ ì²˜ë¦¬")
            
            # ìš”ì•½ ìƒì„± (í•œê¸€) - ë³€í™˜ëœ ë³´ê³ ì„œ ì‚¬ìš©
            logger.info("ğŸ“ ìš”ì•½ ìƒì„± ì‹œì‘...")
            summary_prompt = f"ë‹¤ìŒ í•œêµ­ì–´ ë³´ê³ ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{processed_report[:1000]}"
            
            summary_response = await self.provider.generate(
                prompt=summary_prompt,
                system_prompt="You are a summarization expert.",
                temperature=0.5,
                max_tokens=200
            )
            
            summary = summary_response.content
            logger.info(f"âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ - {len(summary)} ë¬¸ì")
            
            logger.info(f"ğŸ‰ AI ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
            logger.info(f"   - ì „ì²´ ë³´ê³ ì„œ: {len(processed_report)} ë¬¸ì")
            logger.info(f"   - ìš”ì•½: {len(summary)} ë¬¸ì")
            logger.info(f"   - ê°ì£¼ ìˆ˜: {len(footnote_mapping)}ê°œ")
            
            return {
                "summary": summary,
                "full_report": processed_report,
                "footnote_mapping": footnote_mapping
            }
            
        except Exception as e:
            logger.error(f"{self.provider.provider_name} API error in generate_report: {str(e)}")
            raise OpenAIAPIException(f"Failed to generate report: {str(e)}")
    
    def _format_posts_for_prompt(self, posts: List[Dict[str, Any]]) -> str:
        """ê²Œì‹œë¬¼ì„ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        formatted_posts = []
        
        for i, post in enumerate(posts, 1):
            # ê°œì„ ëœ í¬ë§·íŒ…ì— ë£¨ë¨¸ ì ìˆ˜ì™€ ìˆ˜ì§‘ ë²¡í„° ì •ë³´ í¬í•¨
            vector_info = post.get('collection_vector', 'unknown')
            rumor_score = post.get('rumor_score', 0)
            linguistic_flags = post.get('linguistic_flags', [])
            
            post_text = f"""[ê²Œì‹œë¬¼ {i}]
POST_ID: {post['id']}
ì œëª©: {post['title']}
ì ìˆ˜: {post['score']} | ëŒ“ê¸€: {post['num_comments']} | ë£¨ë¨¸ì ìˆ˜: {rumor_score}/10
ì„œë¸Œë ˆë”§: r/{post['subreddit']} | ìˆ˜ì§‘ë²¡í„°: {vector_info}
ì–¸ì–´ì‹ í˜¸: {', '.join(linguistic_flags) if linguistic_flags else 'ì—†ìŒ'}
ë‚´ìš©: {post['selftext'][:200] if post['selftext'] else '(ë‚´ìš© ì—†ìŒ)'}
---"""
            formatted_posts.append(post_text)
        
        logger.debug(f"ğŸ“„ ê²Œì‹œë¬¼ í¬ë§·íŒ…: {len(formatted_posts)}ê°œ ê²Œì‹œë¬¼")
        return "\n".join(formatted_posts)
    
    def _extract_footnote_mapping(self, report: str, posts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë³´ê³ ì„œì—ì„œ ê°ì£¼ ë§¤í•‘ ì¶”ì¶œ ë° [ref:POST_ID]ë¥¼ ë²ˆí˜¸ë¡œ ë³€í™˜"""
        import re
        
        footnote_mapping = []
        ref_to_footnote = {}  # POST_ID -> footnote_number ë§¤í•‘
        
        # [ref:POST_ID] íŒ¨í„´ ì°¾ê¸°
        ref_pattern = r'\[ref:([^\]]+)\]'
        refs = re.findall(ref_pattern, report)
        
        if not refs:
            logger.info("ğŸ“„ ì°¸ì¡°ê°€ ë°œê²¬ë˜ì§€ ì•ŠìŒ")
            return footnote_mapping
        
        logger.info(f"ğŸ”— ì°¸ì¡° ë°œê²¬: {len(refs)}ê°œ (ê³ ìœ : {len(set(refs))}ê°œ)")
        
        # ê³ ìœ í•œ POST_IDë“¤ì„ ì¶”ì¶œí•˜ê³  ë²ˆí˜¸ í• ë‹¹
        unique_refs = []
        for ref in refs:
            if ref not in ref_to_footnote:
                unique_refs.append(ref)
                ref_to_footnote[ref] = len(unique_refs)
        
        # ê° ê³ ìœ í•œ ì°¸ì¡°ì— ëŒ€í•´ ê²Œì‹œë¬¼ ì •ë³´ ì°¾ê¸°
        posts_by_id = {post['id']: post for post in posts}
        
        for post_id, footnote_number in ref_to_footnote.items():
            if post_id in posts_by_id:
                post = posts_by_id[post_id]
                footnote_mapping.append({
                    "footnote_number": footnote_number,
                    "post_id": post['id'],
                    "url": post['url'],
                    "title": post['title'],
                    "score": post['score'],
                    "comments": post['num_comments'],
                    "created_utc": post['created_utc'],
                    "subreddit": post['subreddit'],
                    "author": post['author'],
                    "position_in_report": footnote_number
                })
            else:
                logger.warning(f"âš ï¸ ì°¸ì¡°ëœ POST_IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {post_id}")
        
        # ê°ì£¼ ë²ˆí˜¸ìˆœìœ¼ë¡œ ì •ë ¬
        footnote_mapping.sort(key=lambda x: x['footnote_number'])
        
        logger.info(f"ğŸ”— ê°ì£¼ ë§¤í•‘ ì™„ë£Œ: {len(footnote_mapping)}ê°œ")
        return footnote_mapping
    
    def _convert_refs_to_footnotes(self, report: str, footnote_mapping: List[Dict[str, Any]]) -> str:
        """[ref:POST_ID] ë§ˆì»¤ë¥¼ ë²ˆí˜¸ ê°ì£¼ [1], [2] ë“±ìœ¼ë¡œ ë³€í™˜"""
        import re
        
        # footnote_mappingì—ì„œ post_id -> footnote_number ë§¤í•‘ ìƒì„±
        post_id_to_footnote = {
            item['post_id']: item['footnote_number'] 
            for item in footnote_mapping
        }
        
        # ëª¨ë“  [ref:POST_ID] íŒ¨í„´ì„ ì°¾ì•„ ë²ˆí˜¸ë¡œ ë³€í™˜
        def replace_ref(match):
            post_id = match.group(1)
            if post_id in post_id_to_footnote:
                return f"[{post_id_to_footnote[post_id]}]"
            return match.group(0)  # ë§¤í•‘ì´ ì—†ìœ¼ë©´ ì›ë³¸ ìœ ì§€
        
        processed_report = re.sub(r'\[ref:([^\]]+)\]', replace_ref, report)
        
        # ë³´ê³ ì„œ ëì— ì°¸ì¡° ëª©ë¡ ì¶”ê°€
        if footnote_mapping:
            processed_report += "\n\n## ì°¸ì¡° ëª©ë¡\n\n"
            for item in footnote_mapping:
                processed_report += f"[{item['footnote_number']}] {item['title']} - r/{item['subreddit']} (ì ìˆ˜: {item['score']}, ëŒ“ê¸€: {item['comments']})\n"
        
        return processed_report

    async def _call_llm(self, prompt: str, temperature: float = 0.7) -> str:
        """ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œ - ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€"""
        response = await self.provider.generate(
            prompt=prompt,
            system_prompt="You are a professional analyst and expert writer.",
            temperature=temperature,
            max_tokens=4000
        )
        return response.content