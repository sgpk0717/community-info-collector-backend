from typing import List, Dict, Any, Optional
from openai import OpenAI
from app.core.exceptions import OpenAIAPIException
from app.schemas.search import ReportLength
import logging
import json

logger = logging.getLogger(__name__)

class LLMService:
    def __init__(self):
        self.client = OpenAI()  # OpenAI 1.58.1 ë°©ì‹
    
    async def translate_to_english(self, query: str) -> str:
        """í•œê¸€ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"""
        try:
            prompt = f"""Translate the following Korean keyword to English. 
            If it's already in English, return as is.
            Only return the translated text, nothing else.
            
            Keyword: {query}
            """
            
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a professional translator."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=100
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"Translation error: {str(e)}")
            return query  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    async def expand_keywords(self, query: str) -> List[str]:
        """ì£¼ì–´ì§„ í‚¤ì›Œë“œë¥¼ í™•ì¥í•˜ì—¬ ê´€ë ¨ ê²€ìƒ‰ì–´ ìƒì„± (ì˜ì–´)"""
        try:
            # ë¨¼ì € ì˜ì–´ë¡œ ë²ˆì—­
            english_query = await self.translate_to_english(query)
            logger.info(f"Translated query: {query} -> {english_query}")
            
            prompt = f"""Generate 5 related search keywords for: "{english_query}"
            
            Requirements:
            1. All keywords must be in English
            2. Cover different aspects (technical, business, social, future trends)
            3. Be specific and relevant to the original keyword
            4. Return as JSON array only
            
            Example format: ["keyword1", "keyword2", "keyword3", "keyword4", "keyword5"]
            """
            
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a keyword expansion expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=200
            )
            
            content = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                keywords = json.loads(content)
                if isinstance(keywords, list):
                    return keywords[:5]  # ìµœëŒ€ 5ê°œ
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse keywords JSON: {content}")
            
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ í‚¤ì›Œë“œë§Œ ë°˜í™˜
            return []
            
        except Exception as e:
            logger.error(f"OpenAI API error in expand_keywords: {str(e)}")
            return []  # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
    
    async def generate_report(self, posts: List[Dict[str, Any]], query: str, length: ReportLength) -> Dict[str, Any]:
        """ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        try:
            logger.info(f"ğŸ“ ë³´ê³ ì„œ ìƒì„± ì‹œì‘ - í‚¤ì›Œë“œ: '{query}', ê¸¸ì´: {length.value}, ê²Œì‹œë¬¼ ìˆ˜: {len(posts)}")
            
            # ê²Œì‹œë¬¼ ì •ë³´ í¬ë§·íŒ…
            posts_text = self._format_posts_for_prompt(posts[:30])  # ìµœëŒ€ 30ê°œ ê²Œì‹œë¬¼
            logger.info(f"ğŸ“„ ê²Œì‹œë¬¼ í¬ë§·íŒ… ì™„ë£Œ - {min(len(posts), 30)}ê°œ ê²Œì‹œë¬¼ ì‚¬ìš©")
            
            # ë³´ê³ ì„œ ê¸¸ì´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
            length_guide = {
                ReportLength.simple: "ê°„ë‹¨íˆ 3-5 ë¬¸ì¥ìœ¼ë¡œ",
                ReportLength.moderate: "ì ë‹¹íˆ ìƒì„¸í•˜ê²Œ 2-3 ë‹¨ë½ìœ¼ë¡œ", 
                ReportLength.detailed: "ë§¤ìš° ìƒì„¸í•˜ê²Œ ê° ì„¹ì…˜ë³„ë¡œ"
            }
            
            prompt = f"""You are a professional community analyst. The following are social media posts collected with the keyword '{query}'.

{posts_text}

Based on this English data, create a comprehensive analysis report in KOREAN following these guidelines:

Length: {length_guide[length]}

Required sections (write all section headers and content in Korean):

1. **í•µì‹¬ ìš”ì•½**: Summarize the key findings
2. **ì£¼ìš” í† í”½**: Categorize and explain main topics discussed
3. **ì»¤ë®¤ë‹ˆí‹° ë°˜ì‘**: Analyze positive/negative sentiment ratios with evidence
4. **ì¸ìƒì ì¸ ì˜ê²¬**: Highlight 2-3 most notable opinions or insights
5. **ì¢…í•© ë¶„ì„**: Overall community perspective and trends

**CRITICAL FOOTNOTE REQUIREMENTS:**
- When referencing specific posts or opinions, you MUST use the exact format [ref:POST_ID] where POST_ID is the Reddit post ID from the data
- Example: "ë§ì€ ì‚¬ìš©ìë“¤ì´ ë°°í„°ë¦¬ ë¬¸ì œë¥¼ ì§€ì í–ˆìŠµë‹ˆë‹¤ [ref:t3_abc123]. íŠ¹íˆ í•œ ì‚¬ìš©ìëŠ” ì„±ëŠ¥ì´ 50% ì €í•˜ë˜ì—ˆë‹¤ê³  ë³´ê³ í–ˆìŠµë‹ˆë‹¤ [ref:t3_def456]."
- Use [ref:POST_ID] markers for:
  - Direct quotes from posts
  - Specific statistics or claims
  - Notable opinions or insights
  - Any fact that comes from a specific post
- You can use multiple references in one sentence: [ref:id1][ref:id2]
- These markers will be converted to numbered footnotes later, so use them liberally

DO NOT create a References section - the system will handle that automatically.

Important: 
- The input data is in English, but write the ENTIRE report in Korean
- Use markdown format
- Maintain objective and balanced perspective
- Translate key terms appropriately into Korean
- MUST include [ref:POST_ID] markers when referencing specific posts
"""
            
            logger.info("ğŸ¤– OpenAI API í˜¸ì¶œ ì‹œì‘...")
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a professional community analyst who creates insightful reports in Korean."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=2000 if length == ReportLength.detailed else 1000
            )
            
            full_report = response.choices[0].message.content.strip()
            logger.info(f"âœ… OpenAI API ì‘ë‹µ ìˆ˜ì‹  - ë³´ê³ ì„œ ê¸¸ì´: {len(full_report)} ë¬¸ì")
            
            # ê°ì£¼ ë§¤í•‘ ì¶”ì¶œ (ë³€í™˜ ì „)
            footnote_mapping = self._extract_footnote_mapping(full_report, posts)
            
            # [ref:POST_ID]ë¥¼ ë²ˆí˜¸ë¡œ ë³€í™˜
            logger.info("ğŸ”„ ê°ì£¼ ë³€í™˜ ì‹œì‘...")
            processed_report = self._convert_refs_to_footnotes(full_report, footnote_mapping)
            logger.info(f"âœ… ê°ì£¼ ë³€í™˜ ì™„ë£Œ - {len(footnote_mapping)}ê°œ ê°ì£¼ ì²˜ë¦¬")
            
            # ìš”ì•½ ìƒì„± (í•œê¸€) - ë³€í™˜ëœ ë³´ê³ ì„œ ì‚¬ìš©
            logger.info("ğŸ“ ìš”ì•½ ìƒì„± ì‹œì‘...")
            summary_prompt = f"ë‹¤ìŒ í•œêµ­ì–´ ë³´ê³ ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{processed_report[:1000]}"
            
            summary_response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a summarization expert."},
                    {"role": "user", "content": summary_prompt}
                ],
                temperature=0.5,
                max_tokens=200
            )
            
            summary = summary_response.choices[0].message.content.strip()
            logger.info(f"âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ - {len(summary)} ë¬¸ì")
            
            logger.info(f"ğŸ‰ AI ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
            logger.info(f"   - ì „ì²´ ë³´ê³ ì„œ: {len(processed_report)} ë¬¸ì")
            logger.info(f"   - ìš”ì•½: {len(summary)} ë¬¸ì")
            logger.info(f"   - ê°ì£¼ ìˆ˜: {len(footnote_mapping)}ê°œ")
            
            return {
                "summary": summary,
                "full_report": processed_report,
                "footnote_mapping": footnote_mapping
            }
            
        except Exception as e:
            logger.error(f"OpenAI API error in generate_report: {str(e)}")
            raise OpenAIAPIException(f"Failed to generate report: {str(e)}")
    
    def _format_posts_for_prompt(self, posts: List[Dict[str, Any]]) -> str:
        """ê²Œì‹œë¬¼ì„ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        formatted_posts = []
        
        for i, post in enumerate(posts, 1):
            # ê°œì„ ëœ í¬ë§·íŒ…ì— ë£¨ë¨¸ ì ìˆ˜ì™€ ìˆ˜ì§‘ ë²¡í„° ì •ë³´ í¬í•¨
            vector_info = post.get('collection_vector', 'unknown')
            rumor_score = post.get('rumor_score', 0)
            linguistic_flags = post.get('linguistic_flags', [])
            
            post_text = f"""[ê²Œì‹œë¬¼ {i}]
POST_ID: {post['id']}
ì œëª©: {post['title']}
ì ìˆ˜: {post['score']} | ëŒ“ê¸€: {post['num_comments']} | ë£¨ë¨¸ì ìˆ˜: {rumor_score}/10
ì„œë¸Œë ˆë”§: r/{post['subreddit']} | ìˆ˜ì§‘ë²¡í„°: {vector_info}
ì–¸ì–´ì‹ í˜¸: {', '.join(linguistic_flags) if linguistic_flags else 'ì—†ìŒ'}
ë‚´ìš©: {post['selftext'][:200] if post['selftext'] else '(ë‚´ìš© ì—†ìŒ)'}
---"""
            formatted_posts.append(post_text)
        
        logger.debug(f"ğŸ“„ ê²Œì‹œë¬¼ í¬ë§·íŒ…: {len(formatted_posts)}ê°œ ê²Œì‹œë¬¼")
        return "\n".join(formatted_posts)
    
    def _extract_footnote_mapping(self, report: str, posts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë³´ê³ ì„œì—ì„œ ê°ì£¼ ë§¤í•‘ ì¶”ì¶œ ë° [ref:POST_ID]ë¥¼ ë²ˆí˜¸ë¡œ ë³€í™˜"""
        import re
        
        footnote_mapping = []
        ref_to_footnote = {}  # POST_ID -> footnote_number ë§¤í•‘
        
        # [ref:POST_ID] íŒ¨í„´ ì°¾ê¸°
        ref_pattern = r'\[ref:([^\]]+)\]'
        refs = re.findall(ref_pattern, report)
        
        if not refs:
            logger.info("ğŸ“„ ì°¸ì¡°ê°€ ë°œê²¬ë˜ì§€ ì•ŠìŒ")
            return footnote_mapping
        
        logger.info(f"ğŸ”— ì°¸ì¡° ë°œê²¬: {len(refs)}ê°œ (ê³ ìœ : {len(set(refs))}ê°œ)")
        
        # ê³ ìœ í•œ POST_IDë“¤ì„ ì¶”ì¶œí•˜ê³  ë²ˆí˜¸ í• ë‹¹
        unique_refs = []
        for ref in refs:
            if ref not in ref_to_footnote:
                unique_refs.append(ref)
                ref_to_footnote[ref] = len(unique_refs)
        
        # ê° ê³ ìœ í•œ ì°¸ì¡°ì— ëŒ€í•´ ê²Œì‹œë¬¼ ì •ë³´ ì°¾ê¸°
        posts_by_id = {post['id']: post for post in posts}
        
        for post_id, footnote_number in ref_to_footnote.items():
            if post_id in posts_by_id:
                post = posts_by_id[post_id]
                footnote_mapping.append({
                    "footnote_number": footnote_number,
                    "post_id": post['id'],
                    "url": post['url'],
                    "title": post['title'],
                    "score": post['score'],
                    "comments": post['num_comments'],
                    "created_utc": post['created_utc'],
                    "subreddit": post['subreddit'],
                    "author": post['author'],
                    "position_in_report": footnote_number
                })
            else:
                logger.warning(f"âš ï¸ ì°¸ì¡°ëœ POST_IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {post_id}")
        
        # ê°ì£¼ ë²ˆí˜¸ìˆœìœ¼ë¡œ ì •ë ¬
        footnote_mapping.sort(key=lambda x: x['footnote_number'])
        
        logger.info(f"ğŸ”— ê°ì£¼ ë§¤í•‘ ì™„ë£Œ: {len(footnote_mapping)}ê°œ")
        return footnote_mapping
    
    def _convert_refs_to_footnotes(self, report: str, footnote_mapping: List[Dict[str, Any]]) -> str:
        """[ref:POST_ID] ë§ˆì»¤ë¥¼ ë²ˆí˜¸ ê°ì£¼ [1], [2] ë“±ìœ¼ë¡œ ë³€í™˜"""
        import re
        
        # footnote_mappingì—ì„œ post_id -> footnote_number ë§¤í•‘ ìƒì„±
        post_id_to_footnote = {
            item['post_id']: item['footnote_number'] 
            for item in footnote_mapping
        }
        
        # ëª¨ë“  [ref:POST_ID] íŒ¨í„´ì„ ì°¾ì•„ ë²ˆí˜¸ë¡œ ë³€í™˜
        def replace_ref(match):
            post_id = match.group(1)
            if post_id in post_id_to_footnote:
                return f"[{post_id_to_footnote[post_id]}]"
            return match.group(0)  # ë§¤í•‘ì´ ì—†ìœ¼ë©´ ì›ë³¸ ìœ ì§€
        
        processed_report = re.sub(r'\[ref:([^\]]+)\]', replace_ref, report)
        
        # ë³´ê³ ì„œ ëì— ì°¸ì¡° ëª©ë¡ ì¶”ê°€
        if footnote_mapping:
            processed_report += "\n\n## ì°¸ì¡° ëª©ë¡\n\n"
            for item in footnote_mapping:
                processed_report += f"[{item['footnote_number']}] {item['title']} - r/{item['subreddit']} (ì ìˆ˜: {item['score']}, ëŒ“ê¸€: {item['comments']})\n"
        
        return processed_report