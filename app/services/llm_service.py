from typing import List, Dict, Any, Optional, Literal, Tuple
from app.core.exceptions import OpenAIAPIException
from app.schemas.search import ReportLength
from app.services.llm_providers import BaseLLMProvider, OpenAIProvider, GeminiProvider
import logging
import json
import os
import asyncio
from datetime import datetime

logger = logging.getLogger(__name__)

# Provider íƒ€ì… ì •ì˜
LLMProviderType = Literal["openai", "gemini"]


class LLMService:
    """ë‹¤ì¤‘ LLM Providerë¥¼ ì§€ì›í•˜ëŠ” í†µí•© LLM Service"""
    
    def __init__(self, provider_type: Optional[LLMProviderType] = None, api_semaphore: Optional[asyncio.Semaphore] = None):
        """
        LLMService ì´ˆê¸°í™”
        
        Args:
            provider_type: ì‚¬ìš©í•  LLM provider ("openai" ë˜ëŠ” "gemini")
                          Noneì¸ ê²½ìš° í™˜ê²½ë³€ìˆ˜ LLM_PROVIDERì—ì„œ ì½ìŒ (ê¸°ë³¸ê°’: "openai")
        """
        # Provider íƒ€ì… ê²°ì •
        if provider_type is None:
            provider_type = os.getenv('LLM_PROVIDER', 'openai').lower()
        
        # Provider ì´ˆê¸°í™”
        self.api_semaphore = api_semaphore
        self.provider = self._initialize_provider(provider_type)
        logger.info(f"LLMService ì´ˆê¸°í™” ì™„ë£Œ - Provider: {self.provider.provider_name}, Model: {self.provider.default_model}")
    
    def _initialize_provider(self, provider_type: str) -> BaseLLMProvider:
        """Provider íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ provider ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        if provider_type == "openai":
            return OpenAIProvider(api_semaphore=self.api_semaphore)
        elif provider_type == "gemini":
            return GeminiProvider()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” provider íƒ€ì…: {provider_type}")
    
    async def translate_to_english(self, query: str) -> str:
        """í•œê¸€ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"""
        logger.info(f"ğŸŒ ë²ˆì—­ ì‹œì‘: '{query}'")
        try:
            prompt = f"""Translate the following Korean search query to English for Reddit search.
            
            Rules:
            1. If it's already in English, return as is
            2. Translate company/brand names to their official English names
            3. Keep the search intent clear and specific
            4. Use common English terms that Reddit users would use
            5. Only return the translated text, nothing else
            
            Examples:
            - "êµ¬ê¸€ ì‹¤ì ë°œí‘œ ì˜ˆì¸¡" â†’ "Google earnings prediction"
            - "í…ŒìŠ¬ë¼ ììœ¨ì£¼í–‰ ê¸°ìˆ " â†’ "Tesla autonomous driving technology"
            - "ì‚¼ì„± ì‹ ì œí’ˆ ë£¨ë¨¸" â†’ "Samsung new product rumors"
            
            Keyword: {query}
            Translation:
            """
            
            response = await self.provider.generate(
                prompt=prompt,
                system_prompt="You are a professional translator.",
                temperature=0.3,
                max_tokens=100
            )
            
            translated = response.content.strip()
            logger.info(f"âœ… ë²ˆì—­ ì™„ë£Œ: '{query}' â†’ '{translated}'")
            return translated
            
        except Exception as e:
            logger.error(f"âŒ ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"   Provider: {self.provider.provider_name}")
            logger.error(f"   Model: {self.provider.default_model}")
            logger.error(f"   Query: '{query}'")
            import traceback
            logger.error(f"   Stack trace:\n{traceback.format_exc()}")
            return query  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    async def expand_keywords(self, query: str) -> List[str]:
        """ì£¼ì–´ì§„ í‚¤ì›Œë“œë¥¼ í™•ì¥í•˜ì—¬ ê´€ë ¨ ê²€ìƒ‰ì–´ ìƒì„± (ì˜ì–´)"""
        logger.info(f"ğŸ” í‚¤ì›Œë“œ í™•ì¥ ì‹œì‘: '{query}'")
        try:
            # ë¨¼ì € ì˜ì–´ë¡œ ë²ˆì—­
            english_query = await self.translate_to_english(query)
            logger.info(f"   ë²ˆì—­ëœ ì¿¼ë¦¬: '{english_query}'")
            
            prompt = f"""Extract ALL effective search keywords for Reddit about: "{english_query}"
            
            SEARCH STRATEGY RULES:
            1. Generate SHORT, HIGH-IMPACT keywords (1-3 words preferred)
            2. Include multiple variations:
               - Main topic alone (e.g., "Google")
               - Topic + action words (e.g., "Google earnings", "GOOGL forecast")
               - Stock symbols if applicable (e.g., "GOOGL", "GOOG")
               - Common abbreviations and full names
               - Singular AND plural forms
               - Present AND future tense variations
            3. Focus on HIGH-INTENT search patterns:
               - Questions: "how", "what", "when" + topic
               - Comparisons: "vs", "versus", "or"
               - Opinions: "best", "worst", "review"
               - Predictions: "forecast", "prediction", "outlook"
            4. Include Reddit-specific terms:
               - DD (Due Diligence)
               - YOLO, calls, puts (for stock-related)
               - ELI5 (Explain Like I'm 5)
            5. Extract 10-20 keywords to maximize coverage
            
            Examples:
            - For "Tesla earnings prediction": ["Tesla", "TSLA", "Tesla earnings", "TSLA earnings", "Tesla Q4", "Tesla forecast", "TSLA prediction", "Tesla revenue", "Tesla results", "Tesla call", "TSLA DD", "Tesla outlook", "when Tesla earnings", "TSLA vs", "Tesla profit"]
            - For "Apple AI": ["Apple", "AAPL", "Apple AI", "Apple artificial intelligence", "Apple ML", "Apple GPT", "Apple Siri", "AAPL AI", "Apple vs Google AI", "Apple AI news", "when Apple AI", "Apple AI chip"]
            
            Generate comprehensive keywords for: "{english_query}"
            Return as JSON array:
            """
            
            response = await self.provider.generate(
                prompt=prompt,
                system_prompt="You are a keyword expansion expert.",
                temperature=0.7,
                max_tokens=200
            )
            
            content = response.content
            logger.info(f"   LLM ì‘ë‹µ ìˆ˜ì‹  (ê¸¸ì´: {len(content)})")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # ì½”ë“œ ë¸”ë¡ ì œê±°
                if '```json' in content:
                    content = content.replace('```json', '').replace('```', '').strip()
                
                keywords = json.loads(content)
                if isinstance(keywords, list):
                    result = keywords  # ì „ì²´ ì‚¬ìš© (ì œí•œ í•´ì œ)
                    logger.info(f"âœ… í‚¤ì›Œë“œ í™•ì¥ ì™„ë£Œ: {len(result)}ê°œ - {result}")
                    return result
            except json.JSONDecodeError:
                logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸: {content[:200]}...")
            
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ í‚¤ì›Œë“œë§Œ ë°˜í™˜
            logger.warning(f"âš ï¸ í‚¤ì›Œë“œ í™•ì¥ ì‹¤íŒ¨, ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
            return []
            
        except Exception as e:
            logger.error(f"âŒ í‚¤ì›Œë“œ í™•ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            logger.error(f"   Provider: {self.provider.provider_name}")
            logger.error(f"   Model: {self.provider.default_model}")
            import traceback
            logger.error(f"   Stack trace:\n{traceback.format_exc()}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ë¥¼ ì „íŒŒ
            raise Exception(f"í‚¤ì›Œë“œ í™•ì¥ ì‹¤íŒ¨: {str(e)}")
    
    async def generate_report(self, posts: List[Dict[str, Any]], query: str, length: ReportLength, cluster_info: Optional[Dict[str, Any]] = None, time_filter: Optional[str] = None) -> Dict[str, Any]:
        """ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        try:
            logger.info(f"ğŸ“ ë³´ê³ ì„œ ìƒì„± ì‹œì‘ - í‚¤ì›Œë“œ: '{query}', ê¸¸ì´: {length.value}, ê²Œì‹œë¬¼ ìˆ˜: {len(posts)}")
            
            # ê²Œì‹œë¬¼ ì •ë³´ í¬ë§·íŒ… (ì¸ë±ìŠ¤ ë§¤í•‘ê³¼ í•¨ê»˜)
            posts_text, index_mapping = self._format_posts_for_prompt(posts[:30])  # ìµœëŒ€ 30ê°œ ê²Œì‹œë¬¼
            logger.info(f"ğŸ“„ ê²Œì‹œë¬¼ í¬ë§·íŒ… ì™„ë£Œ - {min(len(posts), 30)}ê°œ ê²Œì‹œë¬¼ ì‚¬ìš©")
            self._index_mapping = index_mapping  # ë‚˜ì¤‘ì— ê°ì£¼ ì²˜ë¦¬ì— ì‚¬ìš©í•  ë§¤í•‘ ì €ì¥
            
            # í´ëŸ¬ìŠ¤í„° ì •ë³´ í¬ë§·íŒ…
            cluster_text = ""
            if cluster_info and cluster_info.get('clusters'):
                cluster_text = self._format_cluster_info(cluster_info)
                logger.info(f"ğŸ¯ í´ëŸ¬ìŠ¤í„° ì •ë³´ í¬í•¨ - {len(cluster_info['clusters'])}ê°œ ì£¼ì œ")
            
            # ë³´ê³ ì„œ ê¸¸ì´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
            length_guide = {
                ReportLength.simple: "ê° ì„¹ì…˜ì„ 1-2 ë‹¨ë½ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ",
                ReportLength.moderate: "ê° ì„¹ì…˜ì„ 2-3 ë‹¨ë½ìœ¼ë¡œ ìƒì„¸í•˜ê²Œ", 
                ReportLength.detailed: "ê° ì„¹ì…˜ì„ 3-5 ë‹¨ë½ìœ¼ë¡œ ë§¤ìš° ìƒì„¸í•˜ê²Œ, êµ¬ì²´ì ì¸ ì‚¬ë¡€ì™€ ì¸ìš©ì„ í’ë¶€í•˜ê²Œ í¬í•¨"
            }
            
            # í´ëŸ¬ìŠ¤í„° ì •ë³´ê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
            cluster_section = ""
            if cluster_text:
                cluster_section = f"""
ì£¼ì œë³„ ë¶„ë¥˜ ì •ë³´:
{cluster_text}

ìœ„ì˜ ì£¼ì œë³„ ë¶„ë¥˜ë¥¼ ì°¸ê³ í•˜ì—¬ ë³´ê³ ì„œë¥¼ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.
"""
            
            # ì‹œê°„ í•„í„° ì •ë³´ ì¶”ê°€
            time_filter_text = ""
            if time_filter:
                time_filter_map = {
                    '1h': 'ìµœê·¼ 1ì‹œê°„',
                    '3h': 'ìµœê·¼ 3ì‹œê°„',
                    '6h': 'ìµœê·¼ 6ì‹œê°„',
                    '12h': 'ìµœê·¼ 12ì‹œê°„',
                    '1d': 'ìµœê·¼ 24ì‹œê°„(1ì¼)',
                    '3d': 'ìµœê·¼ 3ì¼',
                    '1w': 'ìµœê·¼ 1ì£¼ì¼',
                    '1m': 'ìµœê·¼ 1ê°œì›”'
                }
                time_period = time_filter_map.get(time_filter, 'ì „ì²´ ê¸°ê°„')
                time_filter_text = f"\n\nâš ï¸ ì¤‘ìš”: ëª¨ë“  ë¶„ì„ì€ {time_period} ë™ì•ˆì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ë³´ê³ ì„œì—ì„œ 'ìµœê·¼ 2ì£¼ê°„' ê°™ì€ ì˜ëª»ëœ ê¸°ê°„ í‘œí˜„ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ë°˜ë“œì‹œ '{time_period}' ë˜ëŠ” ì ì ˆí•œ ì‹œê°„ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”."
            
            prompt = f"""You are a professional community analyst. The following are social media posts collected with the keyword '{query}'.{time_filter_text}

{posts_text}
{cluster_section}
Based on this English data, create a HIGHLY DETAILED analysis report in KOREAN following these guidelines:

Report Length: {length_guide[length]}

Required sections (write all section headers and content in Korean):

## 1. í•µì‹¬ ìš”ì•½ (Executive Summary)
- ì „ì²´ ì»¤ë®¤ë‹ˆí‹° ë°˜ì‘ì˜ í•µì‹¬ì„ 2-3 ë‹¨ë½ìœ¼ë¡œ ìƒì„¸íˆ ìš”ì•½
- ê°€ì¥ ì¤‘ìš”í•œ ë°œê²¬ì‚¬í•­ 3-5ê°€ì§€ë¥¼ ëª…í™•íˆ ì œì‹œ
- ì „ë°˜ì ì¸ ì—¬ë¡  ë™í–¥ê³¼ í•µì‹¬ í†µê³„ í¬í•¨

## 2. ì£¼ìš” í† í”½ ë¶„ì„ (Topic Analysis)
- ë…¼ì˜ë˜ëŠ” ì£¼ìš” ì£¼ì œë¥¼ 5-7ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
- ê° í† í”½ë³„ë¡œ ìƒì„¸í•œ ì„¤ëª…ê³¼ êµ¬ì²´ì ì¸ ì˜ˆì‹œ í¬í•¨
- í† í”½ë³„ ë…¼ì˜ ë¹ˆë„ì™€ ì¤‘ìš”ë„ ë¶„ì„

## 3. ì»¤ë®¤ë‹ˆí‹° ë°˜ì‘ ë¶„ì„ (Sentiment Analysis)
- ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì˜ê²¬ì˜ êµ¬ì²´ì ì¸ ë¹„ìœ¨ ì œì‹œ
- ê° ê°ì •ë³„ ëŒ€í‘œì ì¸ ì˜ê²¬ë“¤ì„ ì›ë¬¸ê³¼ í•¨ê»˜ ì¸ìš©
- ê°ì • ë³€í™”ì˜ ì›ì¸ê³¼ ë§¥ë½ ë¶„ì„

## 4. ì£¼ëª©í•  ë§Œí•œ ì˜ê²¬ë“¤ (Notable Opinions)
- ê°€ì¥ ë§ì€ ê³µê°ì„ ë°›ì€ ì˜ê²¬ 5-7ê°œ ìƒì„¸ ë¶„ì„
- **âš ï¸ ë°˜ë“œì‹œ "ì˜ë¬¸ ì›ë¬¸" (í•œêµ­ì–´ ë²ˆì—­) í˜•ì‹ìœ¼ë¡œ ì¸ìš©**
- ì˜ˆì‹œ: "This is the future of AI" (ì´ê²ƒì´ AIì˜ ë¯¸ë˜ì…ë‹ˆë‹¤) [ref:123]
- **ëŒ“ê¸€ ì¸ìš© ì‹œ**: ë°˜ë“œì‹œ ì–´ë–¤ ê²Œì‹œê¸€ì— ë‹¬ë¦° ëŒ“ê¸€ì¸ì§€ ëª…ì‹œí•˜ê³ , ê²Œì‹œê¸€ì˜ ì œëª©ê³¼ í•µì‹¬ ë‚´ìš©ë„ í•¨ê»˜ ì„¤ëª…
- ì˜ˆì‹œ: "í…ŒìŠ¬ë¼ì˜ FSDê°€ ì™œ ì‹¤íŒ¨í•  ìˆ˜ë°–ì— ì—†ëŠ”ê°€"ë¼ëŠ” ê²Œì‹œê¸€ì— ë‹¬ë¦° ëŒ“ê¸€: "LiDAR is essential" (LiDARëŠ” í•„ìˆ˜ë‹¤) [ref:COMMENT_456]
- í•´ë‹¹ ì˜ê²¬ì´ ì£¼ëª©ë°›ëŠ” ì´ìœ ì™€ ë§¥ë½ ì„¤ëª…

## 5. êµ¬ì²´ì ì¸ ì‚¬ë¡€ì™€ ì¸ìš© (Specific Examples)
- ì‹¤ì œ ì‚¬ìš©ìë“¤ì˜ ìƒìƒí•œ ê²½í—˜ë‹´ 5-10ê°œ ì†Œê°œ
- **âš ï¸ ëª¨ë“  ì¸ìš©ì€ ë°˜ë“œì‹œ í˜•ì‹ ì¤€ìˆ˜: "ì˜ë¬¸" (í•œê¸€ ë²ˆì—­) [ref:ID]**
- ì˜¬ë°”ë¥¸ ì˜ˆ: "I tried it yesterday and it worked perfectly" (ì–´ì œ ì‹œë„í•´ë´¤ëŠ”ë° ì™„ë²½í•˜ê²Œ ì‘ë™í–ˆì–´ìš”) [ref:456]
- ì˜ëª»ëœ ì˜ˆ: "I tried it yesterday" [ref:456] â† ë²ˆì—­ ëˆ„ë½ âŒ

## 6. í†µê³„ì  ë¶„ì„ (Statistical Analysis)
- ê²Œì‹œë¬¼ ì‘ì„± ì‹œê°„ëŒ€ ë¶„í¬
- ê°€ì¥ í™œë°œí•œ ë…¼ì˜ê°€ ì´ë£¨ì–´ì§„ ì„œë¸Œë ˆë”§
- í‰ê·  ëŒ“ê¸€ ìˆ˜, ì¶”ì²œ ìˆ˜ ë“± ì°¸ì—¬ë„ ì§€í‘œ

## 7. ì¢…í•© ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ (Comprehensive Analysis)
- ìˆ˜ì§‘ëœ ë°ì´í„°ì—ì„œ ë„ì¶œí•  ìˆ˜ ìˆëŠ” ì‹¬ì¸µì  ì¸ì‚¬ì´íŠ¸
- í–¥í›„ ì „ë§ì´ë‚˜ ì˜ˆì¸¡ ê°€ëŠ¥í•œ íŠ¸ë Œë“œ
- ì£¼ëª©í•´ì•¼ í•  ì‹œì‚¬ì ê³¼ í•¨ì˜

**CRITICAL REQUIREMENTS:**
1. QUOTATION FORMAT: 
   âš ï¸ **ëª¨ë“  ì˜ë¬¸ ì¸ìš©ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ ë²ˆì—­ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤!**
   - ì˜¬ë°”ë¥¸ í˜•ì‹: "This is amazing!" (ì´ê²ƒì€ ë†€ë¼ì›Œìš”!) [ref:POST_ID]
   - ì˜¬ë°”ë¥¸ í˜•ì‹: "I can't believe this happened" (ì´ëŸ° ì¼ì´ ì¼ì–´ë‚¬ë‹¤ë‹ˆ ë¯¿ì„ ìˆ˜ ì—†ì–´ìš”) [ref:POST_ID]
   - ì˜ëª»ëœ í˜•ì‹: "This is amazing!" [ref:POST_ID] â† ë²ˆì—­ ì—†ìŒ âŒ
   - ê¸´ ì¸ìš©ë¬¸ë„ ë™ì¼í•œ ê·œì¹™ ì ìš©
   - ë¸”ë¡ ì¸ìš© ì‚¬ìš© ì‹œì—ë„ ë°˜ë“œì‹œ ë²ˆì—­ í¬í•¨

2. DETAIL LEVEL: 
   - Include SPECIFIC numbers, percentages, and statistics
   - Provide CONCRETE examples with full context
   - Use ACTUAL quotes from posts, not paraphrases
   - Include post metadata (upvotes, comments, subreddit) when relevant

3. FOOTNOTE REQUIREMENTS:
   - Use [ref:POST_ID] for EVERY claim, quote, or specific example
   - Multiple references allowed: [ref:id1][ref:id2]
   - Place references immediately after the relevant content

4. LANGUAGE:
   - Write the ENTIRE report in Korean
   - Keep English quotes in original form
   - **âš ï¸ ALWAYS provide Korean translations in parentheses after EVERY English quote**
   - ì ˆëŒ€ ë²ˆì—­ ì—†ì´ ì˜ë¬¸ë§Œ ì¸ìš©í•˜ì§€ ë§ˆì„¸ìš”!
   - Use appropriate Korean business/analytical terminology

5. MINIMUM CONTENT:
   - At least 10-15 direct quotes from posts
   - At least 20 [ref:POST_ID] citations throughout
   - Each section must be substantial and detailed
   - Total report should be comprehensive and thorough

Remember: This is a DETAILED analytical report, not a summary. Include as much relevant information as possible while maintaining clarity and organization."""
            
            # í”„ë¡¬í”„íŠ¸ë§Œ ë¡œê¹… (ë°ì´í„° ì œì™¸)
            prompt_preview = prompt.split('\n\n')[0] + "\n\n[ê²Œì‹œë¬¼ ë°ì´í„° ìƒëµ...]\n\n" + "\n\n".join(prompt.split('\n\n')[2:])
            logger.info(f"ğŸ¤– {self.provider.provider_name} API í˜¸ì¶œ ì‹œì‘...")
            logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸:\n{prompt_preview}")
            
            response = await self.provider.generate(
                prompt=prompt,
                system_prompt="You are a professional community analyst who creates comprehensive, detailed reports in Korean. Focus on providing rich content with specific examples and direct quotations.",
                temperature=0.7,
                max_tokens=4000 if length == ReportLength.detailed else 2500 if length == ReportLength.moderate else 1500
            )
            
            full_report = response.content
            logger.info(f"âœ… {self.provider.provider_name} API ì‘ë‹µ ìˆ˜ì‹  - ë³´ê³ ì„œ ê¸¸ì´: {len(full_report)} ë¬¸ì")
            
            # ê°ì£¼ ë§¤í•‘ ì¶”ì¶œ (ë³€í™˜ ì „) - ì¸ë±ìŠ¤ ë§¤í•‘ ì‚¬ìš©
            footnote_mapping = self._extract_footnote_mapping(full_report, posts, self._index_mapping)
            
            # [ref:POST_ID]ë¥¼ ë²ˆí˜¸ë¡œ ë³€í™˜
            logger.info("ğŸ”„ ê°ì£¼ ë³€í™˜ ì‹œì‘...")
            processed_report = self._convert_refs_to_footnotes(full_report, footnote_mapping)
            logger.info(f"âœ… ê°ì£¼ ë³€í™˜ ì™„ë£Œ - {len(footnote_mapping)}ê°œ ê°ì£¼ ì²˜ë¦¬")
            
            # ìš”ì•½ ìƒì„± (í•œê¸€) - ë³€í™˜ëœ ë³´ê³ ì„œ ì‚¬ìš©
            logger.info("ğŸ“ ìš”ì•½ ìƒì„± ì‹œì‘...")
            summary_prompt = f"ë‹¤ìŒ í•œêµ­ì–´ ë³´ê³ ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{processed_report[:1000]}"
            
            summary_response = await self.provider.generate(
                prompt=summary_prompt,
                system_prompt="You are a summarization expert.",
                temperature=0.5,
                max_tokens=200
            )
            
            summary = summary_response.content
            logger.info(f"âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ - {len(summary)} ë¬¸ì")
            
            logger.info(f"ğŸ‰ AI ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
            logger.info(f"   - ì „ì²´ ë³´ê³ ì„œ: {len(processed_report)} ë¬¸ì")
            logger.info(f"   - ìš”ì•½: {len(summary)} ë¬¸ì")
            logger.info(f"   - ê°ì£¼ ìˆ˜: {len(footnote_mapping)}ê°œ")
            
            return {
                "summary": summary,
                "full_report": processed_report,
                "footnote_mapping": footnote_mapping
            }
            
        except Exception as e:
            logger.error(f"{self.provider.provider_name} API error in generate_report: {str(e)}")
            raise OpenAIAPIException(f"Failed to generate report: {str(e)}")
    
    def _format_posts_for_prompt(self, posts: List[Dict[str, Any]]) -> Tuple[str, Dict[int, Dict[str, Any]]]:
        """ê²Œì‹œë¬¼ê³¼ ëŒ“ê¸€ì„ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ… (ì¸ë±ìŠ¤ ë§¤í•‘ê³¼ í•¨ê»˜ ë°˜í™˜)"""
        formatted_posts = []
        index_to_post = {}  # ì¸ë±ìŠ¤ -> ê²Œì‹œë¬¼ ë§¤í•‘
        
        # ê²Œì‹œë¬¼ IDë¡œ ë§¤í•‘ ìƒì„± (ëŒ“ê¸€ì—ì„œ ë¶€ëª¨ ê²Œì‹œë¬¼ ì°¸ì¡°ìš©)
        posts_by_id = {p.get('id'): p for p in posts if p.get('type') == 'post' and p.get('id')}
        
        for i, item in enumerate(posts, 1):
            # ê°œì„ ëœ í¬ë§·íŒ…ì— ë£¨ë¨¸ ì ìˆ˜ì™€ ìˆ˜ì§‘ ë²¡í„° ì •ë³´ í¬í•¨
            vector_info = item.get('collection_vector', 'unknown')
            rumor_score = item.get('rumor_score', 0)
            linguistic_flags = item.get('linguistic_flags', [])
            
            # ê´€ë ¨ì„± ì ìˆ˜ ì •ë³´ ì¶”ê°€
            relevance_score = item.get('relevance_score', 0)
            relevance_reason = item.get('relevance_reason', 'í‰ê°€ ì—†ìŒ')
            
            if item.get('type') == 'comment':
                # ëŒ“ê¸€ì¸ ê²½ìš° ë¶€ëª¨ ê²Œì‹œë¬¼ ì •ë³´ë„ í¬í•¨
                parent_post_id = item.get('post_id') or item.get('parent_id')
                parent_post = posts_by_id.get(parent_post_id, {})
                
                # ID í¬ë§· ìˆ˜ì •: ì»¨í…ì¸  ë²ˆí˜¸ë¥¼ COMMENT_IDë¡œ ì‚¬ìš©
                post_text = f"""[ì»¨í…ì¸  {i} - ëŒ“ê¸€]
COMMENT_ID: COMMENT_{i}
ë¶€ëª¨ ê²Œì‹œê¸€ ì œëª©: {parent_post.get('title', 'ì œëª© ì—†ìŒ')}
ë¶€ëª¨ ê²Œì‹œê¸€ ë‚´ìš©: {parent_post.get('selftext', '')[:100] if parent_post.get('selftext') else '(ë‚´ìš© ì—†ìŒ)'}
ëŒ“ê¸€ ë‚´ìš©: {item.get('content', '')[:300]}
ëŒ“ê¸€ ì¶”ì²œìˆ˜: {item.get('score', 0)} | ë¶€ëª¨ ê²Œì‹œê¸€ ì¶”ì²œìˆ˜: {parent_post.get('score', 0)}
ì„œë¸Œë ˆë”§: r/{item.get('subreddit', 'unknown')}
ì‘ì„±ì: {item.get('author', 'unknown')}
ê´€ë ¨ì„±: {relevance_score}/10 | ë£¨ë¨¸ì ìˆ˜: {rumor_score}/10
---"""
            else:
                # ê²Œì‹œë¬¼ì¸ ê²½ìš°
                # ID í¬ë§· ìˆ˜ì •: ì»¨í…ì¸  ë²ˆí˜¸ë¥¼ POST_IDë¡œ ì‚¬ìš©
                post_text = f"""[ì»¨í…ì¸  {i} - ê²Œì‹œë¬¼]
POST_ID: POST_{i}
ì œëª©: {item.get('title', 'ì œëª© ì—†ìŒ')}
ì ìˆ˜: {item.get('score', 0)} | ëŒ“ê¸€: {item.get('num_comments', 0)} | ë£¨ë¨¸ì ìˆ˜: {rumor_score}/10 | ê´€ë ¨ì„±: {relevance_score}/10
ì„œë¸Œë ˆë”§: r/{item.get('subreddit', 'unknown')} | ìˆ˜ì§‘ë²¡í„°: {vector_info}
ì–¸ì–´ì‹ í˜¸: {', '.join(linguistic_flags) if linguistic_flags else 'ì—†ìŒ'}
ê´€ë ¨ì„±ì´ìœ : {relevance_reason}
ë‚´ìš©: {item.get('selftext', '')[:200] if item.get('selftext') else '(ë‚´ìš© ì—†ìŒ)'}
---"""
            formatted_posts.append(post_text)
            index_to_post[i] = item  # ì¸ë±ìŠ¤ ë§¤í•‘ ì €ì¥
        
        logger.debug(f"ğŸ“„ ê²Œì‹œë¬¼ í¬ë§·íŒ…: {len(formatted_posts)}ê°œ ê²Œì‹œë¬¼")
        return "\n".join(formatted_posts), index_to_post
    
    def _extract_footnote_mapping(self, report: str, posts: List[Dict[str, Any]], index_mapping: Dict[int, Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """ë³´ê³ ì„œì—ì„œ ê°ì£¼ ë§¤í•‘ ì¶”ì¶œ ë° [ref:POST_ID]ë¥¼ ë²ˆí˜¸ë¡œ ë³€í™˜"""
        import re
        
        footnote_mapping = []
        ref_to_footnote = {}  # POST_ID -> footnote_number ë§¤í•‘
        
        # [ref:POST_ID] íŒ¨í„´ ì°¾ê¸°
        ref_pattern = r'\[ref:([^\]]+)\]'
        refs = re.findall(ref_pattern, report)
        
        if not refs:
            logger.info("ğŸ“„ ì°¸ì¡°ê°€ ë°œê²¬ë˜ì§€ ì•ŠìŒ")
            return footnote_mapping
        
        logger.info(f"ğŸ”— ì°¸ì¡° ë°œê²¬: {len(refs)}ê°œ (ê³ ìœ : {len(set(refs))}ê°œ)")
        logger.debug(f"   ì°¸ì¡° ëª©ë¡: {list(set(refs))[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ ë¡œê¹…
        
        # ê³ ìœ í•œ POST_IDë“¤ì„ ì¶”ì¶œí•˜ê³  ë²ˆí˜¸ í• ë‹¹
        unique_refs = []
        for ref in refs:
            if ref not in ref_to_footnote:
                unique_refs.append(ref)
                ref_to_footnote[ref] = len(unique_refs)
        
        # ê° ê³ ìœ í•œ ì°¸ì¡°ì— ëŒ€í•´ ê²Œì‹œë¬¼ ì •ë³´ ì°¾ê¸°
        # ì¸ë±ìŠ¤ ë§¤í•‘ì„ ì‚¬ìš©í•˜ì—¬ IDë¡œ ê²Œì‹œë¬¼ ì°¾ê¸°
        posts_by_ref_id = {}
        if index_mapping:
            for idx, post in index_mapping.items():
                if post.get('type') == 'post':
                    posts_by_ref_id[f'POST_{idx}'] = post
                else:  # comment
                    posts_by_ref_id[f'COMMENT_{idx}'] = post
        else:
            # í´ë°±: ì˜ˆì „ ë°©ì‹
            for idx, post in enumerate(posts, 1):
                if post.get('type') == 'post':
                    posts_by_ref_id[f'POST_{idx}'] = post
                else:  # comment
                    posts_by_ref_id[f'COMMENT_{idx}'] = post
        
        logger.debug(f"ğŸ“š ê²Œì‹œë¬¼ ì°¸ì¡° ID ë§¤í•‘ ìƒì„±: {len(posts_by_ref_id)}ê°œ")
        logger.debug(f"   ì°¸ì¡° ID ì˜ˆì‹œ: {list(posts_by_ref_id.keys())[:5]}...")  # ì²˜ìŒ 5ê°œë§Œ
        
        for post_id, footnote_number in ref_to_footnote.items():
            if post_id in posts_by_ref_id:
                post = posts_by_ref_id[post_id]
                # created_utcë¥¼ Unix timestampë¡œ ë³€í™˜
                created_utc = post.get('created_utc', '')
                if created_utc and isinstance(created_utc, str):
                    try:
                        # ISO í˜•ì‹ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ íŒŒì‹± í›„ Unix timestampë¡œ ë³€í™˜
                        dt = datetime.fromisoformat(created_utc.replace('Z', '+00:00'))
                        created_utc = dt.timestamp()
                    except:
                        # ì´ë¯¸ ìˆ«ìí˜•ì´ê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì§€
                        pass
                elif not created_utc:
                    created_utc = 0  # ë¹ˆ ê°’ì€ 0ìœ¼ë¡œ
                
                footnote_mapping.append({
                    "footnote_number": footnote_number,
                    "post_id": post.get('id', ''),
                    "url": post.get('url', ''),
                    "title": post.get('title', ''),
                    "score": post.get('score', 0),
                    "comments": post.get('num_comments', 0),
                    "created_utc": created_utc,
                    "subreddit": post.get('subreddit', ''),
                    "author": post.get('author', ''),
                    "position_in_report": footnote_number
                })
            else:
                logger.warning(f"âš ï¸ ì°¸ì¡°ëœ POST_IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {post_id}")
                logger.debug(f"   ì‚¬ìš© ê°€ëŠ¥í•œ IDë“¤: {list(posts_by_ref_id.keys())[:10]}")
        
        # ê°ì£¼ ë²ˆí˜¸ìˆœìœ¼ë¡œ ì •ë ¬
        footnote_mapping.sort(key=lambda x: x['footnote_number'])
        
        logger.info(f"ğŸ”— ê°ì£¼ ë§¤í•‘ ì™„ë£Œ: {len(footnote_mapping)}ê°œ")
        return footnote_mapping
    
    def _convert_refs_to_footnotes(self, report: str, footnote_mapping: List[Dict[str, Any]]) -> str:
        """[ref:POST_ID] ë§ˆì»¤ë¥¼ ë²ˆí˜¸ ê°ì£¼ [1], [2] ë“±ìœ¼ë¡œ ë³€í™˜"""
        import re
        
        # footnote_mappingì—ì„œ post_id -> footnote_number ë§¤í•‘ ìƒì„±
        post_id_to_footnote = {
            item['post_id']: item['footnote_number'] 
            for item in footnote_mapping
        }
        
        # ëª¨ë“  [ref:POST_ID] íŒ¨í„´ì„ ì°¾ì•„ ë²ˆí˜¸ë¡œ ë³€í™˜
        def replace_ref(match):
            post_id = match.group(1)
            if post_id in post_id_to_footnote:
                return f"[{post_id_to_footnote[post_id]}]"
            return match.group(0)  # ë§¤í•‘ì´ ì—†ìœ¼ë©´ ì›ë³¸ ìœ ì§€
        
        processed_report = re.sub(r'\[ref:([^\]]+)\]', replace_ref, report)
        
        # ë³´ê³ ì„œ ëì— ì°¸ì¡° ëª©ë¡ ì¶”ê°€
        if footnote_mapping:
            processed_report += "\n\n## ì°¸ì¡° ëª©ë¡\n\n"
            for item in footnote_mapping:
                processed_report += f"[{item['footnote_number']}] {item['title']} - r/{item['subreddit']} (ì ìˆ˜: {item['score']}, ëŒ“ê¸€: {item['comments']})\n"
        
        return processed_report

    async def _call_llm(self, prompt: str, temperature: float = 0.7) -> str:
        """ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œ - ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€"""
        response = await self.provider.generate(
            prompt=prompt,
            system_prompt="You are a professional analyst and expert writer.",
            temperature=temperature,
            max_tokens=4000
        )
        return response.content
    
    def _format_cluster_info(self, cluster_info: Dict[str, Any]) -> str:
        """í´ëŸ¬ìŠ¤í„° ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        clusters = cluster_info.get('clusters', [])
        statistics = cluster_info.get('statistics', {})
        
        if not clusters:
            return ""
        
        formatted_lines = ["ì‹ë³„ëœ ì£¼ìš” ì£¼ì œ:"]
        
        for idx, cluster in enumerate(clusters, 1):
            topic = cluster['topic']
            item_count = len(cluster['items'])
            avg_relevance = cluster.get('average_relevance', 0)
            
            formatted_lines.append(f"\n{idx}. {topic['name']} ({item_count}ê°œ ì½˜í…ì¸ , í‰ê·  ê´€ë ¨ì„±: {avg_relevance:.1f}/10)")
            formatted_lines.append(f"   - ì„¤ëª…: {topic['description']}")
            
            # í•µì‹¬ ì¸ì‚¬ì´íŠ¸ í¬í•¨
            if cluster.get('key_insights'):
                formatted_lines.append("   - ì£¼ìš” ì½˜í…ì¸ :")
                for insight in cluster['key_insights'][:2]:
                    formatted_lines.append(f"     â€¢ {insight['title'][:60]}... (ì ìˆ˜: {insight['score']})")
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        if statistics:
            formatted_lines.append(f"\nì „ì²´ í†µê³„:")
            formatted_lines.append(f"- ì´ ì½˜í…ì¸ : {statistics.get('total_items', 0)}ê°œ")
            formatted_lines.append(f"- í´ëŸ¬ìŠ¤í„°ëœ ì½˜í…ì¸ : {statistics.get('total_clustered', 0)}ê°œ")
            formatted_lines.append(f"- í‰ê·  í´ëŸ¬ìŠ¤í„° í¬ê¸°: {statistics.get('average_cluster_size', 0):.1f}ê°œ")
        
        return "\n".join(formatted_lines)