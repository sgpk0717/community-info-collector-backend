from typing import Dict, Any, List, Optional
from app.services.reddit_service import RedditService
from app.services.llm_service import LLMService
from app.services.database_service import DatabaseService
from app.services.relevance_filtering_service import RelevanceFilteringService
from app.services.topic_clustering_service import TopicClusteringService
from app.services.orchestrator_service import OrchestratorService
from app.schemas.search import SearchRequest, ReportLength, TimeFilter
from app.schemas.report import ReportCreate
import logging
from uuid import uuid4
from concurrent.futures import ThreadPoolExecutor
import asyncio
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class AnalysisService:
    def __init__(self, thread_pool: Optional[ThreadPoolExecutor] = None, api_semaphore: Optional[asyncio.Semaphore] = None):
        self.reddit_service = RedditService(thread_pool=thread_pool)
        self.llm_service = LLMService(api_semaphore=api_semaphore)
        self.relevance_service = RelevanceFilteringService(thread_pool=thread_pool, api_semaphore=api_semaphore)
        self.clustering_service = TopicClusteringService(thread_pool=thread_pool, api_semaphore=api_semaphore)
        self.orchestrator_service = OrchestratorService(thread_pool=thread_pool, api_semaphore=api_semaphore)
        self.db_service = DatabaseService()
        self.thread_pool = thread_pool
        self.api_semaphore = api_semaphore
    
    def _calculate_time_range(self, request: SearchRequest) -> tuple[datetime, datetime, str]:
        """ì‹œê°„ í•„í„°ì— ë”°ë¥¸ ë‚ ì§œ ë²”ìœ„ ê³„ì‚°"""
        now = datetime.now()
        
        if request.time_filter == TimeFilter.custom and request.start_date and request.end_date:
            return request.start_date, request.end_date, 'all'
        
        # ì‹œê°„ í•„í„°ë³„ ê³„ì‚°
        time_ranges = {
            TimeFilter.hour_1: (now - timedelta(hours=1), 'hour'),
            TimeFilter.hour_3: (now - timedelta(hours=3), 'hour'),
            TimeFilter.hour_6: (now - timedelta(hours=6), 'day'),
            TimeFilter.hour_12: (now - timedelta(hours=12), 'day'),
            TimeFilter.day_1: (now - timedelta(days=1), 'day'),
            TimeFilter.day_3: (now - timedelta(days=3), 'week'),
            TimeFilter.week_1: (now - timedelta(weeks=1), 'week'),
            TimeFilter.month_1: (now - timedelta(days=30), 'month'),
        }
        
        if request.time_filter and request.time_filter in time_ranges:
            start_time, reddit_filter = time_ranges[request.time_filter]
            return start_time, now, reddit_filter
        
        # ê¸°ë³¸ê°’: ì „ì²´ ê¸°ê°„
        return datetime.min, now, 'all'
        
    async def process_search_request(self, request: SearchRequest, progress_callback=None) -> Dict[str, Any]:
        """ê²€ìƒ‰ ìš”ì²­ ì²˜ë¦¬ ë° ë¶„ì„ - ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì‚¬ìš©"""
        try:
            logger.info(f"ğŸš€ ë¶„ì„ ì„œë¹„ìŠ¤ ì‹œì‘: '{request.query}' (ì‚¬ìš©ì: {request.user_nickname})")
            
            # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
            if progress_callback:
                await progress_callback("ë¶„ì„ ì¤€ë¹„ ì¤‘", 0)
            
            # 1. ì‚¬ìš©ì í™•ì¸/ìƒì„±
            logger.info(f"ğŸ‘¤ ì‚¬ìš©ì í™•ì¸/ìƒì„±: {request.user_nickname}")
            user = await self.db_service.get_or_create_user(request.user_nickname)
            
            # 2. ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¥¼ í†µí•œ ì „ì²´ ë¶„ì„ ìˆ˜í–‰
            logger.info("ğŸ¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ê¸°ë°˜ ë¶„ì„ ì‹œì‘")
            orchestration_result = await self.orchestrator_service.orchestrate_analysis(
                request,
                progress_callback
            )
            
            # 3. ë³´ê³ ì„œ ì €ì¥
            if progress_callback:
                await progress_callback("ë³´ê³ ì„œ ì €ì¥ ì¤‘", 80)
            
            report_data = orchestration_result['report']
            metadata = orchestration_result['metadata']
            
            logger.info(f"ğŸ’¾ ë³´ê³ ì„œ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œì‘")
            
            # í‚¤ì›Œë“œ ì •ë³´ ìƒì„±
            keywords_used = []
            keyword_stats = metadata.get('keyword_stats', {})
            
            for idx, kw in enumerate(metadata['expanded_keywords'][:10]):
                # í‚¤ì›Œë“œë³„ í†µê³„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                kw_stat = keyword_stats.get(kw, {})
                
                keywords_used.append({
                    'keyword': kw,
                    'translated_keyword': None,
                    'posts_found': kw_stat.get('posts_found', 0),
                    'sample_titles': kw_stat.get('sample_titles', [])
                })
            
            report_create = ReportCreate(
                user_nickname=request.user_nickname,
                query_text=request.query,
                summary=report_data['summary'],
                full_report=report_data['full_report'],
                posts_collected=metadata['filtered_count'],
                report_length=request.length.value,
                session_id=request.session_id,
                keywords_used=keywords_used
            )
            
            report_id = await self.db_service.save_report(report_create)
            logger.info(f"âœ… ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {report_id}")
            
            # ê°ì£¼ ë§¤í•‘ ì €ì¥ (ìˆì„ ê²½ìš°)
            footnote_mapping = report_data.get('footnote_mapping', [])
            if footnote_mapping:
                logger.info(f"ğŸ”— ê°ì£¼ ë§¤í•‘ ì €ì¥ ì‹œì‘: {len(footnote_mapping)}ê°œ")
                await self.db_service.save_report_links(report_id, footnote_mapping)
                logger.info(f"âœ… ê°ì£¼ ë§¤í•‘ ì €ì¥ ì™„ë£Œ")
            
            # 4. ìŠ¤ì¼€ì¤„ ìƒì„± (ìš”ì²­ ì‹œ)
            schedule_id = None
            if request.schedule_yn == "Y":
                logger.info(f"ğŸ“… ìŠ¤ì¼€ì¤„ ìƒì„± ì‹œì‘ (ì£¼ê¸°: {request.schedule_period}ë¶„, íšŸìˆ˜: {request.schedule_count}íšŒ)")
                schedule_data = {
                    'user_nickname': request.user_nickname,
                    'keyword': request.query,
                    'interval_minutes': request.schedule_period,
                    'total_reports': request.schedule_count,
                    'next_run': request.schedule_start_time.isoformat() if request.schedule_start_time else None,
                    'report_length': request.length.value,
                    'sources': request.sources,
                    'notification_enabled': bool(request.push_token)
                }
                schedule_id = await self.db_service.create_schedule(schedule_data)
                logger.info(f"âœ… ìŠ¤ì¼€ì¤„ ìƒì„± ì™„ë£Œ: {schedule_id}")
            
            if progress_callback:
                await progress_callback("ì™„ë£Œ", 100)
            
            logger.info(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ! ë³´ê³ ì„œ ID: {report_id}")
            
            return {
                'report_id': report_id,
                'summary': report_data['summary'],
                'full_report': report_data['full_report'],
                'posts_collected': metadata['filtered_count'],
                'schedule_id': schedule_id,
                'quality_metrics': report_data.get('quality_metrics', {})
            }
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def _deduplicate_posts(self, posts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ê²Œì‹œë¬¼ ì œê±°"""
        seen_ids = set()
        unique_posts = []
        duplicates_removed = 0
        
        for post in posts:
            if post['id'] not in seen_ids:
                seen_ids.add(post['id'])
                unique_posts.append(post)
            else:
                duplicates_removed += 1
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        unique_posts.sort(key=lambda x: x['score'], reverse=True)
        
        if duplicates_removed > 0:
            logger.info(f"ğŸ”„ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {duplicates_removed}ê°œ ê²Œì‹œë¬¼ ì œê±°")
        
        # ìƒìœ„ ê²Œì‹œë¬¼ ì •ë³´ ë¡œê·¸
        if unique_posts:
            top_post = unique_posts[0]
            logger.info(f"ğŸ† ìµœê³  ì ìˆ˜ ê²Œì‹œë¬¼: {top_post['score']}ì  - {top_post['title'][:50]}...")
        
        return unique_posts