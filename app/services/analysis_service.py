from typing import Dict, Any, List, Optional
from app.services.reddit_service import RedditService
from app.services.llm_service import LLMService
from app.services.database_service import DatabaseService
from app.schemas.search import SearchRequest, ReportLength, TimeFilter
from app.schemas.report import ReportCreate
import logging
from uuid import uuid4
from concurrent.futures import ThreadPoolExecutor
import asyncio
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class AnalysisService:
    def __init__(self, thread_pool: Optional[ThreadPoolExecutor] = None, api_semaphore: Optional[asyncio.Semaphore] = None):
        self.reddit_service = RedditService(thread_pool=thread_pool)
        self.llm_service = LLMService(api_semaphore=api_semaphore)
        self.db_service = DatabaseService()
        self.thread_pool = thread_pool
        self.api_semaphore = api_semaphore
    
    def _calculate_time_range(self, request: SearchRequest) -> tuple[datetime, datetime, str]:
        """ì‹œê°„ í•„í„°ì— ë”°ë¥¸ ë‚ ì§œ ë²”ìœ„ ê³„ì‚°"""
        now = datetime.now()
        
        if request.time_filter == TimeFilter.custom and request.start_date and request.end_date:
            return request.start_date, request.end_date, 'all'
        
        # ì‹œê°„ í•„í„°ë³„ ê³„ì‚°
        time_ranges = {
            TimeFilter.hour_1: (now - timedelta(hours=1), 'hour'),
            TimeFilter.hour_3: (now - timedelta(hours=3), 'hour'),
            TimeFilter.hour_6: (now - timedelta(hours=6), 'day'),
            TimeFilter.hour_12: (now - timedelta(hours=12), 'day'),
            TimeFilter.day_1: (now - timedelta(days=1), 'day'),
            TimeFilter.day_3: (now - timedelta(days=3), 'week'),
            TimeFilter.week_1: (now - timedelta(weeks=1), 'week'),
            TimeFilter.month_1: (now - timedelta(days=30), 'month'),
        }
        
        if request.time_filter and request.time_filter in time_ranges:
            start_time, reddit_filter = time_ranges[request.time_filter]
            return start_time, now, reddit_filter
        
        # ê¸°ë³¸ê°’: ì „ì²´ ê¸°ê°„
        return datetime.min, now, 'all'
        
    async def process_search_request(self, request: SearchRequest, progress_callback=None) -> Dict[str, Any]:
        """ê²€ìƒ‰ ìš”ì²­ ì²˜ë¦¬ ë° ë¶„ì„"""
        try:
            logger.info(f"ğŸš€ ë¶„ì„ ì„œë¹„ìŠ¤ ì‹œì‘: '{request.query}' (ì‚¬ìš©ì: {request.user_nickname})")
            
            # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
            if progress_callback:
                await progress_callback("ë¶„ì„ ì¤€ë¹„ ì¤‘", 0)
            
            # 1. ì‚¬ìš©ì í™•ì¸/ìƒì„±
            logger.info(f"ğŸ‘¤ ì‚¬ìš©ì í™•ì¸/ìƒì„±: {request.user_nickname}")
            user = await self.db_service.get_or_create_user(request.user_nickname)
            
            # 2. í•œê¸€ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë²ˆì—­
            if progress_callback:
                await progress_callback("í‚¤ì›Œë“œ ë²ˆì—­ ì¤‘", 5)
            
            logger.info(f"ğŸŒ í‚¤ì›Œë“œ ë²ˆì—­ ì‹œì‘: '{request.query}' (í•œêµ­ì–´ â†’ ì˜ì–´)")
            english_query = await self.llm_service.translate_to_english(request.query)
            logger.info(f"âœ… ë²ˆì—­ ì™„ë£Œ: '{request.query}' â†’ '{english_query}'")
            
            # 3. í‚¤ì›Œë“œ í™•ì¥ (ì„ íƒì , ì˜ì–´ë¡œ)
            if progress_callback:
                await progress_callback("í‚¤ì›Œë“œ í™•ì¥ ì¤‘", 10)
                
            expanded_keywords = []
            if request.length in [ReportLength.moderate, ReportLength.detailed]:
                logger.info(f"ğŸ” í‚¤ì›Œë“œ í™•ì¥ ì‹œì‘ (ë³´ê³ ì„œ ê¸¸ì´: {request.length.value})")
                expanded_keywords = await self.llm_service.expand_keywords(request.query)  # ë‚´ë¶€ì—ì„œ ë²ˆì—­ë¨
                logger.info(f"ğŸ“ í™•ì¥ëœ í‚¤ì›Œë“œ ({len(expanded_keywords)}ê°œ): {expanded_keywords}")
            
            # 4. ì‹œê°„ ë²”ìœ„ ê³„ì‚°
            start_date, end_date, reddit_time_filter = self._calculate_time_range(request)
            if request.time_filter:
                logger.info(f"â° ì‹œê°„ í•„í„° ì ìš©: {request.time_filter.value} ({start_date.strftime('%Y-%m-%d %H:%M')} ~ {end_date.strftime('%Y-%m-%d %H:%M')})")
            
            # 5. ê²Œì‹œë¬¼ ìˆ˜ì§‘ (ì˜ì–´ë¡œ)
            if progress_callback:
                await progress_callback("ì†Œì…œ ë¯¸ë””ì–´ ë°ì´í„° ìˆ˜ì§‘ ì¤‘", 20)
            
            all_posts = []
            
            # Reddit ê²€ìƒ‰ (ê²Œì‹œë¬¼ + ëŒ“ê¸€ í•¨ê»˜ ìˆ˜ì§‘)
            if "reddit" in request.sources:
                logger.info(f"ğŸ” Reddit ê²Œì‹œë¬¼+ëŒ“ê¸€ ê²€ìƒ‰ ì‹œì‘: '{english_query}' (ì‹œê°„ í•„í„°: {reddit_time_filter})")
                
                # í™•ì¥ëœ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í•¨ê»˜ ì‚¬ìš©, ì—†ìœ¼ë©´ ì›ë³¸ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
                keywords_to_search = [english_query]
                if expanded_keywords:
                    keywords_to_search.extend(expanded_keywords)
                
                logger.info(f"ğŸ“ˆ ì´ {len(keywords_to_search)}ê°œ í‚¤ì›Œë“œë¡œ ê²Œì‹œë¬¼+ëŒ“ê¸€ ìˆ˜ì§‘")
                
                # ê²Œì‹œë¬¼ê³¼ ëŒ“ê¸€ì„ í•¨ê»˜ ìˆ˜ì§‘
                all_content = await self.reddit_service.collect_posts_with_comments(
                    keywords=keywords_to_search,
                    max_comments_per_post=8,  # ê²Œì‹œë¬¼ë‹¹ ìµœëŒ€ 8ê°œ ëŒ“ê¸€
                    posts_limit=15  # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 15ê°œ ê²Œì‹œë¬¼
                )
                
                # ê²Œì‹œë¬¼ê³¼ ëŒ“ê¸€ì„ ë¶„ë¦¬
                posts_only = [item for item in all_content if item['type'] == 'post']
                comments_only = [item for item in all_content if item['type'] == 'comment']
                
                logger.info(f"ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ - ê²Œì‹œë¬¼: {len(posts_only)}ê°œ, ëŒ“ê¸€: {len(comments_only)}ê°œ")
                
                # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ê²Œì‹œë¬¼ë§Œ)
                all_posts.extend([{
                    'id': item['id'],
                    'title': item['title'],
                    'selftext': item['content'],
                    'score': item['score'],
                    'created_utc': item['created_utc'],
                    'subreddit': item['subreddit'],
                    'author': item['author'],
                    'url': item['url'],
                    'num_comments': item['num_comments'],
                    'keyword_source': item['keyword_source']
                } for item in posts_only])
                
                # TODO: ëŒ“ê¸€ ë°ì´í„°ë¥¼ ë‚˜ì¤‘ì— ë¶„ì„ì— í™œìš©í•  ìˆ˜ ìˆë„ë¡ ì €ì¥
                # í˜„ì¬ëŠ” ê²Œì‹œë¬¼ë§Œ ë¶„ì„í•˜ì§€ë§Œ, 3ë‹¨ê³„ì—ì„œ ëŒ“ê¸€ë„ í•¨ê»˜ ë¶„ì„í•˜ë„ë¡ ê°œì„  ì˜ˆì •
                
                if progress_callback:
                    await progress_callback(f"Redditì—ì„œ ê²Œì‹œë¬¼ {len(posts_only)}ê°œ + ëŒ“ê¸€ {len(comments_only)}ê°œ ìˆ˜ì§‘", 50)
            
            # ë‚ ì§œ ë²”ìœ„ì— ë”°ë¥¸ ê²Œì‹œë¬¼ í•„í„°ë§
            if request.time_filter:
                filtered_posts = []
                for post in all_posts:
                    post_date = datetime.fromtimestamp(post['created_utc'])
                    if start_date <= post_date <= end_date:
                        filtered_posts.append(post)
                
                logger.info(f"ğŸ“… ë‚ ì§œ í•„í„°ë§: {len(all_posts)}ê°œ â†’ {len(filtered_posts)}ê°œ (ë²”ìœ„: {start_date} ~ {end_date})")
                all_posts = filtered_posts
            
            # ê²Œì‹œë¬¼ì´ ì—†ìœ¼ë©´ ì—ëŸ¬
            if not all_posts:
                logger.error("âŒ ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                raise Exception("No posts found for the given query")
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            logger.info(f"ğŸ”„ ê²Œì‹œë¬¼ ì¤‘ë³µ ì œê±° ë° ì •ë ¬ ì‹œì‘ (ì›ë³¸: {len(all_posts)}ê°œ)")
            unique_posts = self._deduplicate_posts(all_posts)
            logger.info(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(unique_posts)}ê°œ ê²Œì‹œë¬¼")
            
            # 4. AI ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±
            if progress_callback:
                await progress_callback("AI ë¶„ì„ ì¤‘", 60)
            
            logger.info(f"ğŸ¤– AI ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„± ì‹œì‘ ({len(unique_posts)}ê°œ ê²Œì‹œë¬¼)")
            report_data = await self.llm_service.generate_report(
                posts=unique_posts,
                query=request.query,
                length=request.length
            )
            
            if progress_callback:
                await progress_callback("ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ", 80)
            
            # 5. ë³´ê³ ì„œ ì €ì¥
            logger.info(f"ğŸ’¾ ë³´ê³ ì„œ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œì‘")
            # í‚¤ì›Œë“œ ì •ë³´ ìˆ˜ì§‘
            keywords_used = []
            
            # ì›ë³¸ í‚¤ì›Œë“œ (í•œêµ­ì–´) ì¶”ê°€
            keywords_used.append({
                'keyword': request.query,
                'translated_keyword': english_query,
                'posts_found': len([p for p in unique_posts if request.query.lower() in p.get('title', '').lower() or request.query.lower() in p.get('selftext', '').lower()]),
                'sample_titles': [p['title'] for p in unique_posts[:3]]
            })
            
            # í™•ì¥ëœ í‚¤ì›Œë“œ ì •ë³´ ì¶”ê°€ (ì „ì²´ ì‚¬ìš©)
            if expanded_keywords:
                for kw in expanded_keywords:  # ì „ì²´ í™•ì¥ í‚¤ì›Œë“œ ì‚¬ìš©
                    posts_found_count = len([p for p in unique_posts if kw.lower() in p.get('title', '').lower() or kw.lower() in p.get('selftext', '').lower()])
                    if posts_found_count > 0:  # ì‹¤ì œë¡œ ê²Œì‹œë¬¼ì´ ë°œê²¬ëœ í‚¤ì›Œë“œë§Œ ì €ì¥
                        keywords_used.append({
                            'keyword': kw,
                            'translated_keyword': None,  # ì´ë¯¸ ì˜ì–´
                            'posts_found': posts_found_count,
                            'sample_titles': [p['title'] for p in unique_posts if kw.lower() in p.get('title', '').lower()][:2]  # ìƒ˜í”Œ 2ê°œë§Œ
                        })
            
            report_create = ReportCreate(
                user_nickname=request.user_nickname,
                query_text=request.query,
                summary=report_data['summary'],
                full_report=report_data['full_report'],
                posts_collected=len(unique_posts),
                report_length=request.length.value,
                session_id=request.session_id,
                keywords_used=keywords_used
            )
            
            report_id = await self.db_service.save_report(report_create)
            logger.info(f"âœ… ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {report_id}")
            
            # 6. ê°ì£¼ ë§í¬ ì €ì¥
            footnote_mapping = report_data.get('footnote_mapping', [])
            if footnote_mapping:
                logger.info(f"ğŸ”— ê°ì£¼ ë§í¬ ì €ì¥ ì‹œì‘: {len(footnote_mapping)}ê°œ")
                await self.db_service.save_report_links(report_id, footnote_mapping)
                logger.info(f"âœ… ê°ì£¼ ë§í¬ ì €ì¥ ì™„ë£Œ")
            
            if progress_callback:
                await progress_callback("ì €ì¥ ì™„ë£Œ", 90)
            
            # 7. ìŠ¤ì¼€ì¤„ ìƒì„± (ìš”ì²­ ì‹œ)
            schedule_id = None
            if request.schedule_yn == "Y":
                logger.info(f"ğŸ“… ìŠ¤ì¼€ì¤„ ìƒì„± ì‹œì‘ (ì£¼ê¸°: {request.schedule_period}ë¶„, íšŸìˆ˜: {request.schedule_count}íšŒ)")
                schedule_data = {
                    'user_nickname': request.user_nickname,
                    'keyword': request.query,
                    'interval_minutes': request.schedule_period,
                    'total_reports': request.schedule_count,
                    'next_run': request.schedule_start_time.isoformat() if request.schedule_start_time else None,
                    'report_length': request.length.value,
                    'sources': [s.value for s in request.sources],
                    'notification_enabled': bool(request.push_token)
                }
                schedule_id = await self.db_service.create_schedule(schedule_data)
                logger.info(f"âœ… ìŠ¤ì¼€ì¤„ ìƒì„± ì™„ë£Œ: {schedule_id}")
            
            if progress_callback:
                await progress_callback("ì™„ë£Œ", 100)
            
            logger.info(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ! ë³´ê³ ì„œ ID: {report_id}, ê²Œì‹œë¬¼ ìˆ˜: {len(unique_posts)}ê°œ")
            
            return {
                'report_id': report_id,
                'summary': report_data['summary'],
                'full_report': report_data['full_report'],
                'posts_collected': len(unique_posts),
                'schedule_id': schedule_id
            }
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def _deduplicate_posts(self, posts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ê²Œì‹œë¬¼ ì œê±°"""
        seen_ids = set()
        unique_posts = []
        duplicates_removed = 0
        
        for post in posts:
            if post['id'] not in seen_ids:
                seen_ids.add(post['id'])
                unique_posts.append(post)
            else:
                duplicates_removed += 1
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        unique_posts.sort(key=lambda x: x['score'], reverse=True)
        
        if duplicates_removed > 0:
            logger.info(f"ğŸ”„ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {duplicates_removed}ê°œ ê²Œì‹œë¬¼ ì œê±°")
        
        # ìƒìœ„ ê²Œì‹œë¬¼ ì •ë³´ ë¡œê·¸
        if unique_posts:
            top_post = unique_posts[0]
            logger.info(f"ğŸ† ìµœê³  ì ìˆ˜ ê²Œì‹œë¬¼: {top_post['score']}ì  - {top_post['title'][:50]}...")
        
        return unique_posts