from typing import List, Dict, Any, Optional
import requests
import json
import logging
import os
from .base import BaseLLMProvider, LLMResponse

logger = logging.getLogger(__name__)


class GeminiProvider(BaseLLMProvider):
    """Google Gemini API Provider êµ¬í˜„"""
    
    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        """
        Gemini Provider ì´ˆê¸°í™”
        
        Args:
            api_key: Gemini API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
            model: ì‚¬ìš©í•  ëª¨ë¸ëª… (ê¸°ë³¸ê°’: gemini-2.5-flash-latest)
        """
        self.api_key = api_key or os.getenv('GEMINI_API_KEY')
        if not self.api_key:
            raise ValueError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.model = model or "gemini-1.5-flash"
        self.base_url = "https://generativelanguage.googleapis.com/v1beta/models"
        logger.info(f"Gemini Provider ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë¸: {self.model}")
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> LLMResponse:
        """Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            logger.info(f"ğŸ¤– Gemini API í˜¸ì¶œ ì‹œì‘ - ëª¨ë¸: {self.model}, ì˜¨ë„: {temperature}")
            
            url = f"{self.base_url}/{self.model}:generateContent?key={self.api_key}"
            headers = {"Content-Type": "application/json"}
            
            # ìš”ì²­ ë°ì´í„° êµ¬ì„±
            data = {
                "contents": [{
                    "parts": [{
                        "text": prompt
                    }]
                }],
                "generationConfig": {
                    "temperature": temperature,
                    "maxOutputTokens": max_tokens
                }
            }
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if system_prompt:
                data["systemInstruction"] = {
                    "parts": [{
                        "text": system_prompt
                    }]
                }
            
            # ì¶”ê°€ íŒŒë¼ë¯¸í„° ë³‘í•©
            if kwargs:
                data["generationConfig"].update(kwargs)
            
            # API í˜¸ì¶œ
            response = requests.post(url, headers=headers, data=json.dumps(data))
            response.raise_for_status()
            
            result = response.json()
            
            # ì‘ë‹µ íŒŒì‹±
            if "candidates" in result and result["candidates"]:
                content = result["candidates"][0]["content"]["parts"][0]["text"]
                
                # ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶”ì¶œ (GeminiëŠ” í† í° ì •ë³´ë¥¼ ì§ì ‘ ì œê³µí•˜ì§€ ì•ŠìŒ)
                usage = None
                if "usageMetadata" in result:
                    usage = {
                        "prompt_tokens": result["usageMetadata"].get("promptTokenCount", 0),
                        "completion_tokens": result["usageMetadata"].get("candidatesTokenCount", 0),
                        "total_tokens": result["usageMetadata"].get("totalTokenCount", 0)
                    }
                
                logger.info(f"âœ… Gemini API ì‘ë‹µ ìˆ˜ì‹  - ê¸¸ì´: {len(content)} ë¬¸ì")
                if usage:
                    logger.info(f"   í† í° ì‚¬ìš©: {usage['total_tokens']} (í”„ë¡¬í”„íŠ¸: {usage['prompt_tokens']}, ì™„ì„±: {usage['completion_tokens']})")
                
                return LLMResponse(
                    content=content.strip(),
                    model=self.model,
                    usage=usage,
                    raw_response=result
                )
            else:
                raise ValueError("Gemini API ì‘ë‹µì— ìœ íš¨í•œ ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Gemini API ìš”ì²­ ì‹¤íŒ¨: {str(e)}")
            if hasattr(e.response, 'text'):
                logger.error(f"   ì‘ë‹µ ë‚´ìš©: {e.response.text}")
            raise
        except Exception as e:
            logger.error(f"âŒ Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def generate_with_messages(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> LLMResponse:
        """ë©”ì‹œì§€ í˜•ì‹ì„ Gemini í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í˜¸ì¶œ"""
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì‚¬ìš©ì ë©”ì‹œì§€ ë¶„ë¦¬
        system_prompt = None
        user_messages = []
        
        for msg in messages:
            if msg["role"] == "system":
                system_prompt = msg["content"]
            elif msg["role"] == "user":
                user_messages.append(msg["content"])
            elif msg["role"] == "assistant":
                # GeminiëŠ” ë‹¤ì¤‘ í„´ ëŒ€í™”ë¥¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì§€ê¸ˆì€ ê°„ë‹¨íˆ ì²˜ë¦¬
                user_messages.append(f"Assistant: {msg['content']}")
        
        # ëª¨ë“  ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ í•˜ë‚˜ë¡œ ê²°í•©
        prompt = "\n\n".join(user_messages)
        
        return await self.generate(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
    
    @property
    def provider_name(self) -> str:
        return "Gemini"
    
    @property
    def default_model(self) -> str:
        return self.model