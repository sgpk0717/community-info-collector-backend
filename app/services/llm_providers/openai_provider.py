from typing import List, Dict, Any, Optional
from openai import OpenAI
import logging
from .base import BaseLLMProvider, LLMResponse
import asyncio

logger = logging.getLogger(__name__)


class OpenAIProvider(BaseLLMProvider):
    """OpenAI API Provider êµ¬í˜„"""
    
    # ì¶”ë¡  ëª¨ë¸ ëª©ë¡ (o1, o3, o4 ì‹œë¦¬ì¦ˆ)
    REASONING_MODELS = {
        'o1-preview', 'o1-mini', 
        'o3', 'o3-mini',
        'o4', 'o4-mini'
    }
    
    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None, api_semaphore: Optional[asyncio.Semaphore] = None):
        """
        OpenAI Provider ì´ˆê¸°í™”
        
        Args:
            api_key: OpenAI API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ)
            model: ì‚¬ìš©í•  ëª¨ë¸ëª… (ê¸°ë³¸ê°’: o4-mini)
            api_semaphore: API ë™ì‹œ í˜¸ì¶œ ì œí•œì„ ìœ„í•œ Semaphore
        """
        self.client = OpenAI(api_key=api_key) if api_key else OpenAI()
        self.model = model or "o4-mini"
        self.api_semaphore = api_semaphore or asyncio.Semaphore(3)  # ê¸°ë³¸ê°’: ë™ì‹œ 3ê°œ í˜¸ì¶œ
        logger.info(f"OpenAI Provider ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë¸: {self.model}")
    
    def is_reasoning_model(self, model: Optional[str] = None) -> bool:
        """ì£¼ì–´ì§„ ëª¨ë¸ì´ ì¶”ë¡  ëª¨ë¸ì¸ì§€ í™•ì¸"""
        check_model = model or self.model
        return check_model in self.REASONING_MODELS
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> LLMResponse:
        """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±"""
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        return await self.generate_with_messages(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
    
    async def generate_with_messages(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> LLMResponse:
        """OpenAI Chat Completions API í˜¸ì¶œ"""
        # Semaphoreë¡œ API í˜¸ì¶œ ì œí•œ
        async with self.api_semaphore:
            logger.info(f"ğŸ”’ API Semaphore íšë“ - í˜„ì¬ ëŒ€ê¸°: {self.api_semaphore._value}/{self.api_semaphore._initial_value}")
            
            try:
                # ì¶”ë¡  ëª¨ë¸ ì—¬ë¶€ í™•ì¸
                is_reasoning = self.is_reasoning_model()
            
            if is_reasoning:
                logger.info(f"ğŸ¤– OpenAI ì¶”ë¡  ëª¨ë¸ API í˜¸ì¶œ ì‹œì‘ - ëª¨ë¸: {self.model}")
                logger.info("   ì¶”ë¡  ëª¨ë¸ì´ë¯€ë¡œ modelê³¼ messages íŒŒë¼ë¯¸í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                
                # ì¶”ë¡  ëª¨ë¸ì€ modelê³¼ messagesë§Œ ì§€ì›
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages
                )
            else:
                logger.info(f"ğŸ¤– OpenAI API í˜¸ì¶œ ì‹œì‘ - ëª¨ë¸: {self.model}, ì˜¨ë„: {temperature}")
                
                # ì¼ë°˜ ëª¨ë¸ì€ ëª¨ë“  íŒŒë¼ë¯¸í„° ì§€ì›
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    **kwargs  # ì¶”ê°€ íŒŒë¼ë¯¸í„° (top_p, frequency_penalty ë“±)
                )
            
            content = response.choices[0].message.content.strip()
            
            # ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶”ì¶œ
            usage = None
            if hasattr(response, 'usage'):
                usage = {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            
                logger.info(f"âœ… OpenAI API ì‘ë‹µ ìˆ˜ì‹  - ê¸¸ì´: {len(content)} ë¬¸ì")
                if usage:
                    logger.info(f"   í† í° ì‚¬ìš©: {usage['total_tokens']} (í”„ë¡¬í”„íŠ¸: {usage['prompt_tokens']}, ì™„ì„±: {usage['completion_tokens']})")
                
                return LLMResponse(
                    content=content,
                    model=self.model,
                    usage=usage,
                    raw_response=response
                )
                
            except Exception as e:
                logger.error(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
                raise
            finally:
                logger.info(f"ğŸ”“ API Semaphore í•´ì œ")
    
    @property
    def provider_name(self) -> str:
        return "OpenAI"
    
    @property
    def default_model(self) -> str:
        return self.model