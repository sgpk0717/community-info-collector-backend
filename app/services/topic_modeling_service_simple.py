import logging
from typing import List, Dict, Any, Optional
from collections import Counter, defaultdict
import re
from app.services.llm_service import LLMService
from app.core.dependencies import get_supabase_client
import json

logger = logging.getLogger(__name__)

class SimpleTopicModelingService:
    """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì£¼ì œ ëª¨ë¸ë§ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        logger.info("ğŸ§  SimpleTopicModelingService ì´ˆê¸°í™”")
        self.llm_service = LLMService()
        self.client = get_supabase_client()
        
        # í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
        self.stop_words = {
            'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜', 'ë”', 'ë§¤ìš°', 'ì™€', 'ì€', 'ëŠ”', 'ì´', 'ê°€',
            'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ë¡œ', 'ìœ¼ë¡œ', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ì—†ë‹¤', 'ì´ë‹¤'
        }
    
    async def analyze_topics(self, session_id: str) -> List[Dict[str, Any]]:
        """ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì œë¥¼ ì¶”ì¶œí•˜ê³  ë¶„ì„"""
        logger.info(f"ğŸ“Š ê°„ë‹¨í•œ ì£¼ì œ ë¶„ì„ ì‹œì‘ - Session ID: {session_id}")
        
        try:
            # 1. ìˆ˜ì§‘ëœ ë°ì´í„° ì¡°íšŒ
            logger.info("ğŸ“¥ ìˆ˜ì§‘ëœ ë°ì´í„° ì¡°íšŒ ì¤‘...")
            result = self.client.table('source_contents')\
                .select("*")\
                .eq('metadata->>session_id', session_id)\
                .execute()
            
            if not result.data:
                logger.warning("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            documents = result.data
            logger.info(f"âœ… {len(documents)}ê°œì˜ ë¬¸ì„œ ì¡°íšŒ ì™„ë£Œ")
            
            # 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
            all_keywords = []
            doc_keywords_map = defaultdict(list)
            
            for doc in documents:
                text = doc['raw_text']
                if text and len(text.strip()) > 10:
                    keywords = self._extract_keywords(text)
                    all_keywords.extend(keywords)
                    doc_keywords_map[doc['content_id']] = keywords
            
            # 3. ë¹ˆë„ ê¸°ë°˜ ì£¼ìš” ì£¼ì œ ì¶”ì¶œ
            keyword_freq = Counter(all_keywords)
            top_keywords = keyword_freq.most_common(20)  # ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ
            
            logger.info(f"ğŸ” ì£¼ìš” í‚¤ì›Œë“œ: {[k for k, v in top_keywords[:10]]}")
            
            # 4. í‚¤ì›Œë“œ ê·¸ë£¹í•‘ìœ¼ë¡œ ì£¼ì œ ìƒì„±
            topics = await self._group_keywords_into_topics(top_keywords, documents)
            
            # 5. ê° ë¬¸ì„œë¥¼ ì£¼ì œì— í• ë‹¹
            for doc in documents:
                doc_id = doc['content_id']
                doc_keywords = set(doc_keywords_map[doc_id])
                
                # ê°€ì¥ ë§ì´ ë§¤ì¹­ë˜ëŠ” ì£¼ì œ ì°¾ê¸°
                best_topic = 0
                max_matches = 0
                
                for i, topic in enumerate(topics):
                    topic_keywords = set(topic['keywords'])
                    matches = len(doc_keywords & topic_keywords)
                    if matches > max_matches:
                        max_matches = matches
                        best_topic = i
                
                # DB ì—…ë°ì´íŠ¸
                self.client.table('source_contents')\
                    .update({'topic_id': best_topic})\
                    .eq('content_id', doc_id)\
                    .execute()
            
            # 6. ê° ì£¼ì œë³„ ë¬¸ì„œ ìˆ˜ ê³„ì‚° ë° ëŒ€í‘œ ë¬¸ì„œ ì„ íƒ
            for i, topic in enumerate(topics):
                topic_docs = [doc for doc in documents 
                             if doc.get('topic_id') == i or 
                             any(kw in doc['raw_text'] for kw in topic['keywords'][:3])]
                
                topic['document_count'] = len(topic_docs)
                topic['representative_docs'] = [doc['raw_text'][:200] + '...' 
                                              for doc in topic_docs[:3]]
                topic['doc_ids'] = [doc['content_id'] for doc in topic_docs]
                
                logger.info(f"ğŸ“Œ ì£¼ì œ {i}: {topic['topic_label']} ({len(topic_docs)}ê°œ ë¬¸ì„œ)")
            
            # ë¬¸ì„œê°€ ì—†ëŠ” ì£¼ì œ ì œê±°
            topics = [t for t in topics if t['document_count'] > 0]
            
            # ë¬¸ì„œ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            topics.sort(key=lambda x: x['document_count'], reverse=True)
            
            logger.info(f"ğŸ‰ ì£¼ì œ ë¶„ì„ ì™„ë£Œ! ì´ {len(topics)}ê°œ ì£¼ì œ")
            return topics
            
        except Exception as e:
            logger.error(f"âŒ ì£¼ì œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë‹¨ì¼ ì£¼ì œë¡œ ì²˜ë¦¬
            return await self._create_single_topic(documents if 'documents' in locals() else [])
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ëª…ì‚¬ ìœ„ì£¼)
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
        words = re.findall(r'[ê°€-í£]+|[a-zA-Z]+', text.lower())
        
        # 2ê¸€ì ì´ìƒ, ë¶ˆìš©ì–´ ì œì™¸
        keywords = [w for w in words 
                   if len(w) >= 2 and w not in self.stop_words]
        
        # ë¹ˆë„ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        word_freq = Counter(keywords)
        return [word for word, freq in word_freq.most_common(10)]
    
    async def _group_keywords_into_topics(self, top_keywords: List[tuple], 
                                        documents: List[Dict]) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œë¥¼ ì£¼ì œë¡œ ê·¸ë£¹í•‘"""
        # LLMì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œë¥¼ ì£¼ì œë¡œ ê·¸ë£¹í•‘
        keywords_text = ', '.join([f"{kw}({freq})" for kw, freq in top_keywords])
        
        prompt = f"""ë‹¤ìŒì€ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ì£¼ìš” í‚¤ì›Œë“œì™€ ë¹ˆë„ìˆ˜ì…ë‹ˆë‹¤:

{keywords_text}

ì´ í‚¤ì›Œë“œë“¤ì„ 3-5ê°œì˜ ì˜ë¯¸ìˆëŠ” ì£¼ì œë¡œ ê·¸ë£¹í•‘í•´ì£¼ì„¸ìš”. ê° ì£¼ì œë³„ë¡œ:
1. ì£¼ì œëª… (10-20ì)
2. ê´€ë ¨ í‚¤ì›Œë“œ 5-8ê°œ

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
[
  {{
    "topic_label": "ì£¼ì œëª…",
    "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...]
  }},
  ...
]"""
        
        try:
            response = await self.llm_service._call_openai(prompt, temperature=0.3)
            topics_data = json.loads(response)
            
            # topic_id ì¶”ê°€
            topics = []
            for i, topic in enumerate(topics_data):
                topics.append({
                    'topic_id': i,
                    'topic_label': topic['topic_label'],
                    'keywords': topic['keywords'][:8],  # ìµœëŒ€ 8ê°œ
                    'document_count': 0,
                    'representative_docs': [],
                    'doc_ids': []
                })
            
            return topics
            
        except Exception as e:
            logger.error(f"LLM ì£¼ì œ ê·¸ë£¹í•‘ ì‹¤íŒ¨: {str(e)}")
            # í´ë°±: ë‹¨ìˆœ ê·¸ë£¹í•‘
            return self._simple_topic_grouping(top_keywords)
    
    def _simple_topic_grouping(self, top_keywords: List[tuple]) -> List[Dict[str, Any]]:
        """ë‹¨ìˆœ í‚¤ì›Œë“œ ê·¸ë£¹í•‘ (í´ë°±)"""
        # ìƒìœ„ í‚¤ì›Œë“œë¥¼ 3ê°œ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
        topics = []
        keywords_per_topic = len(top_keywords) // 3 + 1
        
        for i in range(3):
            start_idx = i * keywords_per_topic
            end_idx = start_idx + keywords_per_topic
            topic_keywords = [kw for kw, freq in top_keywords[start_idx:end_idx]]
            
            if topic_keywords:
                topics.append({
                    'topic_id': i,
                    'topic_label': f"{topic_keywords[0]} ê´€ë ¨ ë…¼ì˜",
                    'keywords': topic_keywords,
                    'document_count': 0,
                    'representative_docs': [],
                    'doc_ids': []
                })
        
        return topics
    
    async def _create_single_topic(self, documents: List[Dict]) -> List[Dict[str, Any]]:
        """ë¬¸ì„œê°€ ì ì„ ë•Œ ë‹¨ì¼ ì£¼ì œë¡œ ì²˜ë¦¬"""
        logger.info("ğŸ“¦ ë‹¨ì¼ ì£¼ì œë¡œ ì²˜ë¦¬")
        
        if not documents:
            return []
        
        texts = [doc['raw_text'] for doc in documents if doc.get('raw_text')]
        doc_ids = [doc['content_id'] for doc in documents if doc.get('raw_text')]
        
        # ëª¨ë“  ë¬¸ì„œë¥¼ ì£¼ì œ 0ìœ¼ë¡œ í• ë‹¹
        for doc_id in doc_ids:
            self.client.table('source_contents')\
                .update({'topic_id': 0})\
                .eq('content_id', doc_id)\
                .execute()
        
        return [{
            'topic_id': 0,
            'topic_label': 'ì „ì²´ ë‚´ìš© ì¢…í•©',
            'document_count': len(texts),
            'keywords': ['ì¢…í•©', 'ì „ì²´', 'ë¶„ì„'],
            'representative_docs': texts[:3],
            'doc_ids': doc_ids
        }]