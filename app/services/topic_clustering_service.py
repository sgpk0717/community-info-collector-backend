from typing import List, Dict, Any, Optional, Tuple
from app.services.llm_service import LLMService
import logging
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict

logger = logging.getLogger(__name__)

class TopicClusteringService:
    """ë™ì  ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§ ì„œë¹„ìŠ¤
    
    í•„í„°ë§ëœ ì½˜í…ì¸ ë¥¼ ì˜ë¯¸ìˆëŠ” ì£¼ì œë³„ë¡œ ìë™ ê·¸ë£¹í™”í•˜ì—¬
    ì²´ê³„ì ì´ê³  êµ¬ì¡°í™”ëœ ë³´ê³ ì„œ ìƒì„±ì„ ì§€ì›í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, thread_pool: Optional[ThreadPoolExecutor] = None, api_semaphore: Optional[asyncio.Semaphore] = None):
        self.llm_service = LLMService(api_semaphore=api_semaphore)
        self.thread_pool = thread_pool
        self.api_semaphore = api_semaphore
        
        # í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì •
        self.MIN_CLUSTER_SIZE = 3  # í´ëŸ¬ìŠ¤í„°ë¡œ ì¸ì •ë˜ëŠ” ìµœì†Œ ì½˜í…ì¸  ìˆ˜
        self.MAX_CLUSTERS = 7  # ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜ (ë„ˆë¬´ ë§ìœ¼ë©´ ë³µì¡í•´ì§)
        self.MAX_ITEMS_PER_BATCH = 20  # LLM ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°°ì¹˜ í¬ê¸°
    
    async def cluster_content(
        self, 
        content_items: List[Dict[str, Any]], 
        query: str
    ) -> Dict[str, Any]:
        """ì½˜í…ì¸ ë¥¼ ì£¼ì œë³„ë¡œ í´ëŸ¬ìŠ¤í„°ë§
        
        Args:
            content_items: í•„í„°ë§ëœ ê³ í’ˆì§ˆ ì½˜í…ì¸  ëª©ë¡
            query: ì›ë³¸ ê²€ìƒ‰ í‚¤ì›Œë“œ
        
        Returns:
            í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ (í´ëŸ¬ìŠ¤í„° ëª©ë¡, í†µê³„, ë©”íƒ€ë°ì´í„°)
        """
        if not content_items:
            logger.warning("ğŸ“­ í´ëŸ¬ìŠ¤í„°ë§í•  ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {
                'clusters': [],
                'unclustered': [],
                'statistics': {}
            }
        
        logger.info(f"ğŸ¯ ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘ - ëŒ€ìƒ: {len(content_items)}ê°œ ì½˜í…ì¸ ")
        logger.info(f"   í‚¤ì›Œë“œ: '{query}'")
        
        # 1ë‹¨ê³„: ì£¼ì œ ì¶”ì¶œ
        topics = await self._extract_topics(content_items, query)
        logger.info(f"ğŸ“‹ ì¶”ì¶œëœ ì£¼ì œ: {len(topics)}ê°œ")
        for idx, topic in enumerate(topics[:5]):  # ìƒìœ„ 5ê°œë§Œ ë¡œê·¸
            logger.info(f"   {idx+1}. {topic['name']} - {topic['description']}")
        
        # 2ë‹¨ê³„: ì½˜í…ì¸ ë¥¼ ì£¼ì œë³„ë¡œ ë¶„ë¥˜
        clusters = await self._assign_content_to_topics(content_items, topics)
        
        # 3ë‹¨ê³„: ì‘ì€ í´ëŸ¬ìŠ¤í„° ë³‘í•© ë° ì •ë¦¬
        final_clusters = self._optimize_clusters(clusters)
        
        # 4ë‹¨ê³„: í´ëŸ¬ìŠ¤í„° í†µê³„ ìƒì„±
        statistics = self._generate_cluster_statistics(final_clusters, content_items)
        
        logger.info(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ:")
        logger.info(f"   ìµœì¢… í´ëŸ¬ìŠ¤í„°: {len(final_clusters)}ê°œ")
        logger.info(f"   í´ëŸ¬ìŠ¤í„°ëœ ì½˜í…ì¸ : {statistics['total_clustered']}ê°œ")
        logger.info(f"   ë¯¸ë¶„ë¥˜ ì½˜í…ì¸ : {statistics['total_unclustered']}ê°œ")
        
        return {
            'clusters': final_clusters,
            'unclustered': [item for item in content_items if not self._is_item_clustered(item, final_clusters)],
            'statistics': statistics
        }
    
    async def _extract_topics(self, content_items: List[Dict[str, Any]], query: str) -> List[Dict[str, str]]:
        """ì½˜í…ì¸ ì—ì„œ ì£¼ìš” ì£¼ì œ ì¶”ì¶œ"""
        
        # ìƒ˜í”Œ ì½˜í…ì¸  ì¤€ë¹„ (ì„±ëŠ¥ì„ ìœ„í•´ ì¼ë¶€ë§Œ ì‚¬ìš©)
        sample_size = min(len(content_items), 30)
        sample_items = sorted(content_items, key=lambda x: x.get('relevance_score', 0), reverse=True)[:sample_size]
        
        # ì½˜í…ì¸  í…ìŠ¤íŠ¸ ì¤€ë¹„
        content_texts = []
        for idx, item in enumerate(sample_items):
            title = item.get('title', '') if item.get('type') == 'post' else ''
            content = item.get('content', item.get('selftext', ''))[:200]
            relevance = item.get('relevance_score', 0)
            
            text = f"[{idx+1}] {title}\n{content}\nê´€ë ¨ì„±: {relevance}/10"
            content_texts.append(text)
        
        prompt = f"""ë‹¤ìŒì€ "{query}" í‚¤ì›Œë“œë¡œ ìˆ˜ì§‘ëœ ê³ í’ˆì§ˆ ì½˜í…ì¸ ë“¤ì…ë‹ˆë‹¤.

{chr(10).join(content_texts)}

ì´ ì½˜í…ì¸ ë“¤ì„ ë¶„ì„í•˜ì—¬ ì£¼ìš” í† í”½/ì£¼ì œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. 5-7ê°œì˜ ëª…í™•í•˜ê³  êµ¬ë³„ë˜ëŠ” ì£¼ì œë¥¼ ì¶”ì¶œ
2. ê° ì£¼ì œëŠ” ì—¬ëŸ¬ ì½˜í…ì¸ ì—ì„œ ê³µí†µì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒì´ì–´ì•¼ í•¨
3. ë„ˆë¬´ ê´‘ë²”ìœ„í•˜ê±°ë‚˜ ë„ˆë¬´ ì„¸ë¶€ì ì´ì§€ ì•Šì€ ì ì ˆí•œ ìˆ˜ì¤€
4. ê° ì£¼ì œì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª… í¬í•¨

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
[
    {{
        "name": "ì£¼ì œëª… (2-4 ë‹¨ì–´)",
        "description": "ì´ ì£¼ì œì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª… (1ë¬¸ì¥)",
        "keywords": ["ê´€ë ¨", "í‚¤ì›Œë“œ", "3-5ê°œ"]
    }},
    ...
]"""

        try:
            response = await self.llm_service._call_llm(prompt, temperature=0.3)
            topics = self._parse_topics_response(response)
            
            # ê¸°ë³¸ ì£¼ì œê°€ ì—†ìœ¼ë©´ ì¶”ê°€
            if len(topics) < 3:
                topics.extend([
                    {"name": "ì¼ë°˜ ë…¼ì˜", "description": "íŠ¹ì • ì£¼ì œë¡œ ë¶„ë¥˜ë˜ì§€ ì•ŠëŠ” ì¼ë°˜ì ì¸ ë…¼ì˜", "keywords": ["general", "discussion"]},
                    {"name": "ê¸°íƒ€ ì˜ê²¬", "description": "ë‹¤ì–‘í•œ ê°œì¸ì  ì˜ê²¬ê³¼ ê²½í—˜", "keywords": ["opinion", "experience"]}
                ])
            
            return topics[:self.MAX_CLUSTERS]
            
        except Exception as e:
            logger.error(f"âŒ ì£¼ì œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ë¥¼ ì „íŒŒ
            raise Exception(f"LLM ì£¼ì œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    async def _assign_content_to_topics(
        self, 
        content_items: List[Dict[str, Any]], 
        topics: List[Dict[str, str]]
    ) -> List[Dict[str, Any]]:
        """ê° ì½˜í…ì¸ ë¥¼ ì ì ˆí•œ ì£¼ì œì— í• ë‹¹"""
        
        clusters = []
        for topic in topics:
            clusters.append({
                'topic': topic,
                'items': [],
                'average_relevance': 0.0,
                'key_insights': []
            })
        
        # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        batch_size = self.MAX_ITEMS_PER_BATCH
        for i in range(0, len(content_items), batch_size):
            batch = content_items[i:i + batch_size]
            
            # ê° ë°°ì¹˜ì— ëŒ€í•´ ì£¼ì œ í• ë‹¹
            assignments = await self._assign_batch_to_topics(batch, topics)
            
            # í• ë‹¹ ê²°ê³¼ë¥¼ í´ëŸ¬ìŠ¤í„°ì— ì¶”ê°€
            for item_idx, topic_idx in enumerate(assignments):
                if 0 <= topic_idx < len(clusters):
                    clusters[topic_idx]['items'].append(batch[item_idx])
        
        # ê° í´ëŸ¬ìŠ¤í„°ì˜ í†µê³„ ê³„ì‚°
        for cluster in clusters:
            if cluster['items']:
                relevance_scores = [item.get('relevance_score', 0) for item in cluster['items']]
                cluster['average_relevance'] = sum(relevance_scores) / len(relevance_scores)
                
                # í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ (ìƒìœ„ 3ê°œ ê³ ë“ì  ì½˜í…ì¸ )
                top_items = sorted(cluster['items'], key=lambda x: x.get('score', 0), reverse=True)[:3]
                cluster['key_insights'] = [
                    {
                        'title': item.get('title', 'ì œëª© ì—†ìŒ'),
                        'score': item.get('score', 0),
                        'type': item.get('type', 'unknown')
                    }
                    for item in top_items
                ]
        
        return clusters
    
    async def _assign_batch_to_topics(
        self, 
        batch: List[Dict[str, Any]], 
        topics: List[Dict[str, str]]
    ) -> List[int]:
        """ë°°ì¹˜ ì½˜í…ì¸ ë¥¼ ì£¼ì œì— í• ë‹¹"""
        
        # ì½˜í…ì¸ ì™€ ì£¼ì œ ì •ë³´ ì¤€ë¹„
        content_descriptions = []
        for idx, item in enumerate(batch):
            title = item.get('title', '') if item.get('type') == 'post' else 'ëŒ“ê¸€'
            content = item.get('content', item.get('selftext', ''))[:150]
            content_descriptions.append(f"[{idx}] {title}\n{content}")
        
        topic_descriptions = []
        for idx, topic in enumerate(topics):
            topic_descriptions.append(f"[{idx}] {topic['name']}: {topic['description']}")
        
        prompt = f"""ë‹¤ìŒ ì½˜í…ì¸ ë“¤ì„ ê°€ì¥ ì í•©í•œ ì£¼ì œì— í• ë‹¹í•´ì£¼ì„¸ìš”.

ì£¼ì œ ëª©ë¡:
{chr(10).join(topic_descriptions)}

ì½˜í…ì¸ :
{chr(10).join(content_descriptions)}

ê° ì½˜í…ì¸ ì— ëŒ€í•´ ê°€ì¥ ì í•©í•œ ì£¼ì œì˜ ë²ˆí˜¸ë¥¼ í• ë‹¹í•˜ì„¸ìš”.
ë§Œì•½ ì–´ë–¤ ì£¼ì œì—ë„ ë§ì§€ ì•Šìœ¼ë©´ -1ì„ í• ë‹¹í•˜ì„¸ìš”.

JSON ë°°ì—´ë¡œ ì‘ë‹µ (ì½˜í…ì¸  ìˆœì„œëŒ€ë¡œ):
[0, 2, 1, -1, 0, ...]"""

        try:
            response = await self.llm_service._call_llm(prompt, temperature=0.2)
            assignments = self._parse_assignments_response(response, len(batch))
            return assignments
            
        except Exception as e:
            logger.error(f"âŒ ì£¼ì œ í• ë‹¹ ì‹¤íŒ¨: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ë¥¼ ì „íŒŒ
            raise Exception(f"LLM ì£¼ì œ í• ë‹¹ ì‹¤íŒ¨ (ë°°ì¹˜): {str(e)}")
    
    def _optimize_clusters(self, clusters: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì‘ì€ í´ëŸ¬ìŠ¤í„° ë³‘í•© ë° ìµœì í™”"""
        
        # í¬ê¸°ê°€ ì¶©ë¶„í•œ í´ëŸ¬ìŠ¤í„°ì™€ ì‘ì€ í´ëŸ¬ìŠ¤í„° ë¶„ë¦¬
        large_clusters = []
        small_clusters = []
        
        for cluster in clusters:
            if len(cluster['items']) >= self.MIN_CLUSTER_SIZE:
                large_clusters.append(cluster)
            elif cluster['items']:  # ë¹„ì–´ìˆì§€ ì•Šì€ ì‘ì€ í´ëŸ¬ìŠ¤í„°
                small_clusters.append(cluster)
        
        # ì‘ì€ í´ëŸ¬ìŠ¤í„°ë“¤ì„ ë³‘í•©í•˜ê±°ë‚˜ ê°€ì¥ ìœ ì‚¬í•œ í° í´ëŸ¬ìŠ¤í„°ì— ë³‘í•©
        if small_clusters:
            # ëª¨ë“  ì‘ì€ í´ëŸ¬ìŠ¤í„°ì˜ ì•„ì´í…œì„ ëª¨ìŒ
            orphan_items = []
            for cluster in small_clusters:
                orphan_items.extend(cluster['items'])
            
            if orphan_items:
                # "ê¸°íƒ€" í´ëŸ¬ìŠ¤í„° ìƒì„±
                misc_cluster = {
                    'topic': {
                        'name': 'ê¸°íƒ€ ê´€ë ¨ ë‚´ìš©',
                        'description': 'ë‹¤ì–‘í•œ ê´€ë ¨ ì£¼ì œë“¤',
                        'keywords': []
                    },
                    'items': orphan_items,
                    'average_relevance': sum(item.get('relevance_score', 0) for item in orphan_items) / len(orphan_items),
                    'key_insights': []
                }
                
                # í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
                top_items = sorted(orphan_items, key=lambda x: x.get('score', 0), reverse=True)[:3]
                misc_cluster['key_insights'] = [
                    {
                        'title': item.get('title', 'ì œëª© ì—†ìŒ'),
                        'score': item.get('score', 0),
                        'type': item.get('type', 'unknown')
                    }
                    for item in top_items
                ]
                
                large_clusters.append(misc_cluster)
        
        # í´ëŸ¬ìŠ¤í„°ë¥¼ í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬
        final_clusters = sorted(large_clusters, key=lambda x: len(x['items']), reverse=True)
        
        return final_clusters
    
    def _generate_cluster_statistics(
        self, 
        clusters: List[Dict[str, Any]], 
        all_items: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ í†µê³„ ìƒì„±"""
        
        total_clustered = sum(len(cluster['items']) for cluster in clusters)
        total_items = len(all_items)
        
        cluster_sizes = [len(cluster['items']) for cluster in clusters]
        
        statistics = {
            'total_items': total_items,
            'total_clustered': total_clustered,
            'total_unclustered': total_items - total_clustered,
            'num_clusters': len(clusters),
            'average_cluster_size': total_clustered / len(clusters) if clusters else 0,
            'largest_cluster_size': max(cluster_sizes) if cluster_sizes else 0,
            'smallest_cluster_size': min(cluster_sizes) if cluster_sizes else 0,
            'cluster_distribution': {}
        }
        
        # ê° í´ëŸ¬ìŠ¤í„°ì˜ ë¶„í¬
        for cluster in clusters:
            topic_name = cluster['topic']['name']
            statistics['cluster_distribution'][topic_name] = {
                'count': len(cluster['items']),
                'percentage': (len(cluster['items']) / total_items * 100) if total_items > 0 else 0,
                'average_relevance': cluster['average_relevance']
            }
        
        return statistics
    
    def _is_item_clustered(self, item: Dict[str, Any], clusters: List[Dict[str, Any]]) -> bool:
        """ì•„ì´í…œì´ í´ëŸ¬ìŠ¤í„°ì— í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        item_id = item.get('id')
        for cluster in clusters:
            for cluster_item in cluster['items']:
                if cluster_item.get('id') == item_id:
                    return True
        return False
    
    def _parse_topics_response(self, response: str) -> List[Dict[str, str]]:
        """LLM ì‘ë‹µì—ì„œ ì£¼ì œ ëª©ë¡ íŒŒì‹±"""
        try:
            # JSON ë¸”ë¡ ì¶”ì¶œ
            response_clean = response.strip()
            if '```json' in response_clean:
                response_clean = response_clean.split('```json')[1].split('```')[0].strip()
            elif '```' in response_clean:
                response_clean = response_clean.split('```')[1].strip()
            
            topics = json.loads(response_clean)
            
            if not isinstance(topics, list):
                raise ValueError("ì‘ë‹µì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœê°€ ì•„ë‹˜")
            
            # ê° ì£¼ì œ ê²€ì¦
            validated_topics = []
            for topic in topics:
                if isinstance(topic, dict) and 'name' in topic and 'description' in topic:
                    validated_topics.append({
                        'name': topic['name'],
                        'description': topic['description'],
                        'keywords': topic.get('keywords', [])
                    })
            
            return validated_topics
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì£¼ì œ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _parse_assignments_response(self, response: str, expected_length: int) -> List[int]:
        """LLM ì‘ë‹µì—ì„œ ì£¼ì œ í• ë‹¹ ê²°ê³¼ íŒŒì‹±"""
        try:
            # JSON ë°°ì—´ ì¶”ì¶œ
            response_clean = response.strip()
            if '[' in response_clean:
                start_idx = response_clean.find('[')
                end_idx = response_clean.rfind(']') + 1
                response_clean = response_clean[start_idx:end_idx]
            
            assignments = json.loads(response_clean)
            
            if not isinstance(assignments, list):
                raise ValueError("ì‘ë‹µì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœê°€ ì•„ë‹˜")
            
            # ê¸¸ì´ ë§ì¶”ê¸°
            if len(assignments) < expected_length:
                # ë¶€ì¡±í•œ ë¶€ë¶„ì€ 0(ì²« ë²ˆì§¸ ì£¼ì œ)ìœ¼ë¡œ ì±„ìš°ê¸°
                assignments.extend([0] * (expected_length - len(assignments)))
            elif len(assignments) > expected_length:
                # ì´ˆê³¼ ë¶€ë¶„ ì œê±°
                assignments = assignments[:expected_length]
            
            # ìœ íš¨ì„± ê²€ì¦ (ìŒìˆ˜ê°€ ì•„ë‹Œ ì •ìˆ˜)
            validated = []
            for val in assignments:
                if isinstance(val, int) and val >= -1:
                    validated.append(val)
                else:
                    validated.append(0)  # ì˜ëª»ëœ ê°’ì€ ì²« ë²ˆì§¸ ì£¼ì œë¡œ
            
            return validated
            
        except Exception as e:
            logger.warning(f"âš ï¸ í• ë‹¹ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            # í´ë°±: ëª¨ë‘ ì²« ë²ˆì§¸ ì£¼ì œì— í• ë‹¹
            return [0] * expected_length
    
    def get_cluster_summary(self, clusters: List[Dict[str, Any]]) -> str:
        """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ì½ê¸° ì‰¬ìš´ ìš”ì•½ìœ¼ë¡œ ë³€í™˜"""
        
        if not clusters:
            return "í´ëŸ¬ìŠ¤í„°ë§ëœ ì£¼ì œê°€ ì—†ìŠµë‹ˆë‹¤."
        
        summary_lines = ["ğŸ“Š ì£¼ì œë³„ ë¶„ë¥˜ ê²°ê³¼:\n"]
        
        for idx, cluster in enumerate(clusters, 1):
            topic = cluster['topic']
            item_count = len(cluster['items'])
            avg_relevance = cluster['average_relevance']
            
            summary_lines.append(f"{idx}. {topic['name']} ({item_count}ê°œ ì½˜í…ì¸ )")
            summary_lines.append(f"   - {topic['description']}")
            summary_lines.append(f"   - í‰ê·  ê´€ë ¨ì„±: {avg_relevance:.1f}/10")
            
            if cluster['key_insights']:
                summary_lines.append("   - ì£¼ìš” ë‚´ìš©:")
                for insight in cluster['key_insights'][:2]:
                    summary_lines.append(f"     â€¢ {insight['title'][:50]}...")
            
            summary_lines.append("")
        
        return "\n".join(summary_lines)