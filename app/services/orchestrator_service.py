from typing import List, Dict, Any, Optional, Tuple
from app.services.llm_service import LLMService
from app.services.reddit_service import RedditService
from app.services.relevance_filtering_service import RelevanceFilteringService
from app.services.topic_clustering_service import TopicClusteringService
import logging
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor
from app.schemas.search import SearchRequest, ReportLength

logger = logging.getLogger(__name__)

class OrchestratorService:
    """
    ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„œë¹„ìŠ¤
    - ê° ë‹¨ê³„ì˜ í’ˆì§ˆ ê²€ì¦
    - ì„¹ì…˜ë³„ ì¤‘ë³µ ì œê±°
    - ì¼ê´€ëœ í’ˆì§ˆ ë³´ì¥
    """
    
    def __init__(self, thread_pool: Optional[ThreadPoolExecutor] = None, api_semaphore: Optional[asyncio.Semaphore] = None):
        self.llm_service = LLMService(api_semaphore=api_semaphore)
        self.reddit_service = RedditService(thread_pool=thread_pool)
        self.relevance_service = RelevanceFilteringService(thread_pool=thread_pool, api_semaphore=api_semaphore)
        self.clustering_service = TopicClusteringService(thread_pool=thread_pool, api_semaphore=api_semaphore)
        self.thread_pool = thread_pool
        self.api_semaphore = api_semaphore
    
    async def orchestrate_analysis(
        self, 
        request: SearchRequest,
        progress_callback: Optional[Any] = None
    ) -> Dict[str, Any]:
        """
        ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ë¥¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
        """
        try:
            logger.info(f"ğŸ¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹œì‘ - í‚¤ì›Œë“œ: '{request.query}'")
            
            # 1ë‹¨ê³„: í‚¤ì›Œë“œ í™•ì¥
            if progress_callback:
                await progress_callback("í‚¤ì›Œë“œ ë¶„ì„ ì¤‘", 10)
            
            expanded_keywords = await self.llm_service.expand_keywords(request.query)
            logger.info(f"âœ… í‚¤ì›Œë“œ í™•ì¥ ì™„ë£Œ: {len(expanded_keywords)}ê°œ")
            
            # 2ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘ (ê²Œì‹œë¬¼ + ëŒ“ê¸€)
            if progress_callback:
                await progress_callback("ë°ì´í„° ìˆ˜ì§‘ ì¤‘", 20)
            
            collection_result = await self._collect_data_with_quality_check(
                expanded_keywords, 
                progress_callback
            )
            
            # 3ë‹¨ê³„: ê´€ë ¨ì„± í•„í„°ë§
            if progress_callback:
                await progress_callback("ê´€ë ¨ì„± ë¶„ì„ ì¤‘", 40)
            
            filtered_content = await self._filter_with_quality_check(
                collection_result['content'],
                request.query,
                expanded_keywords
            )
            
            # 4ë‹¨ê³„: ì£¼ì œë³„ í´ëŸ¬ìŠ¤í„°ë§
            if progress_callback:
                await progress_callback("ì£¼ì œ ë¶„ë¥˜ ì¤‘", 55)
            
            clustering_result = await self._cluster_with_quality_check(
                filtered_content,
                request.query
            )
            
            # 5ë‹¨ê³„: í†µí•© ë³´ê³ ì„œ ìƒì„±
            if progress_callback:
                await progress_callback("ë³´ê³ ì„œ ì‘ì„± ì¤‘", 70)
            
            report = await self._generate_quality_report(
                clustering_result,
                request.query,
                request.length,
                expanded_keywords
            )
            
            # 6ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦ ë° ê°œì„ 
            if progress_callback:
                await progress_callback("í’ˆì§ˆ ê²€ì¦ ì¤‘", 85)
            
            final_report = await self._quality_assurance(report)
            
            if progress_callback:
                await progress_callback("ë¶„ì„ ì™„ë£Œ", 100)
            
            logger.info("ğŸ‰ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì™„ë£Œ!")
            
            return {
                'report': final_report,
                'metadata': {
                    'expanded_keywords': expanded_keywords,
                    'total_collected': collection_result['total'],
                    'filtered_count': len(filtered_content),
                    'cluster_count': len(clustering_result['clusters']),
                    'quality_score': final_report.get('quality_score', 0)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def _collect_data_with_quality_check(
        self, 
        keywords: List[str], 
        progress_callback: Optional[Any] = None
    ) -> Dict[str, Any]:
        """ë°ì´í„° ìˆ˜ì§‘ ë° í’ˆì§ˆ ì²´í¬"""
        all_content = []
        
        # í‚¤ì›Œë“œë³„ë¡œ ìˆ˜ì§‘
        for idx, keyword in enumerate(keywords[:10]):  # ìµœëŒ€ 10ê°œ í‚¤ì›Œë“œ
            if progress_callback:
                progress = 20 + (idx / len(keywords[:10])) * 15
                await progress_callback(f"'{keyword}' ìˆ˜ì§‘ ì¤‘", int(progress))
            
            # collect_posts_with_commentsëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•¨
            content_items = await self.reddit_service.collect_posts_with_comments(
                keywords=[keyword],  # keywords íŒŒë¼ë¯¸í„°ë¡œ ë³€ê²½
                posts_limit=15  # posts_limit íŒŒë¼ë¯¸í„°ëª…ìœ¼ë¡œ ë³€ê²½
            )
            
            # ìˆ˜ì§‘ëœ ì½˜í…ì¸ ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for item in content_items:
                item['keyword_source'] = keyword
                all_content.append(item)
        
        # ì¤‘ë³µ ì œê±°
        unique_content = self._remove_duplicates(all_content)
        
        logger.info(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: ì „ì²´ {len(all_content)}ê°œ â†’ ì¤‘ë³µ ì œê±° í›„ {len(unique_content)}ê°œ")
        
        return {
            'content': unique_content,
            'total': len(all_content),
            'unique': len(unique_content)
        }
    
    async def _filter_with_quality_check(
        self,
        content: List[Dict[str, Any]],
        query: str,
        keywords: List[str]
    ) -> List[Dict[str, Any]]:
        """ê´€ë ¨ì„± í•„í„°ë§ ë° í’ˆì§ˆ ê²€ì¦"""
        # ê´€ë ¨ì„± í•„í„°ë§
        filtered = await self.relevance_service.filter_relevant_content(
            content_items=content,
            query=query,
            expanded_keywords=keywords
        )
        
        # í’ˆì§ˆ ê¸°ì¤€ í™•ì¸
        high_quality = [item for item in filtered if item.get('relevance_score', 0) >= 7.5]
        medium_quality = [item for item in filtered if 6 <= item.get('relevance_score', 0) < 7.5]
        
        logger.info(f"ğŸ¯ í’ˆì§ˆ ë¶„í¬: ê³ í’ˆì§ˆ {len(high_quality)}ê°œ, ì¤‘í’ˆì§ˆ {len(medium_quality)}ê°œ")
        
        # ìµœì†Œ í’ˆì§ˆ ê¸°ì¤€ ë³´ì¥
        if len(high_quality) < 5:
            logger.warning("âš ï¸ ê³ í’ˆì§ˆ ì½˜í…ì¸  ë¶€ì¡±, ì¤‘í’ˆì§ˆ ì½˜í…ì¸  í¬í•¨")
            filtered = high_quality + medium_quality[:10-len(high_quality)]
        else:
            filtered = high_quality
        
        return filtered
    
    async def _cluster_with_quality_check(
        self,
        content: List[Dict[str, Any]],
        query: str
    ) -> Dict[str, Any]:
        """ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§ ë° í’ˆì§ˆ ê²€ì¦"""
        # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        clustering_result = await self.clustering_service.cluster_content(
            content_items=content,
            query=query
        )
        
        # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ê²€ì¦
        quality_clusters = []
        for cluster in clustering_result['clusters']:
            # ë„ˆë¬´ ì‘ì€ í´ëŸ¬ìŠ¤í„°ëŠ” ì œì™¸
            if len(cluster['items']) >= 2:
                quality_clusters.append(cluster)
            else:
                logger.info(f"ğŸ” ì‘ì€ í´ëŸ¬ìŠ¤í„° ì œì™¸: {cluster['topic']['name']} ({len(cluster['items'])}ê°œ)")
        
        # í´ëŸ¬ìŠ¤í„° ì¬ì •ë ¬ (í¬ê¸°ìˆœ)
        quality_clusters.sort(key=lambda x: len(x['items']), reverse=True)
        
        return {
            'clusters': quality_clusters,
            'statistics': clustering_result['statistics']
        }
    
    async def _generate_quality_report(
        self,
        clustering_result: Dict[str, Any],
        query: str,
        length: ReportLength,
        keywords: List[str]
    ) -> Dict[str, Any]:
        """í†µí•© í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
        
        # êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
        structured_prompt = await self._create_structured_prompt(
            clustering_result,
            query,
            length,
            keywords
        )
        
        # LLM í˜¸ì¶œë¡œ ë³´ê³ ì„œ ìƒì„±
        try:
            response = await self.llm_service._call_llm(
                structured_prompt,
                temperature=0.3  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ temperature
            )
            
            # ë³´ê³ ì„œ íŒŒì‹±
            report_sections = self._parse_report_sections(response)
            
            return {
                'full_report': response,
                'sections': report_sections,
                'quality_score': await self._calculate_quality_score(report_sections)
            }
            
        except Exception as e:
            logger.error(f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def _quality_assurance(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """ìµœì¢… í’ˆì§ˆ ë³´ì¦ ë° ê°œì„ """
        
        # 1. ì¤‘ë³µ ë‚´ìš© ì œê±°
        cleaned_sections = await self._remove_section_duplicates(report['sections'])
        
        # 2. ì¼ê´€ì„± ê²€ì¦
        consistency_score = await self._check_consistency(cleaned_sections)
        
        # 3. í•„ìˆ˜ ìš”ì†Œ í™•ì¸
        completeness = self._check_completeness(cleaned_sections)
        
        # 4. ê°œì„ ì´ í•„ìš”í•œ ê²½ìš° ì¬ìƒì„±
        if consistency_score < 0.7 or not completeness['is_complete']:
            logger.info("ğŸ“ í’ˆì§ˆ ê°œì„ ì„ ìœ„í•œ ë³´ê³ ì„œ ì¬ìƒì„±")
            improved_report = await self._improve_report(report, completeness['missing'])
            # ê°œì„ ëœ ë³´ê³ ì„œì—ì„œ summaryì™€ full_report í™•ì¸
            return {
                'summary': improved_report.get('summary', self._extract_summary(improved_report.get('full_report', ''))),
                'full_report': improved_report.get('full_report', ''),
                'quality_metrics': {
                    'consistency_score': consistency_score,
                    'completeness': completeness,
                    'quality_score': improved_report.get('quality_score', 0)
                }
            }
        
        # 5. ìµœì¢… í¬ë§·íŒ…
        final_report = self._format_final_report(cleaned_sections)
        
        return {
            'summary': self._extract_summary(final_report),
            'full_report': final_report,
            'quality_metrics': {
                'consistency_score': consistency_score,
                'completeness': completeness,
                'quality_score': report.get('quality_score', 0)
            }
        }
    
    async def _create_structured_prompt(
        self,
        clustering_result: Dict[str, Any],
        query: str,
        length: ReportLength,
        keywords: List[str]
    ) -> str:
        """êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        clusters = clustering_result['clusters']
        
        # í´ëŸ¬ìŠ¤í„° ì •ë³´ ì •ë¦¬
        cluster_summaries = []
        for idx, cluster in enumerate(clusters, 1):
            topic = cluster['topic']
            items = cluster['items']
            
            # ê° í´ëŸ¬ìŠ¤í„°ì˜ í•µì‹¬ ì½˜í…ì¸  ì¶”ì¶œ
            top_items = sorted(items, key=lambda x: x.get('relevance_score', 0), reverse=True)[:3]
            
            cluster_summary = f"""
ì£¼ì œ {idx}: {topic['name']}
ì„¤ëª…: {topic['description']}
ê´€ë ¨ ì½˜í…ì¸  ìˆ˜: {len(items)}ê°œ
í•µì‹¬ ë‚´ìš©:
"""
            for item in top_items:
                if item['type'] == 'post':
                    cluster_summary += f"- {item['title'][:80]}... (ì ìˆ˜: {item.get('score', 0)})\n"
                else:
                    cluster_summary += f"- ëŒ“ê¸€: {item['body'][:80]}... (ì¶”ì²œ: {item.get('score', 0)})\n"
            
            cluster_summaries.append(cluster_summary)
        
        # ê¸¸ì´ë³„ ê°€ì´ë“œ
        length_guides = {
            ReportLength.simple: "ê° ì„¹ì…˜ 1-2 ë‹¨ë½, ì „ì²´ 500-700ì",
            ReportLength.moderate: "ê° ì„¹ì…˜ 2-3 ë‹¨ë½, ì „ì²´ 1000-1500ì",
            ReportLength.detailed: "ê° ì„¹ì…˜ 3-5 ë‹¨ë½, ì „ì²´ 2000-3000ì"
        }
        
        prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ì»¤ë®¤ë‹ˆí‹° ë¶„ì„ê°€ì…ë‹ˆë‹¤. '{query}' í‚¤ì›Œë“œë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê³ í’ˆì§ˆ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords[:5])}
ìˆ˜ì§‘ëœ ê³ í’ˆì§ˆ ì½˜í…ì¸ : {sum(len(c['items']) for c in clusters)}ê°œ

ì£¼ì œë³„ ë¶„ë¥˜ ê²°ê³¼:
{''.join(cluster_summaries)}

ë‹¤ìŒ êµ¬ì¡°ë¡œ {length_guides[length]} ë¶„ëŸ‰ì˜ í•œêµ­ì–´ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

## 1. í•µì‹¬ ìš”ì•½
- ì „ì²´ ì—¬ë¡ ê³¼ íŠ¸ë Œë“œë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
- ê°€ì¥ ì¤‘ìš”í•œ ë°œê²¬ì‚¬í•­ 1-2ê°€ì§€ ê°•ì¡°

## 2. ì£¼ìš” ì£¼ì œ ë¶„ì„
- ìœ„ì—ì„œ ë¶„ë¥˜ëœ ê° ì£¼ì œë³„ë¡œ ì„¹ì…˜ êµ¬ì„±
- ê° ì£¼ì œì˜ í•µì‹¬ ë…¼ì ê³¼ ì—¬ë¡  ì •ë¦¬
- êµ¬ì²´ì ì¸ ì‚¬ë¡€ì™€ ì¸ìš© í¬í•¨

## 3. ê°ì„± ë¶„ì„
- ì „ë°˜ì ì¸ ê°ì„± ë¶„í¬ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
- ì£¼ì œë³„ ê°ì„± ì°¨ì´ ë¶„ì„
- íŠ¹íˆ ë¶€ì •ì  ì˜ê²¬ì˜ ì£¼ìš” ì›ì¸

## 4. ì£¼ëª©í•  ë§Œí•œ ì¸ì‚¬ì´íŠ¸
- ì˜ˆìƒì¹˜ ëª»í•œ ë°œê²¬ì‚¬í•­
- ì†Œìˆ˜ì˜ê²¬ì´ì§€ë§Œ ì¤‘ìš”í•œ ê´€ì 
- í–¥í›„ ì£¼ëª©í•´ì•¼ í•  ì‹ í˜¸

## 5. ê²°ë¡  ë° ì‹œì‚¬ì 
- ì „ì²´ ë¶„ì„ ê²°ê³¼ ì¢…í•©
- ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ì œì‹œ

ì¤‘ìš” ì§€ì¹¨:
1. ê° ì„¹ì…˜ ê°„ ì¤‘ë³µì„ í”¼í•˜ê³  ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ì‚¬ë¡€ë¡œ ì£¼ì¥ ë’·ë°›ì¹¨
3. ê°ê´€ì ì´ê³  ê· í˜•ì¡íŒ ì‹œê° ìœ ì§€
4. ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰¬ìš´ ë¬¸ì²´ ì‚¬ìš©"""
        
        return prompt
    
    def _remove_duplicates(self, content: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ì½˜í…ì¸  ì œê±°"""
        seen_ids = set()
        unique_content = []
        
        for item in content:
            item_id = item.get('id')
            if item_id and item_id not in seen_ids:
                seen_ids.add(item_id)
                unique_content.append(item)
        
        return unique_content
    
    def _parse_report_sections(self, report_text: str) -> Dict[str, str]:
        """ë³´ê³ ì„œë¥¼ ì„¹ì…˜ë³„ë¡œ íŒŒì‹±"""
        sections = {
            'summary': '',
            'topic_analysis': '',
            'sentiment_analysis': '',
            'insights': '',
            'conclusion': ''
        }
        
        # ì„¹ì…˜ ë§¤í•‘
        section_markers = {
            '## 1. í•µì‹¬ ìš”ì•½': 'summary',
            '## 2. ì£¼ìš” ì£¼ì œ ë¶„ì„': 'topic_analysis',
            '## 3. ê°ì„± ë¶„ì„': 'sentiment_analysis',
            '## 4. ì£¼ëª©í•  ë§Œí•œ ì¸ì‚¬ì´íŠ¸': 'insights',
            '## 5. ê²°ë¡  ë° ì‹œì‚¬ì ': 'conclusion'
        }
        
        current_section = None
        lines = report_text.split('\n')
        
        for line in lines:
            # ì„¹ì…˜ ì‹œì‘ í™•ì¸
            for marker, section_name in section_markers.items():
                if line.strip().startswith(marker):
                    current_section = section_name
                    break
            else:
                # í˜„ì¬ ì„¹ì…˜ì— ë‚´ìš© ì¶”ê°€
                if current_section:
                    sections[current_section] += line + '\n'
        
        return sections
    
    async def _calculate_quality_score(self, sections: Dict[str, str]) -> float:
        """ë³´ê³ ì„œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ê° ì„¹ì…˜ ì¡´ì¬ ì—¬ë¶€ (50%)
        for section, content in sections.items():
            if content.strip():
                score += 0.1
        
        # ì„¹ì…˜ë³„ ìµœì†Œ ê¸¸ì´ ì¶©ì¡± (30%)
        min_lengths = {
            'summary': 100,
            'topic_analysis': 300,
            'sentiment_analysis': 200,
            'insights': 150,
            'conclusion': 150
        }
        
        for section, min_length in min_lengths.items():
            if len(sections.get(section, '')) >= min_length:
                score += 0.06
        
        # êµ¬ì²´ì  ìˆ˜ì¹˜/ì¸ìš© í¬í•¨ ì—¬ë¶€ (20%)
        all_content = ' '.join(sections.values())
        if any(char.isdigit() for char in all_content):
            score += 0.1
        if '"' in all_content or 'ã€Œ' in all_content:
            score += 0.1
        
        return min(score, 1.0)
    
    async def _remove_section_duplicates(self, sections: Dict[str, str]) -> Dict[str, str]:
        """ì„¹ì…˜ ê°„ ì¤‘ë³µ ë‚´ìš© ì œê±°"""
        cleaned_sections = {}
        used_sentences = set()
        
        for section_name, content in sections.items():
            sentences = content.split('.')
            cleaned_sentences = []
            
            for sentence in sentences:
                sentence = sentence.strip()
                if sentence and sentence not in used_sentences:
                    used_sentences.add(sentence)
                    cleaned_sentences.append(sentence)
            
            cleaned_sections[section_name] = '. '.join(cleaned_sentences) + '.'
        
        return cleaned_sections
    
    async def _check_consistency(self, sections: Dict[str, str]) -> float:
        """ì„¹ì…˜ ê°„ ì¼ê´€ì„± ê²€ì¦"""
        # ê°„ë‹¨í•œ ì¼ê´€ì„± ì²´í¬ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
        consistency_score = 1.0
        
        # ìš”ì•½ê³¼ ê²°ë¡ ì˜ ì¼ì¹˜ë„ í™•ì¸
        summary = sections.get('summary', '').lower()
        conclusion = sections.get('conclusion', '').lower()
        
        # ê³µí†µ í‚¤ì›Œë“œ í™•ì¸
        summary_words = set(summary.split())
        conclusion_words = set(conclusion.split())
        
        if summary_words and conclusion_words:
            overlap = len(summary_words & conclusion_words)
            consistency_score = min(overlap / min(len(summary_words), len(conclusion_words)) * 2, 1.0)
        
        return consistency_score
    
    def _check_completeness(self, sections: Dict[str, str]) -> Dict[str, Any]:
        """ë³´ê³ ì„œ ì™„ì„±ë„ í™•ì¸"""
        required_sections = ['summary', 'topic_analysis', 'sentiment_analysis', 'insights', 'conclusion']
        missing_sections = []
        
        for section in required_sections:
            if not sections.get(section, '').strip():
                missing_sections.append(section)
        
        return {
            'is_complete': len(missing_sections) == 0,
            'missing': missing_sections,
            'completeness_ratio': (len(required_sections) - len(missing_sections)) / len(required_sections)
        }
    
    async def _improve_report(self, report: Dict[str, Any], missing_sections: List[str]) -> Dict[str, Any]:
        """ë³´ê³ ì„œ ê°œì„ """
        # ëˆ„ë½ëœ ì„¹ì…˜ì— ëŒ€í•œ ì¶”ê°€ ìƒì„± í”„ë¡¬í”„íŠ¸
        improvement_prompt = f"""ë‹¤ìŒ ë³´ê³ ì„œì—ì„œ ëˆ„ë½ëœ ì„¹ì…˜ì„ ë³´ì™„í•´ì£¼ì„¸ìš”:

í˜„ì¬ ë³´ê³ ì„œ:
{report['full_report']}

ëˆ„ë½ëœ ì„¹ì…˜: {', '.join(missing_sections)}

ìœ„ ì„¹ì…˜ë“¤ì„ ì¶”ê°€í•˜ì—¬ ì™„ì„±ë„ ë†’ì€ ë³´ê³ ì„œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."""
        
        try:
            improved_response = await self.llm_service._call_llm(improvement_prompt, temperature=0.3)
            improved_sections = self._parse_report_sections(improved_response)
            
            # ê¸°ì¡´ ì„¹ì…˜ê³¼ ë³‘í•©
            for section, content in improved_sections.items():
                if section in missing_sections and content.strip():
                    report['sections'][section] = content
            
            report['full_report'] = self._format_final_report(report['sections'])
            
        except Exception as e:
            logger.error(f"ë³´ê³ ì„œ ê°œì„  ì‹¤íŒ¨: {str(e)}")
        
        # summaryì™€ quality_metricsê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ì¶”ê°€
        if 'summary' not in report:
            report['summary'] = self._extract_summary(report['full_report'])
        if 'quality_score' not in report:
            report['quality_score'] = await self._calculate_quality_score(report['sections'])
        
        return report
    
    def _format_final_report(self, sections: Dict[str, str]) -> str:
        """ìµœì¢… ë³´ê³ ì„œ í¬ë§·íŒ…"""
        formatted_parts = []
        
        section_titles = {
            'summary': '## 1. í•µì‹¬ ìš”ì•½',
            'topic_analysis': '## 2. ì£¼ìš” ì£¼ì œ ë¶„ì„',
            'sentiment_analysis': '## 3. ê°ì„± ë¶„ì„',
            'insights': '## 4. ì£¼ëª©í•  ë§Œí•œ ì¸ì‚¬ì´íŠ¸',
            'conclusion': '## 5. ê²°ë¡  ë° ì‹œì‚¬ì '
        }
        
        for section_key, title in section_titles.items():
            content = sections.get(section_key, '').strip()
            if content:
                formatted_parts.append(f"{title}\n\n{content}")
        
        return '\n\n'.join(formatted_parts)
    
    def _extract_summary(self, report: str) -> str:
        """ë³´ê³ ì„œì—ì„œ ìš”ì•½ ì¶”ì¶œ"""
        lines = report.split('\n')
        summary_lines = []
        in_summary = False
        
        for line in lines:
            if '## 1. í•µì‹¬ ìš”ì•½' in line:
                in_summary = True
                continue
            elif line.startswith('## 2.'):
                break
            elif in_summary and line.strip():
                summary_lines.append(line.strip())
        
        return ' '.join(summary_lines[:3])  # ìµœëŒ€ 3ì¤„