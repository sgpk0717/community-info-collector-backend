from typing import List, Dict, Any, Optional, Tuple
from app.services.llm_service import LLMService
from app.services.reddit_service import RedditService
from app.services.relevance_filtering_service import RelevanceFilteringService
from app.services.topic_clustering_service import TopicClusteringService
import logging
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor
from app.schemas.search import SearchRequest, ReportLength

logger = logging.getLogger(__name__)

class OrchestratorService:
    """
    ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„œë¹„ìŠ¤
    - ê° ë‹¨ê³„ì˜ í’ˆì§ˆ ê²€ì¦
    - ì„¹ì…˜ë³„ ì¤‘ë³µ ì œê±°
    - ì¼ê´€ëœ í’ˆì§ˆ ë³´ì¥
    """
    
    def __init__(self, thread_pool: Optional[ThreadPoolExecutor] = None, api_semaphore: Optional[asyncio.Semaphore] = None):
        self.llm_service = LLMService(api_semaphore=api_semaphore)
        self.reddit_service = RedditService(thread_pool=thread_pool)
        self.relevance_service = RelevanceFilteringService(thread_pool=thread_pool, api_semaphore=api_semaphore)
        self.clustering_service = TopicClusteringService(thread_pool=thread_pool, api_semaphore=api_semaphore)
        self.thread_pool = thread_pool
        self.api_semaphore = api_semaphore
    
    async def orchestrate_analysis(
        self, 
        request: SearchRequest,
        progress_callback: Optional[Any] = None
    ) -> Dict[str, Any]:
        """
        ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ë¥¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
        """
        try:
            logger.info(f"ğŸ¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹œì‘ - í‚¤ì›Œë“œ: '{request.query}'")
            
            # 1ë‹¨ê³„: í‚¤ì›Œë“œ í™•ì¥
            if progress_callback:
                await progress_callback("í‚¤ì›Œë“œ ë¶„ì„ ì¤‘", 10)
            
            expanded_keywords = await self.llm_service.expand_keywords(request.query)
            logger.info(f"âœ… í‚¤ì›Œë“œ í™•ì¥ ì™„ë£Œ: {len(expanded_keywords)}ê°œ")
            
            # 2ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘ (ê²Œì‹œë¬¼ + ëŒ“ê¸€)
            if progress_callback:
                await progress_callback("ë°ì´í„° ìˆ˜ì§‘ ì¤‘", 20)
            
            collection_result = await self._collect_data_with_quality_check(
                expanded_keywords, 
                progress_callback,
                request
            )
            
            # 3ë‹¨ê³„: ê´€ë ¨ì„± í•„í„°ë§
            if progress_callback:
                await progress_callback("ê´€ë ¨ì„± ë¶„ì„ ì¤‘", 40)
            
            filtered_content = await self._filter_with_quality_check(
                collection_result['content'],
                request.query,
                expanded_keywords
            )
            
            # 4ë‹¨ê³„: ì£¼ì œë³„ í´ëŸ¬ìŠ¤í„°ë§
            if progress_callback:
                await progress_callback("ì£¼ì œ ë¶„ë¥˜ ì¤‘", 55)
            
            clustering_result = await self._cluster_with_quality_check(
                filtered_content,
                request.query
            )
            
            # 5ë‹¨ê³„: í†µí•© ë³´ê³ ì„œ ìƒì„±
            if progress_callback:
                await progress_callback("ë³´ê³ ì„œ ì‘ì„± ì¤‘", 70)
            
            report = await self._generate_quality_report(
                clustering_result,
                request.query,
                request.length,
                expanded_keywords
            )
            
            # 6ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦ ë° ê°œì„ 
            if progress_callback:
                await progress_callback("í’ˆì§ˆ ê²€ì¦ ì¤‘", 85)
            
            final_report = await self._quality_assurance(
                report, 
                clustering_result=clustering_result,
                query=request.query,
                keywords=expanded_keywords
            )
            
            if progress_callback:
                await progress_callback("ë¶„ì„ ì™„ë£Œ", 100)
            
            logger.info("ğŸ‰ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì™„ë£Œ!")
            
            return {
                'report': final_report,
                'metadata': {
                    'expanded_keywords': expanded_keywords,
                    'total_collected': collection_result['total'],
                    'filtered_count': len(filtered_content),
                    'cluster_count': len(clustering_result['clusters']),
                    'quality_score': final_report.get('quality_score', 0),
                    'keyword_stats': collection_result.get('keyword_stats', {})  # í‚¤ì›Œë“œë³„ í†µê³„ ì¶”ê°€
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def _collect_data_with_quality_check(
        self, 
        keywords: List[str], 
        progress_callback: Optional[Any] = None,
        request: Optional[SearchRequest] = None
    ) -> Dict[str, Any]:
        """ë°ì´í„° ìˆ˜ì§‘ ë° í’ˆì§ˆ ì²´í¬"""
        all_content = []
        keyword_stats = {}  # í‚¤ì›Œë“œë³„ í†µê³„ ì •ë³´
        
        # í‚¤ì›Œë“œë³„ë¡œ ìˆ˜ì§‘
        for idx, keyword in enumerate(keywords[:10]):  # ìµœëŒ€ 10ê°œ í‚¤ì›Œë“œ
            if progress_callback:
                progress = 20 + (idx / len(keywords[:10])) * 15
                await progress_callback(f"'{keyword}' ìˆ˜ì§‘ ì¤‘", int(progress))
            
            # collect_posts_with_commentsëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•¨
            # time_filter ë°›ì•„ì„œ ì „ë‹¬
            time_filter = 'all'
            if request and request.time_filter:
                # TimeFilter enumì„ Reddit API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                time_filter_map = {
                    '1h': 'hour',
                    '3h': 'hour',
                    '6h': 'day',
                    '12h': 'day',
                    '1d': 'day',
                    '3d': 'week',
                    '1w': 'week',
                    '1m': 'month'
                }
                time_filter = time_filter_map.get(request.time_filter.value, 'all')
            
            content_items = await self.reddit_service.collect_posts_with_comments(
                keywords=[keyword],  # keywords íŒŒë¼ë¯¸í„°ë¡œ ë³€ê²½
                posts_limit=15,  # posts_limit íŒŒë¼ë¯¸í„°ëª…ìœ¼ë¡œ ë³€ê²½
                time_filter=time_filter
            )
            
            # í‚¤ì›Œë“œë³„ í†µê³„ ì •ë³´ ìˆ˜ì§‘
            keyword_posts = [item for item in content_items if item['type'] == 'post']
            keyword_stats[keyword] = {
                'posts_found': len(keyword_posts),
                'sample_titles': [post['title'] for post in keyword_posts[:3]]  # ìƒìœ„ 3ê°œ ì œëª©
            }
            
            # ìˆ˜ì§‘ëœ ì½˜í…ì¸ ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for item in content_items:
                item['keyword_source'] = keyword
                all_content.append(item)
        
        # ì¤‘ë³µ ì œê±°
        unique_content = self._remove_duplicates(all_content)
        
        logger.info(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: ì „ì²´ {len(all_content)}ê°œ â†’ ì¤‘ë³µ ì œê±° í›„ {len(unique_content)}ê°œ")
        
        return {
            'content': unique_content,
            'total': len(all_content),
            'unique': len(unique_content),
            'keyword_stats': keyword_stats  # í‚¤ì›Œë“œë³„ í†µê³„ ì¶”ê°€
        }
    
    async def _filter_with_quality_check(
        self,
        content: List[Dict[str, Any]],
        query: str,
        keywords: List[str]
    ) -> List[Dict[str, Any]]:
        """ê´€ë ¨ì„± í•„í„°ë§ ë° í’ˆì§ˆ ê²€ì¦"""
        # ê´€ë ¨ì„± í•„í„°ë§
        filtered = await self.relevance_service.filter_relevant_content(
            content_items=content,
            query=query,
            expanded_keywords=keywords
        )
        
        # í’ˆì§ˆ ê¸°ì¤€ í™•ì¸
        high_quality = [item for item in filtered if item.get('relevance_score', 0) >= 7.5]
        medium_quality = [item for item in filtered if 6 <= item.get('relevance_score', 0) < 7.5]
        
        logger.info(f"ğŸ¯ í’ˆì§ˆ ë¶„í¬: ê³ í’ˆì§ˆ {len(high_quality)}ê°œ, ì¤‘í’ˆì§ˆ {len(medium_quality)}ê°œ")
        
        # ìµœì†Œ í’ˆì§ˆ ê¸°ì¤€ ë³´ì¥
        if len(high_quality) < 5:
            logger.warning("âš ï¸ ê³ í’ˆì§ˆ ì½˜í…ì¸  ë¶€ì¡±, ì¤‘í’ˆì§ˆ ì½˜í…ì¸  í¬í•¨")
            filtered = high_quality + medium_quality[:10-len(high_quality)]
        else:
            filtered = high_quality
        
        return filtered
    
    async def _cluster_with_quality_check(
        self,
        content: List[Dict[str, Any]],
        query: str
    ) -> Dict[str, Any]:
        """ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§ ë° í’ˆì§ˆ ê²€ì¦"""
        # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        clustering_result = await self.clustering_service.cluster_content(
            content_items=content,
            query=query
        )
        
        # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ê²€ì¦
        quality_clusters = []
        for cluster in clustering_result['clusters']:
            # ë„ˆë¬´ ì‘ì€ í´ëŸ¬ìŠ¤í„°ëŠ” ì œì™¸
            if len(cluster['items']) >= 2:
                quality_clusters.append(cluster)
            else:
                logger.info(f"ğŸ” ì‘ì€ í´ëŸ¬ìŠ¤í„° ì œì™¸: {cluster['topic']['name']} ({len(cluster['items'])}ê°œ)")
        
        # í´ëŸ¬ìŠ¤í„° ì¬ì •ë ¬ (í¬ê¸°ìˆœ)
        quality_clusters.sort(key=lambda x: len(x['items']), reverse=True)
        
        return {
            'clusters': quality_clusters,
            'statistics': clustering_result['statistics']
        }
    
    async def _generate_quality_report(
        self,
        clustering_result: Dict[str, Any],
        query: str,
        length: ReportLength,
        keywords: List[str]
    ) -> Dict[str, Any]:
        """í†µí•© í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„± - ê°ì£¼ ì‹œìŠ¤í…œ í¬í•¨"""
        
        # ëª¨ë“  ì»¨í…ì¸  ìˆ˜ì§‘ (ê²Œì‹œë¬¼ + ëŒ“ê¸€)
        all_content = []
        for cluster in clustering_result['clusters']:
            for item in cluster['items']:
                all_content.append(item)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_content = {item.get('id'): item for item in all_content if item.get('id')}.values()
        sorted_content = sorted(unique_content, key=lambda x: x.get('relevance_score', 0), reverse=True)
        
        # LLM ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ë³´ê³ ì„œ ìƒì„± (ê°ì£¼ í¬í•¨)
        try:
            report_result = await self.llm_service.generate_report(
                posts=list(sorted_content),
                query=query,
                length=length,
                cluster_info=clustering_result
            )
            
            # ë³´ê³ ì„œ íŒŒì‹±
            report_sections = self._parse_report_sections(report_result['full_report'])
            
            return {
                'full_report': report_result['full_report'],
                'sections': report_sections,
                'quality_score': await self._calculate_quality_score(report_sections),
                'footnote_mapping': report_result.get('footnote_mapping', [])
            }
            
        except Exception as e:
            logger.error(f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def _quality_assurance(self, report: Dict[str, Any], 
                               clustering_result: Dict[str, Any] = None,
                               query: str = None,
                               keywords: List[str] = None) -> Dict[str, Any]:
        """ìµœì¢… í’ˆì§ˆ ë³´ì¦ ë° ê°œì„ """
        
        # 1. ì¤‘ë³µ ë‚´ìš© ì œê±°
        cleaned_sections = await self._remove_section_duplicates(report['sections'])
        
        # 2. ì¼ê´€ì„± ê²€ì¦
        consistency_score = await self._check_consistency(cleaned_sections)
        
        # 3. í•„ìˆ˜ ìš”ì†Œ í™•ì¸
        completeness = self._check_completeness(cleaned_sections)
        
        # 4. ê°œì„ ì´ í•„ìš”í•œ ê²½ìš° ì¬ìƒì„±
        if consistency_score < 0.7 or not completeness['is_complete']:
            logger.info("ğŸ“ í’ˆì§ˆ ê°œì„ ì„ ìœ„í•œ ë³´ê³ ì„œ ì¬ìƒì„±")
            improved_report = await self._improve_report(
                report, 
                completeness['missing'],
                clustering_result=clustering_result,
                query=query,
                keywords=keywords
            )
            # ê°œì„ ëœ ë³´ê³ ì„œì—ì„œ summaryì™€ full_report í™•ì¸
            return {
                'summary': improved_report.get('summary', self._extract_summary(improved_report.get('full_report', ''))),
                'full_report': improved_report.get('full_report', ''),
                'quality_metrics': {
                    'consistency_score': consistency_score,
                    'completeness': completeness,
                    'quality_score': improved_report.get('quality_score', 0)
                },
                'footnote_mapping': report.get('footnote_mapping', [])
            }
        
        # 5. ìµœì¢… í¬ë§·íŒ…
        final_report = self._format_final_report(cleaned_sections)
        
        return {
            'summary': self._extract_summary(final_report),
            'full_report': final_report,
            'quality_metrics': {
                'consistency_score': consistency_score,
                'completeness': completeness,
                'quality_score': report.get('quality_score', 0)
            },
            'footnote_mapping': report.get('footnote_mapping', [])
        }
    
    async def _create_structured_prompt(
        self,
        clustering_result: Dict[str, Any],
        query: str,
        length: ReportLength,
        keywords: List[str]
    ) -> str:
        """êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        clusters = clustering_result['clusters']
        
        # í´ëŸ¬ìŠ¤í„° ì •ë³´ ì •ë¦¬
        cluster_summaries = []
        for idx, cluster in enumerate(clusters, 1):
            topic = cluster['topic']
            items = cluster['items']
            
            # ê° í´ëŸ¬ìŠ¤í„°ì˜ í•µì‹¬ ì½˜í…ì¸  ì¶”ì¶œ
            top_items = sorted(items, key=lambda x: x.get('relevance_score', 0), reverse=True)[:3]
            
            cluster_summary = f"""
ì£¼ì œ {idx}: {topic['name']}
ì„¤ëª…: {topic['description']}
ê´€ë ¨ ì½˜í…ì¸  ìˆ˜: {len(items)}ê°œ
í•µì‹¬ ë‚´ìš©:
"""
            for item in top_items:
                if item['type'] == 'post':
                    cluster_summary += f"- {item['title'][:80]}... (ì ìˆ˜: {item.get('score', 0)})\n"
                else:
                    cluster_summary += f"- ëŒ“ê¸€: {item.get('content', '')[:80]}... (ì¶”ì²œ: {item.get('score', 0)})\n"
            
            cluster_summaries.append(cluster_summary)
        
        # ê¸¸ì´ë³„ ê°€ì´ë“œ
        length_guides = {
            ReportLength.simple: "ê° ì„¹ì…˜ 1-2 ë‹¨ë½, ì „ì²´ 500-700ì",
            ReportLength.moderate: "ê° ì„¹ì…˜ 2-3 ë‹¨ë½, ì „ì²´ 1000-1500ì",
            ReportLength.detailed: "ê° ì„¹ì…˜ 3-5 ë‹¨ë½, ì „ì²´ 2000-3000ì"
        }
        
        prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ì»¤ë®¤ë‹ˆí‹° ë¶„ì„ê°€ì…ë‹ˆë‹¤. '{query}' í‚¤ì›Œë“œë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê³ í’ˆì§ˆ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords[:5])}
ìˆ˜ì§‘ëœ ê³ í’ˆì§ˆ ì½˜í…ì¸ : {sum(len(c['items']) for c in clusters)}ê°œ

ì£¼ì œë³„ ë¶„ë¥˜ ê²°ê³¼:
{''.join(cluster_summaries)}

ë‹¤ìŒ êµ¬ì¡°ë¡œ {length_guides[length]} ë¶„ëŸ‰ì˜ í•œêµ­ì–´ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

## 1. í•µì‹¬ ìš”ì•½
- ì „ì²´ ì—¬ë¡ ê³¼ íŠ¸ë Œë“œë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
- ê°€ì¥ ì¤‘ìš”í•œ ë°œê²¬ì‚¬í•­ 1-2ê°€ì§€ ê°•ì¡°

## 2. ì£¼ìš” ì£¼ì œ ë¶„ì„
- ìœ„ì—ì„œ ë¶„ë¥˜ëœ ê° ì£¼ì œë³„ë¡œ ì„¹ì…˜ êµ¬ì„±
- ê° ì£¼ì œì˜ í•µì‹¬ ë…¼ì ê³¼ ì—¬ë¡  ì •ë¦¬
- ê° ì£¼ì œë§ˆë‹¤ ìµœì†Œ 2-3ê°œì˜ ì›ë¬¸ ì¸ìš© í•„ìˆ˜
- ì¸ìš© í˜•ì‹: "ì›ë¬¸ ë‚´ìš©" (ì‘ì„±ì: username, ì¶”ì²œ: N)

## 3. ê°ì„± ë¶„ì„
- ì „ë°˜ì ì¸ ê°ì„± ë¶„í¬ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
- ì£¼ì œë³„ ê°ì„± ì°¨ì´ ë¶„ì„
- íŠ¹íˆ ë¶€ì •ì  ì˜ê²¬ì˜ ì£¼ìš” ì›ì¸

## 4. ì£¼ëª©í•  ë§Œí•œ ì¸ì‚¬ì´íŠ¸
- ì˜ˆìƒì¹˜ ëª»í•œ ë°œê²¬ì‚¬í•­
- ì†Œìˆ˜ì˜ê²¬ì´ì§€ë§Œ ì¤‘ìš”í•œ ê´€ì 
- í–¥í›„ ì£¼ëª©í•´ì•¼ í•  ì‹ í˜¸

## 5. ê²°ë¡  ë° ì‹œì‚¬ì 
- ì „ì²´ ë¶„ì„ ê²°ê³¼ ì¢…í•©
- ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ì œì‹œ

ì¤‘ìš” ì§€ì¹¨:
1. ê° ì„¹ì…˜ ê°„ ì¤‘ë³µì„ í”¼í•˜ê³  ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ì‚¬ë¡€ë¡œ ì£¼ì¥ ë’·ë°›ì¹¨
3. ê°ê´€ì ì´ê³  ê· í˜•ì¡íŒ ì‹œê° ìœ ì§€
4. ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰¬ìš´ ë¬¸ì²´ ì‚¬ìš©"""
        
        return prompt
    
    def _remove_duplicates(self, content: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ì½˜í…ì¸  ì œê±°"""
        seen_ids = set()
        unique_content = []
        
        for item in content:
            item_id = item.get('id')
            if item_id and item_id not in seen_ids:
                seen_ids.add(item_id)
                unique_content.append(item)
        
        return unique_content
    
    def _parse_report_sections(self, report_text: str) -> Dict[str, str]:
        """ë³´ê³ ì„œë¥¼ ì„¹ì…˜ë³„ë¡œ íŒŒì‹±"""
        sections = {
            'summary': '',
            'topic_analysis': '',
            'sentiment_analysis': '',
            'insights': '',
            'conclusion': ''
        }
        
        # ì„¹ì…˜ ë§¤í•‘
        section_markers = {
            '## 1. í•µì‹¬ ìš”ì•½': 'summary',
            '## 2. ì£¼ìš” ì£¼ì œ ë¶„ì„': 'topic_analysis',
            '## 3. ê°ì„± ë¶„ì„': 'sentiment_analysis',
            '## 4. ì£¼ëª©í•  ë§Œí•œ ì¸ì‚¬ì´íŠ¸': 'insights',
            '## 5. ê²°ë¡  ë° ì‹œì‚¬ì ': 'conclusion'
        }
        
        current_section = None
        lines = report_text.split('\n')
        
        for line in lines:
            # ì„¹ì…˜ ì‹œì‘ í™•ì¸
            for marker, section_name in section_markers.items():
                if line.strip().startswith(marker):
                    current_section = section_name
                    break
            else:
                # í˜„ì¬ ì„¹ì…˜ì— ë‚´ìš© ì¶”ê°€
                if current_section:
                    sections[current_section] += line + '\n'
        
        return sections
    
    async def _calculate_quality_score(self, sections: Dict[str, str]) -> float:
        """ë³´ê³ ì„œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ê° ì„¹ì…˜ ì¡´ì¬ ì—¬ë¶€ (50%)
        for section, content in sections.items():
            if content.strip():
                score += 0.1
        
        # ì„¹ì…˜ë³„ ìµœì†Œ ê¸¸ì´ ì¶©ì¡± (30%)
        min_lengths = {
            'summary': 100,
            'topic_analysis': 300,
            'sentiment_analysis': 200,
            'insights': 150,
            'conclusion': 150
        }
        
        for section, min_length in min_lengths.items():
            if len(sections.get(section, '')) >= min_length:
                score += 0.06
        
        # êµ¬ì²´ì  ìˆ˜ì¹˜/ì¸ìš© í¬í•¨ ì—¬ë¶€ (20%)
        all_content = ' '.join(sections.values())
        if any(char.isdigit() for char in all_content):
            score += 0.1
        if '"' in all_content or 'ã€Œ' in all_content:
            score += 0.1
        
        return min(score, 1.0)
    
    async def _remove_section_duplicates(self, sections: Dict[str, str]) -> Dict[str, str]:
        """ì„¹ì…˜ ê°„ ì¤‘ë³µ ë‚´ìš© ì œê±°"""
        cleaned_sections = {}
        used_sentences = set()
        
        for section_name, content in sections.items():
            sentences = content.split('.')
            cleaned_sentences = []
            
            for sentence in sentences:
                sentence = sentence.strip()
                if sentence and sentence not in used_sentences:
                    used_sentences.add(sentence)
                    cleaned_sentences.append(sentence)
            
            cleaned_sections[section_name] = '. '.join(cleaned_sentences) + '.'
        
        return cleaned_sections
    
    async def _check_consistency(self, sections: Dict[str, str]) -> float:
        """ì„¹ì…˜ ê°„ ì¼ê´€ì„± ê²€ì¦"""
        # ê°„ë‹¨í•œ ì¼ê´€ì„± ì²´í¬ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
        consistency_score = 1.0
        
        # ìš”ì•½ê³¼ ê²°ë¡ ì˜ ì¼ì¹˜ë„ í™•ì¸
        summary = sections.get('summary', '').lower()
        conclusion = sections.get('conclusion', '').lower()
        
        # ê³µí†µ í‚¤ì›Œë“œ í™•ì¸
        summary_words = set(summary.split())
        conclusion_words = set(conclusion.split())
        
        if summary_words and conclusion_words:
            overlap = len(summary_words & conclusion_words)
            consistency_score = min(overlap / min(len(summary_words), len(conclusion_words)) * 2, 1.0)
        
        return consistency_score
    
    def _check_completeness(self, sections: Dict[str, str]) -> Dict[str, Any]:
        """ë³´ê³ ì„œ ì™„ì„±ë„ í™•ì¸"""
        required_sections = ['summary', 'topic_analysis', 'sentiment_analysis', 'insights', 'conclusion']
        missing_sections = []
        
        for section in required_sections:
            if not sections.get(section, '').strip():
                missing_sections.append(section)
        
        return {
            'is_complete': len(missing_sections) == 0,
            'missing': missing_sections,
            'completeness_ratio': (len(required_sections) - len(missing_sections)) / len(required_sections)
        }
    
    async def _improve_report(self, report: Dict[str, Any], missing_sections: List[str], 
                           clustering_result: Dict[str, Any] = None, query: str = None, 
                           keywords: List[str] = None) -> Dict[str, Any]:
        """ë³´ê³ ì„œ ê°œì„  - ì›ë³¸ ë°ì´í„°ë¥¼ í™œìš©í•œ ì¢…í•©ì  ê°œì„ """
        
        # ê¸°ì¡´ ì„¹ì…˜ë“¤ì˜ ë‚´ìš© ë³´ì¡´ (ì„ì‹œ ë¹„í™œì„±í™” - ë¹ˆ ë‚´ìš© ë¬¸ì œ í•´ê²°)
        existing_sections = []
        # TODO: ì¶”í›„ ê¸°ì¡´ ë‚´ìš©ì´ ì‹¤ì œë¡œ ìˆì„ ë•Œë§Œ ë³´ì¡´í•˜ë„ë¡ ê°œì„ 
        # for section, content in report.get('sections', {}).items():
        #     if content and content.strip() and section not in missing_sections:
        #         existing_sections.append(f"### {section}\n{content}")
        
        # í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¬êµ¬ì„±
        cluster_info = ""
        if clustering_result and 'clusters' in clustering_result:
            for idx, cluster in enumerate(clustering_result['clusters'], 1):
                cluster_info += f"\ní´ëŸ¬ìŠ¤í„° {idx} - {cluster['topic']['name']}:\n"
                cluster_info += f"  ì„¤ëª…: {cluster['topic']['description']}\n"
                cluster_info += f"  ì½˜í…ì¸  ìˆ˜: {len(cluster['items'])}ê°œ\n"
                
                # ìƒìœ„ ì½˜í…ì¸  ì˜ˆì‹œ
                top_items = sorted(cluster['items'], 
                                 key=lambda x: x.get('relevance_score', 0), 
                                 reverse=True)[:2]
                for item in top_items:
                    if item['type'] == 'post':
                        cluster_info += f"  - {item['title'][:60]}...\n"
                    else:
                        cluster_info += f"  - ëŒ“ê¸€: {item.get('content', '')[:60]}...\n"
        
        improvement_prompt = f"""ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ ì „ë¬¸ ì»¤ë®¤ë‹ˆí‹° ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
ì—¬ëŸ¬ ë¶„ì„ê°€ë“¤ì´ ì‘ì„±í•œ ê°œë³„ ë¶„ì„ë“¤ì„ ì¢…í•©í•˜ì—¬, ì „ì²´ì ì¸ ê´€ì ì—ì„œë§Œ ë³¼ ìˆ˜ ìˆëŠ” í†µì°°ê³¼ í•¨ê»˜ 
í¬ê´„ì ì´ê³  ì‹¬ì¸µì ì¸ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë¶„ì„ ì£¼ì œ: {query or ''}
ê´€ë ¨ í‚¤ì›Œë“œ: {', '.join(keywords[:5]) if keywords else ''}

=== ì›ë³¸ ë°ì´í„° ì •ë³´ ===
{cluster_info}

=== ê¸°ì¡´ì— ì‘ì„±ëœ ê°œë³„ ë¶„ì„ ë‚´ìš©ë“¤ (ëª¨ë‘ í¬í•¨í•˜ì—¬ í™•ì¥) ===
{chr(10).join(existing_sections)}

=== ë³´ì™„ì´ í•„ìš”í•œ ì„¹ì…˜: {', '.join(missing_sections)} ===

ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ì¢…í•© ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

1. **ê¸°ì¡´ ë¶„ì„ í™•ì¥ ë° ì‹¬í™”**
   - ê°œë³„ ë¶„ì„ê°€ë“¤ì´ ì‘ì„±í•œ ë‚´ìš©ì„ ëª¨ë‘ í¬í•¨í•˜ë˜, ë” ê¹Šì´ ìˆê²Œ í™•ì¥
   - ê° ì£¼ì œë³„ë¡œ 3-5ê°œì˜ êµ¬ì²´ì ì¸ ì›ë¬¸ì„ ë°˜ë“œì‹œ ì¸ìš©
   - ì¸ìš© í˜•ì‹: "ì›ë¬¸ ë‚´ìš©" (ì¶œì²˜: ì‘ì„±ìëª…, ì¶”ì²œìˆ˜)
   
2. **ì¢…í•©ì  ì‹œê°ì—ì„œì˜ ìƒˆë¡œìš´ í†µì°°**
   - ê°œë³„ ë¶„ì„ì—ì„œëŠ” ë³´ì´ì§€ ì•Šì•˜ë˜ ì „ì²´ì ì¸ íŒ¨í„´ê³¼ íŠ¸ë Œë“œ íŒŒì•…
   - ì„œë¡œ ë‹¤ë¥¸ ì£¼ì œ/í´ëŸ¬ìŠ¤í„° ê°„ì˜ ì—°ê´€ì„±ê³¼ ìƒí˜¸ì‘ìš© ë¶„ì„
   - í‘œë©´ì ìœ¼ë¡œ ë“œëŸ¬ë‚˜ì§€ ì•Šì€ ìˆ¨ì€ ì˜ë¯¸ì™€ í•¨ì˜ ë„ì¶œ
   
3. **êµ¬ì²´ì ì¸ ì¦ê±°ì™€ ì‚¬ë¡€**
   - ëª¨ë“  ì£¼ì¥ì€ ì‹¤ì œ ê²Œì‹œë¬¼/ëŒ“ê¸€ì˜ ì›ë¬¸ ì¸ìš©ìœ¼ë¡œ ë’·ë°›ì¹¨
   - í†µê³„ì  ìˆ˜ì¹˜ì™€ ë¹„ìœ¨ì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œ
   - "ë§ì€ ì‚¬ëŒë“¤ì´"ê°€ ì•„ë‹Œ "ì „ì²´ ì‘ë‹µìì˜ 65%ê°€" ê°™ì€ ì •í™•í•œ í‘œí˜„
   
4. **ìƒì„¸í•˜ê³  í’ë¶€í•œ ë‚´ìš©**
   - ê° ì„¹ì…˜ì„ ìµœì†Œ 3-4ê°œ ë‹¨ë½ìœ¼ë¡œ êµ¬ì„±
   - í•µì‹¬ ìš”ì•½ì€ 500ì ì´ìƒ
   - ì£¼ìš” ì£¼ì œ ë¶„ì„ì€ ê° ì£¼ì œë‹¹ 300-500ì
   - ì „ì²´ ë³´ê³ ì„œëŠ” 3000-5000ì ìˆ˜ì¤€
   
5. **ë‹¤ì¸µì  ë¶„ì„**
   - í‘œë©´ì  ì˜ê²¬ â†’ ê·¼ë³¸ ì›ì¸ â†’ ì ì¬ì  ì˜í–¥ ìˆœìœ¼ë¡œ ë¶„ì„
   - ë‹¨ê¸°ì  ë°˜ì‘ê³¼ ì¥ê¸°ì  í•¨ì˜ë¥¼ êµ¬ë¶„í•˜ì—¬ ì œì‹œ
   - ì£¼ë¥˜ ì˜ê²¬ê³¼ ì†Œìˆ˜ ì˜ê²¬ì˜ ê°€ì¹˜ë¥¼ ëª¨ë‘ í‰ê°€

ìµœì¢… ë³´ê³ ì„œ êµ¬ì¡°:

## 1. ì¢…í•© ìš”ì•½ ë° í•µì‹¬ ë°œê²¬ì‚¬í•­
- ì „ì²´ ë°ì´í„°ë¥¼ ê´€í†µí•˜ëŠ” í•µì‹¬ ë©”ì‹œì§€ 3-5ê°œ
- ê°€ì¥ ì¤‘ìš”í•œ ë°œê²¬ì‚¬í•­ê³¼ ê·¸ ì˜ë¯¸
- ì˜ˆìƒì¹˜ ëª»í•œ í†µì°°ì´ë‚˜ ì—­ì„¤ì  ë°œê²¬

## 2. ì£¼ì œë³„ ì‹¬ì¸µ ë¶„ì„
- ê° ì£¼ì œë§ˆë‹¤ ë°°ê²½, í˜„í™©, êµ¬ì²´ì  ì‚¬ë¡€(ì›ë¬¸ ì¸ìš© í•„ìˆ˜), ì˜ë¯¸ ë¶„ì„
- ì£¼ì œ ê°„ ì—°ê²°ê³ ë¦¬ì™€ ìƒí˜¸ ì˜í–¥ ê´€ê³„
- ê° ì£¼ì œë³„ë¡œ ëŒ€í‘œì ì¸ ì›ë¬¸ 3-5ê°œ ì¸ìš©

## 3. ì •ì„œ ë° ì—¬ë¡  ë™í–¥ ë¶„ì„
- ì •ëŸ‰ì  ê°ì„± ë¶„í¬ì™€ ì •ì„±ì  ê°ì • ë¶„ì„
- ê°ì • ë³€í™”ì˜ ì›ì¸ê³¼ ë§¥ë½
- íŠ¹ì • ì´ìŠˆì— ëŒ€í•œ ê°ì •ì  ë°˜ì‘ì˜ ì›ë¬¸ ì˜ˆì‹œ

## 4. ìˆ¨ì€ íŒ¨í„´ê³¼ í†µì°°
- ê°œë³„ ë¶„ì„ì—ì„œ ë†“ì¹œ ì „ì²´ì  íŒ¨í„´
- ì•½í•œ ì‹ í˜¸(weak signal)ì´ì§€ë§Œ ì¤‘ìš”í•œ ì§•í›„
- ì»¤ë®¤ë‹ˆí‹°ì˜ ì§‘ë‹¨ ë¬´ì˜ì‹ì´ë‚˜ ì•”ë¬µì  í•©ì˜

## 5. ì „ëµì  í•¨ì˜ì™€ ì œì–¸
- ë¶„ì„ ê²°ê³¼ê°€ ì‹œì‚¬í•˜ëŠ” ë°”
- í–¥í›„ ì˜ˆìƒë˜ëŠ” ì „ê°œ ë°©í–¥
- êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì–¸

ëª¨ë“  ì„¹ì…˜ì—ì„œ êµ¬ì²´ì ì¸ ì›ë¬¸ì„ ì¸ìš©í•˜ê³ , 
ë‹¨ìˆœ ë‚˜ì—´ì´ ì•„ë‹Œ ì„œì‚¬ì  íë¦„ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
        
        try:
            improved_response = await self.llm_service._call_llm(improvement_prompt, temperature=0.5)
            improved_sections = self._parse_report_sections(improved_response)
            
            # ê¸°ì¡´ ì„¹ì…˜ê³¼ ë³‘í•©
            for section, content in improved_sections.items():
                if section in missing_sections and content.strip():
                    report['sections'][section] = content
            
            report['full_report'] = self._format_final_report(report['sections'])
            
        except Exception as e:
            logger.error(f"ë³´ê³ ì„œ ê°œì„  ì‹¤íŒ¨: {str(e)}")
        
        # summaryì™€ quality_metricsê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ì¶”ê°€
        if 'summary' not in report:
            report['summary'] = self._extract_summary(report['full_report'])
        if 'quality_score' not in report:
            report['quality_score'] = await self._calculate_quality_score(report['sections'])
        
        return report
    
    def _format_final_report(self, sections: Dict[str, str]) -> str:
        """ìµœì¢… ë³´ê³ ì„œ í¬ë§·íŒ…"""
        formatted_parts = []
        
        section_titles = {
            'summary': '## 1. í•µì‹¬ ìš”ì•½',
            'topic_analysis': '## 2. ì£¼ìš” ì£¼ì œ ë¶„ì„',
            'sentiment_analysis': '## 3. ê°ì„± ë¶„ì„',
            'insights': '## 4. ì£¼ëª©í•  ë§Œí•œ ì¸ì‚¬ì´íŠ¸',
            'conclusion': '## 5. ê²°ë¡  ë° ì‹œì‚¬ì '
        }
        
        for section_key, title in section_titles.items():
            content = sections.get(section_key, '').strip()
            if content:
                formatted_parts.append(f"{title}\n\n{content}")
        
        return '\n\n'.join(formatted_parts)
    
    def _extract_summary(self, report: str) -> str:
        """ë³´ê³ ì„œì—ì„œ ìš”ì•½ ì¶”ì¶œ"""
        lines = report.split('\n')
        summary_lines = []
        in_summary = False
        
        for line in lines:
            if '## 1. í•µì‹¬ ìš”ì•½' in line:
                in_summary = True
                continue
            elif line.startswith('## 2.'):
                break
            elif in_summary and line.strip():
                summary_lines.append(line.strip())
        
        return ' '.join(summary_lines[:3])  # ìµœëŒ€ 3ì¤„