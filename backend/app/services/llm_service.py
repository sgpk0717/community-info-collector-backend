from typing import List, Dict, Any, Optional
from openai import OpenAI
from app.core.exceptions import OpenAIAPIException
from app.schemas.search import ReportLength
import logging
import json

logger = logging.getLogger(__name__)

class LLMService:
    def __init__(self):
        self.client = OpenAI()
    
    async def translate_to_english(self, query: str) -> str:
        """í•œê¸€ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"""
        try:
            prompt = f"""Translate the following Korean keyword to English. 
            If it's already in English, return as is.
            Only return the translated text, nothing else.
            
            Keyword: {query}
            """
            
            response = self.client.chat.completions.create(
                model="gpt-4.1",
                messages=[
                    {"role": "developer", "content": "You are a professional translator."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=100
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"Translation error: {str(e)}")
            return query  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    async def expand_keywords(self, query: str) -> List[str]:
        """ì£¼ì–´ì§„ í‚¤ì›Œë“œë¥¼ í™•ì¥í•˜ì—¬ ê´€ë ¨ ê²€ìƒ‰ì–´ ìƒì„± (ì˜ì–´)"""
        try:
            # ë¨¼ì € ì˜ì–´ë¡œ ë²ˆì—­
            english_query = await self.translate_to_english(query)
            logger.info(f"Translated query: {query} -> {english_query}")
            
            prompt = f"""Generate 5 related search keywords for: "{english_query}"
            
            Requirements:
            1. All keywords must be in English
            2. Cover different aspects (technical, business, social, future trends)
            3. Be specific and relevant to the original keyword
            4. Return as JSON array only
            
            Example format: ["keyword1", "keyword2", "keyword3", "keyword4", "keyword5"]
            """
            
            response = self.client.chat.completions.create(
                model="gpt-4.1",
                messages=[
                    {"role": "developer", "content": "You are a keyword expansion expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=200
            )
            
            content = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                keywords = json.loads(content)
                if isinstance(keywords, list):
                    return keywords[:5]  # ìµœëŒ€ 5ê°œ
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse keywords JSON: {content}")
            
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ í‚¤ì›Œë“œë§Œ ë°˜í™˜
            return []
            
        except Exception as e:
            logger.error(f"OpenAI API error in expand_keywords: {str(e)}")
            return []  # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
    
    async def generate_report(self, posts: List[Dict[str, Any]], query: str, length: ReportLength) -> Dict[str, Any]:
        """ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        try:
            # ê²Œì‹œë¬¼ ì •ë³´ í¬ë§·íŒ…
            posts_text = self._format_posts_for_prompt(posts[:30])  # ìµœëŒ€ 30ê°œ ê²Œì‹œë¬¼
            
            # ë³´ê³ ì„œ ê¸¸ì´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
            length_guide = {
                ReportLength.simple: "ê°„ë‹¨íˆ 3-5 ë¬¸ì¥ìœ¼ë¡œ",
                ReportLength.moderate: "ì ë‹¹íˆ ìƒì„¸í•˜ê²Œ 2-3 ë‹¨ë½ìœ¼ë¡œ", 
                ReportLength.detailed: "ë§¤ìš° ìƒì„¸í•˜ê²Œ ê° ì„¹ì…˜ë³„ë¡œ"
            }
            
            prompt = f"""You are a professional community analyst. The following are social media posts collected with the keyword '{query}'.

{posts_text}

Based on this English data, create a comprehensive analysis report in KOREAN following these guidelines:

Length: {length_guide[length]}

Required sections (write all section headers and content in Korean):

1. **í•µì‹¬ ìš”ì•½**: Summarize the key findings
2. **ì£¼ìš” í† í”½**: Categorize and explain main topics discussed
3. **ì»¤ë®¤ë‹ˆí‹° ë°˜ì‘**: Analyze positive/negative sentiment ratios with evidence
4. **ì¸ìƒì ì¸ ì˜ê²¬**: Highlight 2-3 most notable opinions or insights
5. **ì¢…í•© ë¶„ì„**: Overall community perspective and trends

**IMPORTANT FOOTNOTE REQUIREMENTS:**
- When referencing specific posts or opinions, add footnotes using [1], [2], [3] format
- Use footnotes for direct quotes, specific claims, or notable opinions
- At the end, provide a "References" section in Korean that lists:
  - **ì°¸ê³  ìë£Œ**
  - [1] ê²Œì‹œë¬¼ 1 ì œëª© (r/subreddit)
  - [2] ê²Œì‹œë¬¼ 2 ì œëª© (r/subreddit)
  - etc.

Important: 
- The input data is in English, but write the ENTIRE report in Korean
- Use markdown format
- Maintain objective and balanced perspective
- Translate key terms appropriately into Korean
- Include footnotes [1], [2], [3] etc. when referencing specific posts
- End with a "References" section mapping footnotes to post information
"""
            
            response = self.client.chat.completions.create(
                model="gpt-4.1",
                messages=[
                    {"role": "developer", "content": "You are a professional community analyst who creates insightful reports in Korean."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=2000 if length == ReportLength.detailed else 1000
            )
            
            full_report = response.choices[0].message.content.strip()
            
            # ìš”ì•½ ìƒì„± (í•œê¸€)
            summary_prompt = f"ë‹¤ìŒ í•œêµ­ì–´ ë³´ê³ ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{full_report[:1000]}"
            
            summary_response = self.client.chat.completions.create(
                model="gpt-4.1",
                messages=[
                    {"role": "developer", "content": "You are a summarization expert."},
                    {"role": "user", "content": summary_prompt}
                ],
                temperature=0.5,
                max_tokens=200
            )
            
            summary = summary_response.choices[0].message.content.strip()
            
            # ê°ì£¼ ë§¤í•‘ ì¶”ì¶œ
            footnote_mapping = self._extract_footnote_mapping(full_report, posts)
            
            logger.info(f"ğŸ‰ AI ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
            logger.info(f"   - ì „ì²´ ë³´ê³ ì„œ: {len(full_report)} ë¬¸ì")
            logger.info(f"   - ìš”ì•½: {len(summary)} ë¬¸ì")
            logger.info(f"   - ê°ì£¼ ìˆ˜: {len(footnote_mapping)}ê°œ")
            
            return {
                "summary": summary,
                "full_report": full_report,
                "footnote_mapping": footnote_mapping
            }
            
        except Exception as e:
            logger.error(f"OpenAI API error in generate_report: {str(e)}")
            raise OpenAIAPIException(f"Failed to generate report: {str(e)}")
    
    def _format_posts_for_prompt(self, posts: List[Dict[str, Any]]) -> str:
        """ê²Œì‹œë¬¼ì„ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        formatted_posts = []
        
        for i, post in enumerate(posts, 1):
            # ê°œì„ ëœ í¬ë§·íŒ…ì— ë£¨ë¨¸ ì ìˆ˜ì™€ ìˆ˜ì§‘ ë²¡í„° ì •ë³´ í¬í•¨
            vector_info = post.get('collection_vector', 'unknown')
            rumor_score = post.get('rumor_score', 0)
            linguistic_flags = post.get('linguistic_flags', [])
            
            post_text = f"""[ê²Œì‹œë¬¼ {i}]
ì œëª©: {post['title']}
ì ìˆ˜: {post['score']} | ëŒ“ê¸€: {post['num_comments']} | ë£¨ë¨¸ì ìˆ˜: {rumor_score}/10
ì„œë¸Œë ˆë”§: r/{post['subreddit']} | ìˆ˜ì§‘ë²¡í„°: {vector_info}
ì–¸ì–´ì‹ í˜¸: {', '.join(linguistic_flags) if linguistic_flags else 'ì—†ìŒ'}
ë‚´ìš©: {post['selftext'][:200] if post['selftext'] else '(ë‚´ìš© ì—†ìŒ)'}
---"""
            formatted_posts.append(post_text)
        
        logger.debug(f"ğŸ“„ ê²Œì‹œë¬¼ í¬ë§·íŒ…: {len(formatted_posts)}ê°œ ê²Œì‹œë¬¼")
        return "\n".join(formatted_posts)
    
    def _extract_footnote_mapping(self, report: str, posts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë³´ê³ ì„œì—ì„œ ê°ì£¼ ë§¤í•‘ ì¶”ì¶œ"""
        import re
        
        footnote_mapping = []
        
        # ê°ì£¼ íŒ¨í„´ ì°¾ê¸° [1], [2], [3] ë“±
        footnote_pattern = r'\[(\d+)\]'
        footnotes = re.findall(footnote_pattern, report)
        
        if not footnotes:
            logger.info("ğŸ“„ ê°ì£¼ê°€ ë°œê²¬ë˜ì§€ ì•ŠìŒ")
            return footnote_mapping
        
        logger.info(f"ğŸ”— ê°ì£¼ ë°œê²¬: {footnotes}")
        
        # ê°ì£¼ ë²ˆí˜¸ì— ë§ëŠ” ê²Œì‹œë¬¼ ë§¤í•‘
        for footnote_num in set(footnotes):
            footnote_int = int(footnote_num)
            
            # ê°ì£¼ ë²ˆí˜¸ì— ë§ëŠ” ê²Œì‹œë¬¼ ì¸ë±ìŠ¤ (ë°°ì—´ì´ë¯€ë¡œ -1)
            post_index = footnote_int - 1
            
            if 0 <= post_index < len(posts):
                post = posts[post_index]
                footnote_mapping.append({
                    "footnote_number": footnote_int,
                    "post_id": post['id'],
                    "url": post['url'],
                    "title": post['title'],
                    "score": post['score'],
                    "comments": post['num_comments'],
                    "created_utc": post['created_utc'],
                    "subreddit": post['subreddit'],
                    "author": post['author'],
                    "position_in_report": footnote_int
                })
        
        # ê°ì£¼ ë²ˆí˜¸ìˆœìœ¼ë¡œ ì •ë ¬
        footnote_mapping.sort(key=lambda x: x['footnote_number'])
        
        logger.info(f"ğŸ”— ê°ì£¼ ë§¤í•‘ ì™„ë£Œ: {len(footnote_mapping)}ê°œ")
        return footnote_mapping