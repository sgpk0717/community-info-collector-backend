import praw
from typing import List, Dict, Any, Optional
from app.core.dependencies import get_reddit_client
from app.core.exceptions import RedditAPIException
import logging
from datetime import datetime
import asyncio
import re

logger = logging.getLogger(__name__)

# ë£¨ë¨¸ íƒì§€ìš© í‚¤ì›Œë“œ ì„¸íŠ¸
SPECULATIVE_WORDS = {
    'might', 'could', 'perhaps', 'likely', 'possibly', 'maybe', 'supposedly',
    'allegedly', 'rumor', 'rumour', 'unconfirmed', 'sources say', 'i heard',
    'word is', 'gossip', 'speculation', 'apparently', 'seems like',
    # í•œêµ­ì–´ ì¶”ì¸¡ í‘œí˜„
    'ì•„ë§ˆë„', 'ì•„ë§ˆ', '~ì¼ ìˆ˜ë„', '~ì¸ ê²ƒ ê°™', 'ì†Œë¬¸ì—', 'ì¹´ë”ë¼', 'ë“¤ì—ˆëŠ”ë°',
    '~ë¼ëŠ” ì–˜ê¸°', '~ê°™ë‹¤ëŠ”', 'ì¶”ì¸¡', 'ì˜ˆìƒ', '~ì¼ì§€ë„'
}

NEGATIVE_EMOTION_WORDS = {
    'disaster', 'crisis', 'collapse', 'crash', 'terrible', 'awful', 'horrible',
    'devastating', 'nightmare', 'panic', 'fear', 'doom', 'failed', 'failure',
    'warning', 'danger', 'risk', 'threat', 'concerned', 'worried',
    # í•œêµ­ì–´ ë¶€ì •ì  ê°ì • í‘œí˜„
    'ì¬ì•™', 'ìœ„ê¸°', 'ë¶•ê´´', 'ì¶”ë½', 'ë”ì°í•œ', 'ë¬´ì„œìš´', 'ê³µí¬', 'ì‹¤íŒ¨', 'ìœ„í—˜',
    'ê²½ê³ ', 'ê±±ì •', 'ìš°ë ¤', 'ë¬¸ì œ', 'ì‹¬ê°í•œ', 'ìµœì•…'
}

class RedditService:
    def __init__(self):
        self.client = get_reddit_client()
    
    def _calculate_rumor_score(self, submission) -> float:
        """ë£¨ë¨¸ ì ìˆ˜ ê³„ì‚° (0-10 ë²”ìœ„)"""
        score = 0.0
        score_breakdown = []
        
        # 1. ë…¼ë€ì„± ì§€í‘œ (upvote_ratio)
        if hasattr(submission, 'upvote_ratio') and submission.upvote_ratio < 0.7:
            controversy_score = (0.7 - submission.upvote_ratio) * 5  # ìµœëŒ€ 3.5ì 
            score += controversy_score
            score_breakdown.append(f"ë…¼ë€ì„±({submission.upvote_ratio:.2f}): +{controversy_score:.1f}")
        
        # 2. ì–¸ì–´í•™ì  ì‹ í˜¸ íƒì§€
        text = (submission.title + ' ' + (submission.selftext or '')).lower()
        
        # ì¶”ì¸¡ì„± ë‹¨ì–´ ê°œìˆ˜
        speculation_count = sum(1 for word in SPECULATIVE_WORDS if word in text)
        if speculation_count > 0:
            speculation_score = min(speculation_count * 1.5, 3.0)  # ìµœëŒ€ 3ì 
            score += speculation_score
            score_breakdown.append(f"ì¶”ì¸¡ì„±({speculation_count}ê°œ): +{speculation_score:.1f}")
        
        # ë¶€ì •ì  ê°ì • ë‹¨ì–´ ê°œìˆ˜
        negative_count = sum(1 for word in NEGATIVE_EMOTION_WORDS if word in text)
        if negative_count > 0:
            negative_score = min(negative_count * 1.0, 2.0)  # ìµœëŒ€ 2ì 
            score += negative_score
            score_breakdown.append(f"ë¶€ì •ê°ì •({negative_count}ê°œ): +{negative_score:.1f}")
        
        # 3. ê²Œì‹œë¬¼ íŠ¹ì„±
        if hasattr(submission, 'is_self') and submission.is_self:
            score += 0.5  # ìì²´ ê²Œì‹œë¬¼ (ë§í¬ê°€ ì•„ë‹Œ í…ìŠ¤íŠ¸)
            score_breakdown.append(f"ìì²´ê²Œì‹œë¬¼: +0.5")
        
        # 4. ìˆ˜ì§‘ ë²¡í„° ê°€ì¤‘ì¹˜
        if hasattr(submission, '_collection_vector'):
            if submission._collection_vector == 'underground':
                score += 1.0  # ë…¼ë€ì„± ë²¡í„°ì—ì„œ ìˆ˜ì§‘ëœ ê²½ìš°
                score_breakdown.append(f"ë…¼ë€ì„±ë²¡í„°: +1.0")
            elif submission._collection_vector == 'vanguard':
                score += 0.5  # ìµœì‹  ë²¡í„°ì—ì„œ ìˆ˜ì§‘ëœ ê²½ìš°
                score_breakdown.append(f"ìµœì‹ ë²¡í„°: +0.5")
        
        final_score = min(score, 10.0)  # ìµœëŒ€ 10ì ìœ¼ë¡œ ì œí•œ
        
        # ë£¨ë¨¸ ì ìˆ˜ ê³„ì‚° ë¡œê·¸ (ì ìˆ˜ê°€ 5 ì´ìƒì¼ ë•Œë§Œ)
        if final_score >= 5.0:
            logger.info(f"ğŸš¨ ë†’ì€ ë£¨ë¨¸ ì ìˆ˜ ê°ì§€ [{final_score:.1f}/10] - {submission.title[:50]}...")
            logger.info(f"   ì„¸ë¶€ì‚¬í•­: {', '.join(score_breakdown)}")
        
        return final_score
    
    def _extract_linguistic_flags(self, text: str) -> List[str]:
        """ì–¸ì–´í•™ì  ì‹ í˜¸ í”Œë˜ê·¸ ì¶”ì¶œ"""
        flags = []
        text_lower = text.lower()
        
        # ì¶”ì¸¡ì„± ì–¸ì–´ íƒì§€
        speculation_words = [word for word in SPECULATIVE_WORDS if word in text_lower]
        if speculation_words:
            flags.append('speculation')
            logger.debug(f"ğŸ” ì¶”ì¸¡ì„± ì–¸ì–´ ê°ì§€: {speculation_words[:3]}...")
        
        # ë¶€ì •ì  ê°ì • íƒì§€
        negative_words = [word for word in NEGATIVE_EMOTION_WORDS if word in text_lower]
        if negative_words:
            flags.append('negative_emotion')
            logger.debug(f"ğŸ˜  ë¶€ì •ì  ê°ì • ê°ì§€: {negative_words[:3]}...")
        
        # ë¹„ê³µì‹ì„± íƒì§€ (ëŠë‚Œí‘œ, ëŒ€ë¬¸ì ê³¼ë‹¤ ì‚¬ìš©)
        exclamation_count = text.count('!')
        caps_words = re.findall(r'[A-Z]{3,}', text)
        if exclamation_count > 2 or len(caps_words) > 0:
            flags.append('informal')
            logger.debug(f"ğŸ“¢ ë¹„ê³µì‹ì„± ê°ì§€: ëŠë‚Œí‘œ {exclamation_count}ê°œ, ëŒ€ë¬¸ì {len(caps_words)}ê°œ")
        
        return flags
        
    async def search_posts(self, query: str, limit: int = 50) -> List[Dict[str, Any]]:
        """Redditì—ì„œ í‚¤ì›Œë“œë¡œ ê²Œì‹œë¬¼ ê²€ìƒ‰ - ë‹¤ì¤‘ ë²¡í„° ìˆ˜ì§‘ ì „ëµ"""
        try:
            logger.info(f"ğŸ” Reddit ê²€ìƒ‰ ì‹œì‘: '{query}' (ìµœëŒ€ {limit}ê°œ ê²Œì‹œë¬¼)")
            posts = []
            
            # Reddit APIëŠ” ë™ê¸°ì‹ì´ë¯€ë¡œ asyncioì˜ run_in_executor ì‚¬ìš©
            loop = asyncio.get_event_loop()
            
            def _search():
                # ë‹¤ì¤‘ ë²¡í„° ìˆ˜ì§‘ ì „ëµ
                vectors = [
                    {'name': 'zeitgeist', 'sort': 'hot', 'time_filter': 'week', 'limit': limit//3},
                    {'name': 'underground', 'sort': 'controversial', 'time_filter': 'month', 'limit': limit//3},
                    {'name': 'vanguard', 'sort': 'new', 'time_filter': 'all', 'limit': limit//3}
                ]
                
                logger.info(f"ğŸ“Š ë‹¤ì¤‘ ë²¡í„° ìˆ˜ì§‘ ì „ëµ ì‹œì‘ - ì´ {len(vectors)}ê°œ ë²¡í„°")
                all_submissions = []
                
                for vector in vectors:
                    try:
                        logger.info(f"ğŸ¯ ë²¡í„° '{vector['name']}' ê²€ìƒ‰ ì‹œì‘ - ì •ë ¬: {vector['sort']}, ê¸°ê°„: {vector['time_filter']}, ì œí•œ: {vector['limit']}")
                        
                        subreddit = self.client.subreddit('all')
                        
                        # ë²¡í„°ë³„ ê²€ìƒ‰ ìˆ˜í–‰
                        if vector['sort'] == 'hot':
                            search_results = subreddit.search(
                                query, 
                                limit=vector['limit'], 
                                sort='hot',
                                time_filter=vector['time_filter']
                            )
                        elif vector['sort'] == 'controversial':
                            search_results = subreddit.search(
                                query, 
                                limit=vector['limit'], 
                                sort='controversial',
                                time_filter=vector['time_filter']
                            )
                        else:  # new
                            search_results = subreddit.search(
                                query, 
                                limit=vector['limit'], 
                                sort='new',
                                time_filter=vector['time_filter']
                            )
                        
                        vector_count = 0
                        for submission in search_results:
                            # ìˆ˜ì§‘ ë²¡í„° ì •ë³´ ì¶”ê°€
                            submission._collection_vector = vector['name']
                            all_submissions.append(submission)
                            vector_count += 1
                            
                        logger.info(f"âœ… ë²¡í„° '{vector['name']}' ì™„ë£Œ - {vector_count}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘")
                            
                    except Exception as e:
                        logger.warning(f"âš ï¸ ë²¡í„° '{vector['name']}' ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                        continue
                
                logger.info(f"ğŸ“ ê²Œì‹œë¬¼ ë¶„ì„ ë° ì¤‘ë³µ ì œê±° ì‹œì‘ - ì´ {len(all_submissions)}ê°œ ê²Œì‹œë¬¼")
                
                # ì¤‘ë³µ ì œê±° ë° í¬ë§·íŒ…
                seen_ids = set()
                processed_count = 0
                
                for submission in all_submissions:
                    if submission.id not in seen_ids:
                        seen_ids.add(submission.id)
                        
                        # ë£¨ë¨¸ ì ìˆ˜ ê³„ì‚°
                        rumor_score = self._calculate_rumor_score(submission)
                        
                        # ì–¸ì–´í•™ì  í”Œë˜ê·¸ ì¶”ì¶œ
                        text_to_analyze = submission.title + ' ' + (submission.selftext or '')
                        linguistic_flags = self._extract_linguistic_flags(text_to_analyze)
                        
                        post_data = {
                            'id': submission.id,
                            'title': submission.title,
                            'selftext': submission.selftext[:1000] if submission.selftext else '',
                            'score': submission.score,
                            'upvote_ratio': submission.upvote_ratio,
                            'num_comments': submission.num_comments,
                            'created_utc': datetime.fromtimestamp(submission.created_utc).isoformat(),
                            'subreddit': submission.subreddit.display_name,
                            'author': str(submission.author) if submission.author else '[deleted]',
                            'url': f"https://reddit.com{submission.permalink}",
                            'is_self': submission.is_self,
                            # ìƒˆë¡œ ì¶”ê°€ëœ í•„ë“œë“¤
                            'collection_vector': getattr(submission, '_collection_vector', 'unknown'),
                            'rumor_score': round(rumor_score, 1),
                            'linguistic_flags': linguistic_flags
                        }
                        
                        posts.append(post_data)
                        processed_count += 1
                        
                        # ì§„í–‰ìƒí™© ë¡œê·¸ (10ê°œë§ˆë‹¤)
                        if processed_count % 10 == 0:
                            logger.info(f"â³ ê²Œì‹œë¬¼ ì²˜ë¦¬ ì¤‘... ({processed_count}/{len(all_submissions)})")
                        
                        if len(posts) >= limit:
                            break
                
                # ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ í†µê³„ ë¡œê·¸
                vector_stats = {}
                for post in posts:
                    vector = post['collection_vector']
                    vector_stats[vector] = vector_stats.get(vector, 0) + 1
                
                logger.info(f"ğŸ“Š ë²¡í„°ë³„ ìˆ˜ì§‘ í†µê³„: {vector_stats}")
                
                return posts
            
            # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            posts = await loop.run_in_executor(None, _search)
            
            logger.info(f"ğŸ‰ Reddit ê²€ìƒ‰ ì™„ë£Œ - ì´ {len(posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘")
            return posts
            
        except Exception as e:
            logger.error(f"âŒ Reddit API ì˜¤ë¥˜: {str(e)}")
            raise RedditAPIException(f"Failed to search Reddit: {str(e)}")
    
    async def get_post_comments(self, post_id: str, limit: int = 20) -> List[Dict[str, Any]]:
        """íŠ¹ì • ê²Œì‹œë¬¼ì˜ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° - ë£¨ë¨¸ ì ìˆ˜ í¬í•¨"""
        try:
            logger.info(f"ğŸ’¬ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° ì‹œì‘: {post_id} (ìµœëŒ€ {limit}ê°œ)")
            comments = []
            
            loop = asyncio.get_event_loop()
            
            def _get_comments():
                submission = self.client.submission(id=post_id)
                submission.comments.replace_more(limit=0)  # 'load more comments' ì œê±°
                
                logger.info(f"ğŸ”„ ëŒ“ê¸€ ë¶„ì„ ì‹œì‘...")
                
                comment_list = []
                processed_count = 0
                
                for comment in submission.comments.list()[:limit]:
                    if hasattr(comment, 'body'):
                        # ëŒ“ê¸€ ë£¨ë¨¸ ì ìˆ˜ ê³„ì‚°
                        comment_rumor_score = self._calculate_comment_rumor_score(comment)
                        
                        # ì–¸ì–´í•™ì  í”Œë˜ê·¸ ì¶”ì¶œ
                        linguistic_flags = self._extract_linguistic_flags(comment.body)
                        
                        comment_data = {
                            'id': comment.id,
                            'body': comment.body[:500],  # ëŒ“ê¸€ ë‚´ìš© ì œí•œ
                            'score': comment.score,
                            'created_utc': datetime.fromtimestamp(comment.created_utc).isoformat(),
                            'author': str(comment.author) if comment.author else '[deleted]',
                            # ìƒˆë¡œ ì¶”ê°€ëœ í•„ë“œë“¤
                            'is_controversial': getattr(comment, 'controversiality', 0) == 1,
                            'rumor_score': round(comment_rumor_score, 1),
                            'linguistic_flags': linguistic_flags
                        }
                        comment_list.append(comment_data)
                        processed_count += 1
                        
                        # ë†’ì€ ë£¨ë¨¸ ì ìˆ˜ ëŒ“ê¸€ ì•Œë¦¼
                        if comment_rumor_score >= 7.0:
                            logger.warning(f"ğŸš¨ ë†’ì€ ë£¨ë¨¸ ì ìˆ˜ ëŒ“ê¸€ ë°œê²¬ [{comment_rumor_score:.1f}/10]: {comment.body[:50]}...")
                
                logger.info(f"âœ… ëŒ“ê¸€ ë¶„ì„ ì™„ë£Œ - {processed_count}ê°œ ëŒ“ê¸€ ì²˜ë¦¬")
                return comment_list
            
            comments = await loop.run_in_executor(None, _get_comments)
            
            logger.info(f"ğŸ’¬ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ - {len(comments)}ê°œ ëŒ“ê¸€")
            return comments
            
        except Exception as e:
            logger.error(f"âŒ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ {post_id}: {str(e)}")
            return []
    
    def _calculate_comment_rumor_score(self, comment) -> float:
        """ëŒ“ê¸€ ë£¨ë¨¸ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # 1. ë…¼ë€ì„± ì§€í‘œ
        if hasattr(comment, 'controversiality') and comment.controversiality == 1:
            score += 3.0
        
        # 2. ì–¸ì–´í•™ì  ì‹ í˜¸ íƒì§€
        text = comment.body.lower()
        
        # ì¶”ì¸¡ì„± ë‹¨ì–´ ê°œìˆ˜
        speculation_count = sum(1 for word in SPECULATIVE_WORDS if word in text)
        score += min(speculation_count * 1.5, 3.0)
        
        # ë¶€ì •ì  ê°ì • ë‹¨ì–´ ê°œìˆ˜
        negative_count = sum(1 for word in NEGATIVE_EMOTION_WORDS if word in text)
        score += min(negative_count * 1.0, 2.0)
        
        # 3. ëŒ“ê¸€ íŠ¹ì„±
        if hasattr(comment, 'score') and comment.score < 0:
            score += 1.0  # ë¶€ì •ì ì¸ ì ìˆ˜
        
        return min(score, 10.0)
    
    async def search_with_keywords(self, keywords: List[str], limit_per_keyword: int = 10) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ (í™•ì¥ëœ í‚¤ì›Œë“œ ì‚¬ìš©)"""
        logger.info(f"ğŸ” ë‹¤ì¤‘ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œì‘: {keywords} (í‚¤ì›Œë“œë‹¹ ìµœëŒ€ {limit_per_keyword}ê°œ)")
        
        all_posts = []
        seen_ids = set()
        
        for i, keyword in enumerate(keywords):
            try:
                logger.info(f"ğŸ¯ í‚¤ì›Œë“œ {i+1}/{len(keywords)} ê²€ìƒ‰: '{keyword}'")
                posts = await self.search_posts(keyword, limit=limit_per_keyword)
                
                new_posts = 0
                for post in posts:
                    if post['id'] not in seen_ids:
                        seen_ids.add(post['id'])
                        all_posts.append(post)
                        new_posts += 1
                        
                logger.info(f"âœ… í‚¤ì›Œë“œ '{keyword}' ì™„ë£Œ - {new_posts}ê°œ ìƒˆë¡œìš´ ê²Œì‹œë¬¼ ì¶”ê°€")
                        
            except Exception as e:
                logger.warning(f"âš ï¸ í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                continue
        
        # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        all_posts.sort(key=lambda x: x['score'], reverse=True)
        
        logger.info(f"ğŸ‰ ë‹¤ì¤‘ í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ - ì´ {len(all_posts)}ê°œ ê²Œì‹œë¬¼ (ì¤‘ë³µ ì œê±°ë¨)")
        return all_posts