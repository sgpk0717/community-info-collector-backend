from typing import List, Dict, Any, Optional
from openai import OpenAI
import logging
from .base import BaseLLMProvider, LLMResponse

logger = logging.getLogger(__name__)


class OpenAIProvider(BaseLLMProvider):
    """OpenAI API Provider êµ¬í˜„"""
    
    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        """
        OpenAI Provider ì´ˆê¸°í™”
        
        Args:
            api_key: OpenAI API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ)
            model: ì‚¬ìš©í•  ëª¨ë¸ëª… (ê¸°ë³¸ê°’: o4-mini)
        """
        self.client = OpenAI(api_key=api_key) if api_key else OpenAI()
        self.model = model or "o4-mini"
        logger.info(f"OpenAI Provider ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë¸: {self.model}")
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> LLMResponse:
        """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±"""
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        return await self.generate_with_messages(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
    
    async def generate_with_messages(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> LLMResponse:
        """OpenAI Chat Completions API í˜¸ì¶œ"""
        try:
            logger.info(f"ğŸ¤– OpenAI API í˜¸ì¶œ ì‹œì‘ - ëª¨ë¸: {self.model}, ì˜¨ë„: {temperature}")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs  # ì¶”ê°€ íŒŒë¼ë¯¸í„° (top_p, frequency_penalty ë“±)
            )
            
            content = response.choices[0].message.content.strip()
            
            # ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶”ì¶œ
            usage = None
            if hasattr(response, 'usage'):
                usage = {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            
            logger.info(f"âœ… OpenAI API ì‘ë‹µ ìˆ˜ì‹  - ê¸¸ì´: {len(content)} ë¬¸ì")
            if usage:
                logger.info(f"   í† í° ì‚¬ìš©: {usage['total_tokens']} (í”„ë¡¬í”„íŠ¸: {usage['prompt_tokens']}, ì™„ì„±: {usage['completion_tokens']})")
            
            return LLMResponse(
                content=content,
                model=self.model,
                usage=usage,
                raw_response=response
            )
            
        except Exception as e:
            logger.error(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    @property
    def provider_name(self) -> str:
        return "OpenAI"
    
    @property
    def default_model(self) -> str:
        return self.model